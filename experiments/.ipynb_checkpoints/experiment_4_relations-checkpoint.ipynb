{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b32ce4d-112a-4c2b-814b-7925eee97ce5",
   "metadata": {},
   "source": [
    "# Experiment 4 - Relations\n",
    "Now that we have our model capable of detecting components, let us consider as label the number of relation: we still split each abstract into sentences and label, make each possible combination between them and label each combined sentence as either *No related* or *Related*, corresponding to 0 and 1 respectively.\n",
    "\n",
    "During inference, we will use the same sampling technique that processes all the sentences that form an abstract. This time, our goal is to determine the percentage of relations between arguments' components that each abstract contains, and inspect if this information along with the percentage of components could help to build a working model that predicts the number of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee6643c-ae6d-4d48-ad83-18f6eb1638bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Antonio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Antonio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from experiment_4_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4b71a-b9f7-4394-ae94-5678e5c8fd67",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "Here we preprocess our data by splitting the abstracts into sentences. Each combination of two sentences is then labeled as either being a `Relationship`, labeled as `1`, or `Not relationship`, labeled as `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471b1a98-8b85-4454-bafe-31ed4ad14d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It happens 24 times that a component is already in a relation in neoplasm_train.\n",
      "It happens 4 times that a component is already in a relation in neoplasm_dev.\n",
      "It happens 21 times that a component is already in a relation in glaucoma_test.\n",
      "It happens 5 times that a component is already in a relation in neoplasm_test.\n",
      "It happens 9 times that a component is already in a relation in mixed_test.\n"
     ]
    }
   ],
   "source": [
    "train_set = read_brat_dataset('../data/train/neoplasm_train') + read_brat_dataset('../data/dev/neoplasm_dev')\n",
    "\n",
    "glaucoma_test = read_brat_dataset('../data/test/glaucoma_test')\n",
    "neoplasm_test = read_brat_dataset('../data/test/neoplasm_test')\n",
    "mixed_test = read_brat_dataset('../data/test/mixed_test')\n",
    "\n",
    "test_set = glaucoma_test + neoplasm_test + mixed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a2cffa-f391-4e11-bbe0-32124242d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "\tLabel 0: 3744 samples\n",
      "\tLabel 1: 1609 samples\n",
      "\n",
      "\tThere are 2 different labels in the train set -> [0, 1]\n",
      "\tAverage number of sentences per file in train set: 13\n",
      "\tMax sentence length: 107\n",
      "\tAverage relationships per file: 4.09\n",
      "\tAverage no relationships per file: 9.36\n",
      "\n",
      "- Test set:\n",
      "\tLabel 0: 2743 samples\n",
      "\tLabel 1: 1085 samples\n",
      "\n",
      "\tThere are 2 different labels in the test set -> [0, 1]\n",
      "\tAverage number of sentences per file in test set: 14\n",
      "\tMax sentence length: 91\n",
      "\tAverage relationships per file: 4.09\n",
      "\tAverage no relationships per file: 10.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_counts_train, avg_sentences_per_file_train = compute_dataset_statistics(train_set, dataset_name=\"train\")\n",
    "label_counts_test, avg_sentences_per_file_test = compute_dataset_statistics(test_set, dataset_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e4d204-eea2-4378-b97d-06bc31fddfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collection = FilenameLabelledCollection([data['sentence'] for data in train_set], \n",
    "                                                 [data['label'] for data in train_set], \n",
    "                                                 [data['filename'] for data in train_set])\n",
    "\n",
    "train_collection, val_collection = train_collection.split_stratified(0.7)\n",
    "# train_collection, val_collection = train_collection.split_stratified_by_filenames(0.75)\n",
    "# val_collection = FilenameLabelledCollection([data['sentence'] for data in val_set], \n",
    "#                                                  [data['label'] for data in val_set], \n",
    "#                                                  [data['filename'] for data in val_set])\n",
    "\n",
    "test_collection = FilenameLabelledCollection([data['sentence'] for data in test_set], \n",
    "                                                 [data['label'] for data in test_set], \n",
    "                                                 [data['filename'] for data in test_set])\n",
    "\n",
    "glaucoma_collection = FilenameLabelledCollection([data['sentence'] for data in glaucoma_test], \n",
    "                                                 [data['label'] for data in glaucoma_test], \n",
    "                                                 [data['filename'] for data in glaucoma_test])\n",
    "\n",
    "neoplasm_collection = FilenameLabelledCollection([data['sentence'] for data in neoplasm_test], \n",
    "                                                 [data['label'] for data in neoplasm_test], \n",
    "                                                 [data['filename'] for data in neoplasm_test])\n",
    "\n",
    "mixed_collection = FilenameLabelledCollection([data['sentence'] for data in mixed_test], \n",
    "                                                 [data['label'] for data in mixed_test], \n",
    "                                                 [data['filename'] for data in mixed_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae1e54e-10bd-40ed-8632-af7263a83863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|███████████████████████████████████████████████| 3747/3747 [00:00<00:00, 34527.43it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 1606/1606 [00:00<00:00, 50190.02it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 3828/3828 [00:00<00:00, 48744.47it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 1288/1288 [00:00<00:00, 33020.97it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 1332/1332 [00:00<00:00, 49349.98it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 1208/1208 [00:00<00:00, 29462.12it/s]\n"
     ]
    }
   ],
   "source": [
    "indexer = qp.data.preprocessing.IndexTransformer(min_df=1)\n",
    "\n",
    "# Create and index the dataset\n",
    "abs_dataset = CustomDataset(training=train_collection, test=test_collection, val=val_collection)\n",
    "index(abs_dataset, indexer, inplace=True)\n",
    "\n",
    "# Index the test collections\n",
    "index(glaucoma_collection, indexer, fit=False, inplace=True)\n",
    "index(neoplasm_collection, indexer, fit=False, inplace=True)\n",
    "index(mixed_collection, indexer, fit=False, inplace=True)\n",
    "\n",
    "qp.environ['SAMPLE_SIZE'] = avg_sentences_per_file_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97b60c-b816-49b5-8b56-8a0a4f499602",
   "metadata": {},
   "source": [
    "### 2. Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f38c32-1d94-4fda-b3d2-287b14861011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:26:42,953] A new study created in memory with name: no-name-e3381943-a2e9-44f2-bd9f-bc1bdcb97ef5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 0 with parameters:\n",
      "    Embedding size: 172 - Hidden size: 281\n",
      "    Optimizer: Adam (lr: 0.0008485856927597929) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 12, 'T_mult': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=7 tr-loss=0.14423 tr-acc=94.66% tr-macroF1=93.62% patience=1/5 val-loss=0.530C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=7 tr-loss=0.14423 tr-acc=94.66% tr-macroF1=93.62% patience=1/5 val-loss=0.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 2\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:29:40,136] Trial 0 finished with value: 0.8037904523511532 and parameters: {'embedding_size': 172, 'hidden_size': 281, 'optimizer': 'Adam', 'lr': 0.0008485856927597929, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 12, 'T_mult': 5}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.40404292941093445 - Best f1 on validation set: 0.8037904523511532\n",
      "Starting trial 1 with parameters:\n",
      "    Embedding size: 196 - Hidden size: 273\n",
      "    Optimizer: Adam (lr: 0.00038589816919175904) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 4, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=23 tr-loss=0.01614 tr-acc=99.44% tr-macroF1=99.33% patience=1/5 val-loss=0.64C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=23 tr-loss=0.01614 tr-acc=99.44% tr-macroF1=99.33% patience=1/5 val-loss=0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 18\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:40:17,044] Trial 1 finished with value: 0.8003767700081093 and parameters: {'embedding_size': 196, 'hidden_size': 273, 'optimizer': 'Adam', 'lr': 0.00038589816919175904, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 4, 'T_mult': 2}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5891496539115906 - Best f1 on validation set: 0.8003767700081093\n",
      "Starting trial 2 with parameters:\n",
      "    Embedding size: 123 - Hidden size: 275\n",
      "    Optimizer: Adam (lr: 2.3829920272740206e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 7})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=45 tr-loss=0.30593 tr-acc=87.06% tr-macroF1=83.83% patience=1/5 val-loss=0.44C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=45 tr-loss=0.30593 tr-acc=87.06% tr-macroF1=83.83% patience=1/5 val-loss=0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 40\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:53:46,262] Trial 2 finished with value: 0.7188190729739279 and parameters: {'embedding_size': 123, 'hidden_size': 275, 'optimizer': 'Adam', 'lr': 2.3829920272740206e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 7}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4410031884908676 - Best f1 on validation set: 0.7188190729739279\n",
      "Starting trial 3 with parameters:\n",
      "    Embedding size: 128 - Hidden size: 291\n",
      "    Optimizer: Adam (lr: 9.94339477109365e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 4, 'T_mult': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=8 tr-loss=0.35579 tr-acc=84.67% tr-macroF1=80.56% patience=1/5 val-loss=0.462C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=8 tr-loss=0.35579 tr-acc=84.67% tr-macroF1=80.56% patience=1/5 val-loss=0.462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 3\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:56:26,466] Trial 3 finished with value: 0.69437834128416 and parameters: {'embedding_size': 128, 'hidden_size': 291, 'optimizer': 'Adam', 'lr': 9.94339477109365e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 4, 'T_mult': 5}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5168560743331909 - Best f1 on validation set: 0.69437834128416\n",
      "Starting trial 4 with parameters:\n",
      "    Embedding size: 199 - Hidden size: 271\n",
      "    Optimizer: Adam (lr: 0.0003889299826688172) - Scheduler: CosineAnnealingLR (params: {'T_max': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=10 tr-loss=0.04451 tr-acc=99.03% tr-macroF1=98.85% patience=1/5 val-loss=0.52C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=10 tr-loss=0.04451 tr-acc=99.03% tr-macroF1=98.85% patience=1/5 val-loss=0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 5\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:00:55,030] Trial 4 finished with value: 0.7772035053925448 and parameters: {'embedding_size': 199, 'hidden_size': 271, 'optimizer': 'Adam', 'lr': 0.0003889299826688172, 'scheduler': 'CosineAnnealingLR', 'T_max': 5}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.41115377843379974 - Best f1 on validation set: 0.7772035053925448\n",
      "Starting trial 5 with parameters:\n",
      "    Embedding size: 125 - Hidden size: 275\n",
      "    Optimizer: Adam (lr: 0.00010211906915266462) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 7, 'T_mult': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.21717 tr-acc=91.80% tr-macroF1=90.01% patience=1/5 val-loss=0.42C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=13 tr-loss=0.21717 tr-acc=91.80% tr-macroF1=90.01% patience=1/5 val-loss=0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:04:55,786] Trial 5 finished with value: 0.7438892046922639 and parameters: {'embedding_size': 125, 'hidden_size': 275, 'optimizer': 'Adam', 'lr': 0.00010211906915266462, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 7, 'T_mult': 5}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4235655516386032 - Best f1 on validation set: 0.7438892046922639\n",
      "Starting trial 6 with parameters:\n",
      "    Embedding size: 180 - Hidden size: 260\n",
      "    Optimizer: Adam (lr: 3.328535004503496e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=35 tr-loss=0.20415 tr-acc=92.70% tr-macroF1=91.16% patience=1/5 val-loss=0.42C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=35 tr-loss=0.20415 tr-acc=92.70% tr-macroF1=91.16% patience=1/5 val-loss=0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 30\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:19:00,749] Trial 6 finished with value: 0.7476459510357816 and parameters: {'embedding_size': 180, 'hidden_size': 260, 'optimizer': 'Adam', 'lr': 3.328535004503496e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4059285819530487 - Best f1 on validation set: 0.7476459510357816\n",
      "Starting trial 7 with parameters:\n",
      "    Embedding size: 106 - Hidden size: 270\n",
      "    Optimizer: Adam (lr: 0.0003096340575734433) - Scheduler: CosineAnnealingLR (params: {'T_max': 9})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.13007 tr-acc=95.52% tr-macroF1=94.65% patience=1/5 val-loss=0.46C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.13007 tr-acc=95.52% tr-macroF1=94.65% patience=1/5 val-loss=0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:22:03,261] Trial 7 finished with value: 0.7856899617844035 and parameters: {'embedding_size': 106, 'hidden_size': 270, 'optimizer': 'Adam', 'lr': 0.0003096340575734433, 'scheduler': 'CosineAnnealingLR', 'T_max': 9}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.41388463973999023 - Best f1 on validation set: 0.7856899617844035\n",
      "Starting trial 8 with parameters:\n",
      "    Embedding size: 179 - Hidden size: 298\n",
      "    Optimizer: Adam (lr: 0.00013230983867777723) - Scheduler: CosineAnnealingLR (params: {'T_max': 10})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=12 tr-loss=0.15421 tr-acc=95.01% tr-macroF1=94.01% patience=1/5 val-loss=0.39C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=12 tr-loss=0.15421 tr-acc=95.01% tr-macroF1=94.01% patience=1/5 val-loss=0.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 7\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:27:22,920] Trial 8 finished with value: 0.776202147525677 and parameters: {'embedding_size': 179, 'hidden_size': 298, 'optimizer': 'Adam', 'lr': 0.00013230983867777723, 'scheduler': 'CosineAnnealingLR', 'T_max': 10}. Best is trial 0 with value: 0.8037904523511532.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4153924733400345 - Best f1 on validation set: 0.776202147525677\n",
      "Starting trial 9 with parameters:\n",
      "    Embedding size: 169 - Hidden size: 278\n",
      "    Optimizer: Adam (lr: 0.0008006129710263081) - Scheduler: CosineAnnealingLR (params: {'T_max': 9})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=9 tr-loss=0.05723 tr-acc=98.08% tr-macroF1=97.72% patience=1/5 val-loss=0.575C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=9 tr-loss=0.05723 tr-acc=98.08% tr-macroF1=97.72% patience=1/5 val-loss=0.575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 4\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:31:00,942] Trial 9 finished with value: 0.8068037111151537 and parameters: {'embedding_size': 169, 'hidden_size': 278, 'optimizer': 'Adam', 'lr': 0.0008006129710263081, 'scheduler': 'CosineAnnealingLR', 'T_max': 9}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3986272066831589 - Best f1 on validation set: 0.8068037111151537\n",
      "Starting trial 10 with parameters:\n",
      "    Embedding size: 151 - Hidden size: 287\n",
      "    Optimizer: Adam (lr: 1.0734940879113042e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 12})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=6 tr-loss=0.60343 tr-acc=69.69% tr-macroF1=46.09% patience=1/5 val-loss=0.592C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=6 tr-loss=0.60343 tr-acc=69.69% tr-macroF1=46.09% patience=1/5 val-loss=0.592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 1\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:33:18,468] Trial 10 finished with value: 0.4104912572855953 and parameters: {'embedding_size': 151, 'hidden_size': 287, 'optimizer': 'Adam', 'lr': 1.0734940879113042e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.6156531870365143 - Best f1 on validation set: 0.4104912572855953\n",
      "Starting trial 11 with parameters:\n",
      "    Embedding size: 154 - Hidden size: 284\n",
      "    Optimizer: Adam (lr: 0.0009064651384380115) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 12, 'T_mult': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=9 tr-loss=0.03925 tr-acc=98.60% tr-macroF1=98.33% patience=1/5 val-loss=0.683C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=9 tr-loss=0.03925 tr-acc=98.60% tr-macroF1=98.33% patience=1/5 val-loss=0.683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 4\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:36:41,538] Trial 11 finished with value: 0.772732212951706 and parameters: {'embedding_size': 154, 'hidden_size': 284, 'optimizer': 'Adam', 'lr': 0.0009064651384380115, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 12, 'T_mult': 4}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.44012413918972015 - Best f1 on validation set: 0.772732212951706\n",
      "Starting trial 12 with parameters:\n",
      "    Embedding size: 167 - Hidden size: 281\n",
      "    Optimizer: Adam (lr: 0.0009710587639767617) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 11, 'T_mult': 1})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=15 tr-loss=0.04787 tr-acc=98.36% tr-macroF1=98.05% patience=1/5 val-loss=0.66C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=15 tr-loss=0.04787 tr-acc=98.36% tr-macroF1=98.05% patience=1/5 val-loss=0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 10\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:42:34,853] Trial 12 finished with value: 0.7848657943396893 and parameters: {'embedding_size': 167, 'hidden_size': 281, 'optimizer': 'Adam', 'lr': 0.0009710587639767617, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 11, 'T_mult': 1}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.646946519613266 - Best f1 on validation set: 0.7848657943396893\n",
      "Starting trial 13 with parameters:\n",
      "    Embedding size: 168 - Hidden size: 262\n",
      "    Optimizer: Adam (lr: 0.0005682106988700279) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 9, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.03212 tr-acc=98.97% tr-macroF1=98.77% patience=1/5 val-loss=0.57C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.03212 tr-acc=98.97% tr-macroF1=98.77% patience=1/5 val-loss=0.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:46:46,315] Trial 13 finished with value: 0.8031364698031365 and parameters: {'embedding_size': 168, 'hidden_size': 262, 'optimizer': 'Adam', 'lr': 0.0005682106988700279, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 9, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4530303329229355 - Best f1 on validation set: 0.8031364698031365\n",
      "Starting trial 14 with parameters:\n",
      "    Embedding size: 167 - Hidden size: 292\n",
      "    Optimizer: Adam (lr: 0.00019006947169647595) - Scheduler: CosineAnnealingLR (params: {'T_max': 7})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.15313 tr-acc=95.05% tr-macroF1=94.07% patience=1/5 val-loss=0.44C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.15313 tr-acc=95.05% tr-macroF1=94.07% patience=1/5 val-loss=0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:51:19,517] Trial 14 finished with value: 0.7697040885475491 and parameters: {'embedding_size': 167, 'hidden_size': 292, 'optimizer': 'Adam', 'lr': 0.00019006947169647595, 'scheduler': 'CosineAnnealingLR', 'T_max': 7}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4690325856208801 - Best f1 on validation set: 0.7697040885475491\n",
      "Starting trial 15 with parameters:\n",
      "    Embedding size: 142 - Hidden size: 266\n",
      "    Optimizer: Adam (lr: 0.0006219422225430925) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 1, 'T_mult': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=8 tr-loss=0.11021 tr-acc=95.93% tr-macroF1=95.15% patience=1/5 val-loss=0.477C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=8 tr-loss=0.11021 tr-acc=95.93% tr-macroF1=95.15% patience=1/5 val-loss=0.477"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 3\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2024-10-30 17:54:05,435] Trial 15 finished with value: 0.7923653846153846 and parameters: {'embedding_size': 142, 'hidden_size': 266, 'optimizer': 'Adam', 'lr': 0.0006219422225430925, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 1, 'T_mult': 4}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3758814185857773 - Best f1 on validation set: 0.7923653846153846\n",
      "Starting trial 16 with parameters:\n",
      "    Embedding size: 184 - Hidden size: 279\n",
      "    Optimizer: Adam (lr: 0.0002159521825089592) - Scheduler: CosineAnnealingLR (params: {'T_max': 12})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=16 tr-loss=0.03758 tr-acc=98.92% tr-macroF1=98.72% patience=1/5 val-loss=0.53C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=16 tr-loss=0.03758 tr-acc=98.92% tr-macroF1=98.72% patience=1/5 val-loss=0.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 11\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:00:50,125] Trial 16 finished with value: 0.7833620195065978 and parameters: {'embedding_size': 184, 'hidden_size': 279, 'optimizer': 'Adam', 'lr': 0.0002159521825089592, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4374668449163437 - Best f1 on validation set: 0.7833620195065978\n",
      "Starting trial 17 with parameters:\n",
      "    Embedding size: 159 - Hidden size: 286\n",
      "    Optimizer: Adam (lr: 6.371478135497518e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 9, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=23 tr-loss=0.12668 tr-acc=95.54% tr-macroF1=94.67% patience=1/5 val-loss=0.45C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=23 tr-loss=0.12668 tr-acc=95.54% tr-macroF1=94.67% patience=1/5 val-loss=0.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 18\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:09:52,817] Trial 17 finished with value: 0.7682145082425996 and parameters: {'embedding_size': 159, 'hidden_size': 286, 'optimizer': 'Adam', 'lr': 6.371478135497518e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 9, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.41532444953918457 - Best f1 on validation set: 0.7682145082425996\n",
      "Starting trial 18 with parameters:\n",
      "    Embedding size: 136 - Hidden size: 299\n",
      "    Optimizer: Adam (lr: 0.0005983679698782991) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 10, 'T_mult': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=7 tr-loss=0.11781 tr-acc=95.29% tr-macroF1=94.37% patience=1/5 val-loss=0.448C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=7 tr-loss=0.11781 tr-acc=95.29% tr-macroF1=94.37% patience=1/5 val-loss=0.448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 2\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:12:11,816] Trial 18 finished with value: 0.782733224222586 and parameters: {'embedding_size': 136, 'hidden_size': 299, 'optimizer': 'Adam', 'lr': 0.0005983679698782991, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 10, 'T_mult': 5}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3951180726289749 - Best f1 on validation set: 0.782733224222586\n",
      "Starting trial 19 with parameters:\n",
      "    Embedding size: 186 - Hidden size: 266\n",
      "    Optimizer: Adam (lr: 0.00024826339125217343) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=10 tr-loss=0.08095 tr-acc=97.40% tr-macroF1=96.88% patience=1/5 val-loss=0.46C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=10 tr-loss=0.08095 tr-acc=97.40% tr-macroF1=96.88% patience=1/5 val-loss=0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 5\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:16:11,319] Trial 19 finished with value: 0.7598104246204949 and parameters: {'embedding_size': 186, 'hidden_size': 266, 'optimizer': 'Adam', 'lr': 0.00024826339125217343, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.41375014185905457 - Best f1 on validation set: 0.7598104246204949\n",
      "Starting trial 20 with parameters:\n",
      "    Embedding size: 173 - Hidden size: 281\n",
      "    Optimizer: Adam (lr: 5.720121019225376e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 8})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=28 tr-loss=0.13907 tr-acc=95.39% tr-macroF1=94.47% patience=1/5 val-loss=0.43C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=28 tr-loss=0.13907 tr-acc=95.39% tr-macroF1=94.47% patience=1/5 val-loss=0.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 23\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:26:48,344] Trial 20 finished with value: 0.7593622543566927 and parameters: {'embedding_size': 173, 'hidden_size': 281, 'optimizer': 'Adam', 'lr': 5.720121019225376e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 8}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4100693017244339 - Best f1 on validation set: 0.7593622543566927\n",
      "Starting trial 21 with parameters:\n",
      "    Embedding size: 162 - Hidden size: 257\n",
      "    Optimizer: Adam (lr: 0.000596095526672219) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 8, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=12 tr-loss=0.02565 tr-acc=99.29% tr-macroF1=99.15% patience=1/5 val-loss=0.64C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=12 tr-loss=0.02565 tr-acc=99.29% tr-macroF1=99.15% patience=1/5 val-loss=0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 7\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:30:50,296] Trial 21 finished with value: 0.7669802626915248 and parameters: {'embedding_size': 162, 'hidden_size': 257, 'optimizer': 'Adam', 'lr': 0.000596095526672219, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 8, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4672970771789551 - Best f1 on validation set: 0.7669802626915248\n",
      "Starting trial 22 with parameters:\n",
      "    Embedding size: 172 - Hidden size: 263\n",
      "    Optimizer: Adam (lr: 0.0005589533046794249) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 12, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=9 tr-loss=0.05242 tr-acc=98.11% tr-macroF1=97.74% patience=1/5 val-loss=0.473C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=9 tr-loss=0.05242 tr-acc=98.11% tr-macroF1=97.74% patience=1/5 val-loss=0.473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 4\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:34:01,483] Trial 22 finished with value: 0.7939602874348011 and parameters: {'embedding_size': 172, 'hidden_size': 263, 'optimizer': 'Adam', 'lr': 0.0005589533046794249, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 12, 'T_mult': 2}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3784707486629486 - Best f1 on validation set: 0.7939602874348011\n",
      "Starting trial 23 with parameters:\n",
      "    Embedding size: 145 - Hidden size: 256\n",
      "    Optimizer: Adam (lr: 0.0009968532374448032) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 10, 'T_mult': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.01987 tr-acc=99.48% tr-macroF1=99.38% patience=1/5 val-loss=0.66C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.01987 tr-acc=99.48% tr-macroF1=99.38% patience=1/5 val-loss=0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:37:23,277] Trial 23 finished with value: 0.7481177347989132 and parameters: {'embedding_size': 145, 'hidden_size': 256, 'optimizer': 'Adam', 'lr': 0.0009968532374448032, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 10, 'T_mult': 4}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5684578120708466 - Best f1 on validation set: 0.7481177347989132\n",
      "Starting trial 24 with parameters:\n",
      "    Embedding size: 190 - Hidden size: 291\n",
      "    Optimizer: Adam (lr: 0.000426838445544328) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 6, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.01842 tr-acc=99.42% tr-macroF1=99.31% patience=1/5 val-loss=0.63C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=18 tr-loss=0.01842 tr-acc=99.42% tr-macroF1=99.31% patience=1/5 val-loss=0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 13\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:45:04,660] Trial 24 finished with value: 0.7971409154090783 and parameters: {'embedding_size': 190, 'hidden_size': 291, 'optimizer': 'Adam', 'lr': 0.000426838445544328, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 6, 'T_mult': 2}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5206275582313538 - Best f1 on validation set: 0.7971409154090783\n",
      "Starting trial 25 with parameters:\n",
      "    Embedding size: 173 - Hidden size: 268\n",
      "    Optimizer: Adam (lr: 0.0006626704845400004) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 9, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=12 tr-loss=0.01855 tr-acc=99.53% tr-macroF1=99.44% patience=1/5 val-loss=0.61C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=12 tr-loss=0.01855 tr-acc=99.53% tr-macroF1=99.44% patience=1/5 val-loss=0.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 7\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:49:22,336] Trial 25 finished with value: 0.7997334381682024 and parameters: {'embedding_size': 173, 'hidden_size': 268, 'optimizer': 'Adam', 'lr': 0.0006626704845400004, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 9, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4875870794057846 - Best f1 on validation set: 0.7997334381682024\n",
      "Starting trial 26 with parameters:\n",
      "    Embedding size: 156 - Hidden size: 277\n",
      "    Optimizer: Adam (lr: 0.0003167393235445352) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 12, 'T_mult': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=27 tr-loss=0.02078 tr-acc=99.20% tr-macroF1=99.05% patience=1/5 val-loss=0.80C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=27 tr-loss=0.02078 tr-acc=99.20% tr-macroF1=99.05% patience=1/5 val-loss=0.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 22\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:58:09,188] Trial 26 finished with value: 0.7888822350862407 and parameters: {'embedding_size': 156, 'hidden_size': 277, 'optimizer': 'Adam', 'lr': 0.0003167393235445352, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 12, 'T_mult': 4}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.6176514625549316 - Best f1 on validation set: 0.7888822350862407\n",
      "Starting trial 27 with parameters:\n",
      "    Embedding size: 163 - Hidden size: 262\n",
      "    Optimizer: Adam (lr: 0.0007883398409050293) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 6, 'T_mult': 1})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.02720 tr-acc=99.42% tr-macroF1=99.31% patience=1/5 val-loss=0.73C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=13 tr-loss=0.02720 tr-acc=99.42% tr-macroF1=99.31% patience=1/5 val-loss=0.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:02:30,779] Trial 27 finished with value: 0.7903071348885884 and parameters: {'embedding_size': 163, 'hidden_size': 262, 'optimizer': 'Adam', 'lr': 0.0007883398409050293, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 6, 'T_mult': 1}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5102259069681168 - Best f1 on validation set: 0.7903071348885884\n",
      "Starting trial 28 with parameters:\n",
      "    Embedding size: 192 - Hidden size: 294\n",
      "    Optimizer: Adam (lr: 0.0004774788218341842) - Scheduler: CosineAnnealingLR (params: {'T_max': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=10 tr-loss=0.10045 tr-acc=96.28% tr-macroF1=95.57% patience=1/5 val-loss=0.46C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=10 tr-loss=0.10045 tr-acc=96.28% tr-macroF1=95.57% patience=1/5 val-loss=0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 5\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:06:41,774] Trial 28 finished with value: 0.7899743230625583 and parameters: {'embedding_size': 192, 'hidden_size': 294, 'optimizer': 'Adam', 'lr': 0.0004774788218341842, 'scheduler': 'CosineAnnealingLR', 'T_max': 5}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.38049736618995667 - Best f1 on validation set: 0.7899743230625583\n",
      "Starting trial 29 with parameters:\n",
      "    Embedding size: 177 - Hidden size: 273\n",
      "    Optimizer: Adam (lr: 0.0001553057482756347) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 10, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.09503 tr-acc=97.27% tr-macroF1=96.74% patience=1/5 val-loss=0.48C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.09503 tr-acc=97.27% tr-macroF1=96.74% patience=1/5 val-loss=0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:10:47,047] Trial 29 finished with value: 0.7870817872475451 and parameters: {'embedding_size': 177, 'hidden_size': 273, 'optimizer': 'Adam', 'lr': 0.0001553057482756347, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 10, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.429781973361969 - Best f1 on validation set: 0.7870817872475451\n",
      "Starting trial 30 with parameters:\n",
      "    Embedding size: 147 - Hidden size: 285\n",
      "    Optimizer: Adam (lr: 0.00032062025401670543) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 8, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=14 tr-loss=0.03786 tr-acc=98.90% tr-macroF1=98.69% patience=1/5 val-loss=0.49C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=14 tr-loss=0.03786 tr-acc=98.90% tr-macroF1=98.69% patience=1/5 val-loss=0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 9\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:15:17,092] Trial 30 finished with value: 0.7812346277157187 and parameters: {'embedding_size': 147, 'hidden_size': 285, 'optimizer': 'Adam', 'lr': 0.00032062025401670543, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 8, 'T_mult': 2}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4391588568687439 - Best f1 on validation set: 0.7812346277157187\n",
      "Starting trial 31 with parameters:\n",
      "    Embedding size: 199 - Hidden size: 276\n",
      "    Optimizer: Adam (lr: 0.00040765525110272237) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 4, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=10 tr-loss=0.05688 tr-acc=98.06% tr-macroF1=97.69% patience=1/5 val-loss=0.51C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=10 tr-loss=0.05688 tr-acc=98.06% tr-macroF1=97.69% patience=1/5 val-loss=0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 5\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:19:25,631] Trial 31 finished with value: 0.7937773070303191 and parameters: {'embedding_size': 199, 'hidden_size': 276, 'optimizer': 'Adam', 'lr': 0.00040765525110272237, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 4, 'T_mult': 2}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.39496470987796783 - Best f1 on validation set: 0.7937773070303191\n",
      "Starting trial 32 with parameters:\n",
      "    Embedding size: 192 - Hidden size: 273\n",
      "    Optimizer: Adam (lr: 0.0008105724333512531) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 4, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=7 tr-loss=0.05091 tr-acc=98.34% tr-macroF1=98.02% patience=1/5 val-loss=0.485C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=7 tr-loss=0.05091 tr-acc=98.34% tr-macroF1=98.02% patience=1/5 val-loss=0.485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 2\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:22:14,628] Trial 32 finished with value: 0.7791919321777263 and parameters: {'embedding_size': 192, 'hidden_size': 273, 'optimizer': 'Adam', 'lr': 0.0008105724333512531, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 4, 'T_mult': 2}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4050210416316986 - Best f1 on validation set: 0.7791919321777263\n",
      "Starting trial 33 with parameters:\n",
      "    Embedding size: 166 - Hidden size: 281\n",
      "    Optimizer: Adam (lr: 0.00046563338303563654) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 2, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.07408 tr-acc=97.63% tr-macroF1=97.18% patience=1/5 val-loss=0.72C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=13 tr-loss=0.07408 tr-acc=97.63% tr-macroF1=97.18% patience=1/5 val-loss=0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:26:52,916] Trial 33 finished with value: 0.7899743230625583 and parameters: {'embedding_size': 166, 'hidden_size': 281, 'optimizer': 'Adam', 'lr': 0.00046563338303563654, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 2, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4403691589832306 - Best f1 on validation set: 0.7899743230625583\n",
      "Starting trial 34 with parameters:\n",
      "    Embedding size: 135 - Hidden size: 273\n",
      "    Optimizer: Adam (lr: 0.0007545644559317492) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 5, 'T_mult': 1})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=10 tr-loss=0.04028 tr-acc=98.64% tr-macroF1=98.39% patience=1/5 val-loss=0.72C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=10 tr-loss=0.04028 tr-acc=98.64% tr-macroF1=98.39% patience=1/5 val-loss=0.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 5\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:29:53,826] Trial 34 finished with value: 0.7725163801058506 and parameters: {'embedding_size': 135, 'hidden_size': 273, 'optimizer': 'Adam', 'lr': 0.0007545644559317492, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 5, 'T_mult': 1}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.42851191759109497 - Best f1 on validation set: 0.7725163801058506\n",
      "Starting trial 35 with parameters:\n",
      "    Embedding size: 116 - Hidden size: 279\n",
      "    Optimizer: Adam (lr: 0.00028844696620494324) - Scheduler: CosineAnnealingLR (params: {'T_max': 10})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.05690 tr-acc=98.04% tr-macroF1=97.66% patience=1/5 val-loss=0.52C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=18 tr-loss=0.05690 tr-acc=98.04% tr-macroF1=97.66% patience=1/5 val-loss=0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 13\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:34:40,137] Trial 35 finished with value: 0.7829169260501547 and parameters: {'embedding_size': 116, 'hidden_size': 279, 'optimizer': 'Adam', 'lr': 0.00028844696620494324, 'scheduler': 'CosineAnnealingLR', 'T_max': 10}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.42854996025562286 - Best f1 on validation set: 0.7829169260501547\n",
      "Starting trial 36 with parameters:\n",
      "    Embedding size: 185 - Hidden size: 268\n",
      "    Optimizer: Adam (lr: 0.0003668286526745206) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 3, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.02678 tr-acc=99.14% tr-macroF1=98.97% patience=1/5 val-loss=0.55C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=13 tr-loss=0.02678 tr-acc=99.14% tr-macroF1=98.97% patience=1/5 val-loss=0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:39:35,559] Trial 36 finished with value: 0.7699069117109854 and parameters: {'embedding_size': 185, 'hidden_size': 268, 'optimizer': 'Adam', 'lr': 0.0003668286526745206, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 3, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.49046850204467773 - Best f1 on validation set: 0.7699069117109854\n",
      "Starting trial 37 with parameters:\n",
      "    Embedding size: 200 - Hidden size: 283\n",
      "    Optimizer: Adam (lr: 0.000501975680366069) - Scheduler: CosineAnnealingLR (params: {'T_max': 6})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.09121 tr-acc=96.81% tr-macroF1=96.21% patience=1/5 val-loss=0.63C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=18 tr-loss=0.09121 tr-acc=96.81% tr-macroF1=96.21% patience=1/5 val-loss=0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 13\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:47:04,283] Trial 37 finished with value: 0.794997159737679 and parameters: {'embedding_size': 200, 'hidden_size': 283, 'optimizer': 'Adam', 'lr': 0.000501975680366069, 'scheduler': 'CosineAnnealingLR', 'T_max': 6}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.648291289806366 - Best f1 on validation set: 0.794997159737679\n",
      "Starting trial 38 with parameters:\n",
      "    Embedding size: 178 - Hidden size: 270\n",
      "    Optimizer: Adam (lr: 1.6385199948579347e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 10})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=6 tr-loss=0.57858 tr-acc=70.57% tr-macroF1=48.11% patience=1/5 val-loss=0.578C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=6 tr-loss=0.57858 tr-acc=70.57% tr-macroF1=48.11% patience=1/5 val-loss=0.578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 1\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:49:17,929] Trial 38 finished with value: 0.4104912572855953 and parameters: {'embedding_size': 178, 'hidden_size': 270, 'optimizer': 'Adam', 'lr': 1.6385199948579347e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 10}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.6072590351104736 - Best f1 on validation set: 0.4104912572855953\n",
      "Starting trial 39 with parameters:\n",
      "    Embedding size: 171 - Hidden size: 260\n",
      "    Optimizer: Adam (lr: 7.412761369975003e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 7, 'T_mult': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=14 tr-loss=0.19748 tr-acc=93.09% tr-macroF1=91.63% patience=1/5 val-loss=0.42C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=14 tr-loss=0.19748 tr-acc=93.09% tr-macroF1=91.63% patience=1/5 val-loss=0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 9\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:54:10,981] Trial 39 finished with value: 0.7429887155658812 and parameters: {'embedding_size': 171, 'hidden_size': 260, 'optimizer': 'Adam', 'lr': 7.412761369975003e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 7, 'T_mult': 4}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.43637366592884064 - Best f1 on validation set: 0.7429887155658812\n",
      "Starting trial 40 with parameters:\n",
      "    Embedding size: 156 - Hidden size: 275\n",
      "    Optimizer: Adam (lr: 4.3170197455852316e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=30 tr-loss=0.21325 tr-acc=92.21% tr-macroF1=90.50% patience=1/5 val-loss=0.42C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=30 tr-loss=0.21325 tr-acc=92.21% tr-macroF1=90.50% patience=1/5 val-loss=0.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 25\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:04:00,520] Trial 40 finished with value: 0.7363465160075329 and parameters: {'embedding_size': 156, 'hidden_size': 275, 'optimizer': 'Adam', 'lr': 4.3170197455852316e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.42039112746715546 - Best f1 on validation set: 0.7363465160075329\n",
      "Starting trial 41 with parameters:\n",
      "    Embedding size: 176 - Hidden size: 267\n",
      "    Optimizer: Adam (lr: 0.0007239994260249259) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 9, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.01259 tr-acc=99.72% tr-macroF1=99.67% patience=1/5 val-loss=0.77C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=13 tr-loss=0.01259 tr-acc=99.72% tr-macroF1=99.67% patience=1/5 val-loss=0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:08:39,475] Trial 41 finished with value: 0.7861556743909686 and parameters: {'embedding_size': 176, 'hidden_size': 267, 'optimizer': 'Adam', 'lr': 0.0007239994260249259, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 9, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5293657332658768 - Best f1 on validation set: 0.7861556743909686\n",
      "Starting trial 42 with parameters:\n",
      "    Embedding size: 181 - Hidden size: 263\n",
      "    Optimizer: Adam (lr: 0.0006989152384656802) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 11, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=7 tr-loss=0.11199 tr-acc=95.74% tr-macroF1=94.91% patience=1/5 val-loss=0.532C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=7 tr-loss=0.11199 tr-acc=95.74% tr-macroF1=94.91% patience=1/5 val-loss=0.532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 2\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:11:15,656] Trial 42 finished with value: 0.7827763378424172 and parameters: {'embedding_size': 181, 'hidden_size': 263, 'optimizer': 'Adam', 'lr': 0.0006989152384656802, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 11, 'T_mult': 3}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4423588663339615 - Best f1 on validation set: 0.7827763378424172\n",
      "Starting trial 43 with parameters:\n",
      "    Embedding size: 173 - Hidden size: 271\n",
      "    Optimizer: Adam (lr: 0.0009020739014232177) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 8, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=15 tr-loss=0.03839 tr-acc=98.64% tr-macroF1=98.39% patience=1/5 val-loss=0.66C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=15 tr-loss=0.03839 tr-acc=98.64% tr-macroF1=98.39% patience=1/5 val-loss=0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 10\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:16:35,659] Trial 43 finished with value: 0.7889716840536513 and parameters: {'embedding_size': 173, 'hidden_size': 271, 'optimizer': 'Adam', 'lr': 0.0009020739014232177, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 8, 'T_mult': 2}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5319197624921799 - Best f1 on validation set: 0.7889716840536513\n",
      "Starting trial 44 with parameters:\n",
      "    Embedding size: 169 - Hidden size: 269\n",
      "    Optimizer: Adam (lr: 0.0006372736299660221) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 9, 'T_mult': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.03038 tr-acc=99.12% tr-macroF1=98.95% patience=1/5 val-loss=0.66C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.03038 tr-acc=99.12% tr-macroF1=98.95% patience=1/5 val-loss=0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:20:25,242] Trial 44 finished with value: 0.7913838650680756 and parameters: {'embedding_size': 169, 'hidden_size': 269, 'optimizer': 'Adam', 'lr': 0.0006372736299660221, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 9, 'T_mult': 4}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.43063829839229584 - Best f1 on validation set: 0.7913838650680756\n",
      "Starting trial 45 with parameters:\n",
      "    Embedding size: 161 - Hidden size: 265\n",
      "    Optimizer: Adam (lr: 0.00010881014372510626) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 11, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.10888 tr-acc=96.60% tr-macroF1=95.94% patience=1/5 val-loss=0.43C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=18 tr-loss=0.10888 tr-acc=96.60% tr-macroF1=95.94% patience=1/5 val-loss=0.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 13\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:26:25,885] Trial 45 finished with value: 0.7551585014409221 and parameters: {'embedding_size': 161, 'hidden_size': 265, 'optimizer': 'Adam', 'lr': 0.00010881014372510626, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 11, 'T_mult': 2}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.40219658613204956 - Best f1 on validation set: 0.7551585014409221\n",
      "Starting trial 46 with parameters:\n",
      "    Embedding size: 153 - Hidden size: 275\n",
      "    Optimizer: Adam (lr: 0.0003754813072579791) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 5, 'T_mult': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=8 tr-loss=0.05993 tr-acc=98.21% tr-macroF1=97.87% patience=1/5 val-loss=0.478C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=8 tr-loss=0.05993 tr-acc=98.21% tr-macroF1=97.87% patience=1/5 val-loss=0.478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 3\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:29:06,233] Trial 46 finished with value: 0.7854237981012078 and parameters: {'embedding_size': 153, 'hidden_size': 275, 'optimizer': 'Adam', 'lr': 0.0003754813072579791, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 5, 'T_mult': 5}. Best is trial 9 with value: 0.8068037111151537.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.42369110882282257 - Best f1 on validation set: 0.7854237981012078\n",
      "Starting trial 47 with parameters:\n",
      "    Embedding size: 195 - Hidden size: 278\n",
      "    Optimizer: Adam (lr: 0.0005161449102180434) - Scheduler: CosineAnnealingLR (params: {'T_max': 11})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=14 tr-loss=0.03197 tr-acc=99.20% tr-macroF1=99.05% patience=1/5 val-loss=0.69C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=14 tr-loss=0.03197 tr-acc=99.20% tr-macroF1=99.05% patience=1/5 val-loss=0.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 9\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:34:47,091] Trial 47 finished with value: 0.8110541940329175 and parameters: {'embedding_size': 195, 'hidden_size': 278, 'optimizer': 'Adam', 'lr': 0.0005161449102180434, 'scheduler': 'CosineAnnealingLR', 'T_max': 11}. Best is trial 47 with value: 0.8110541940329175.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.47745291888713837 - Best f1 on validation set: 0.8110541940329175\n",
      "Starting trial 48 with parameters:\n",
      "    Embedding size: 186 - Hidden size: 288\n",
      "    Optimizer: Adam (lr: 0.0002590951849208455) - Scheduler: CosineAnnealingLR (params: {'T_max': 11})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=14 tr-loss=0.04482 tr-acc=98.67% tr-macroF1=98.41% patience=1/5 val-loss=0.54C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=14 tr-loss=0.04482 tr-acc=98.67% tr-macroF1=98.41% patience=1/5 val-loss=0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 9\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:40:13,149] Trial 48 finished with value: 0.786531279178338 and parameters: {'embedding_size': 186, 'hidden_size': 288, 'optimizer': 'Adam', 'lr': 0.0002590951849208455, 'scheduler': 'CosineAnnealingLR', 'T_max': 11}. Best is trial 47 with value: 0.8110541940329175.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.40929220616817474 - Best f1 on validation set: 0.786531279178338\n",
      "Starting trial 49 with parameters:\n",
      "    Embedding size: 195 - Hidden size: 278\n",
      "    Optimizer: Adam (lr: 0.000523688821899043) - Scheduler: CosineAnnealingLR (params: {'T_max': 9})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=8 tr-loss=0.05311 tr-acc=98.11% tr-macroF1=97.74% patience=1/5 val-loss=0.487C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:672: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=8 tr-loss=0.05311 tr-acc=98.11% tr-macroF1=97.74% patience=1/5 val-loss=0.487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/relations/classifier_net.dat from epoch 3\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:43:29,176] Trial 49 finished with value: 0.7943555065857224 and parameters: {'embedding_size': 195, 'hidden_size': 278, 'optimizer': 'Adam', 'lr': 0.000523688821899043, 'scheduler': 'CosineAnnealingLR', 'T_max': 9}. Best is trial 47 with value: 0.8110541940329175.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3936632573604584 - Best f1 on validation set: 0.7943555065857224\n",
      "Best trial:\n",
      "  Macro F1 Score: 0.8110541940329175\n",
      "  Params:\n",
      "    embedding_size: 195\n",
      "    hidden_size: 278\n",
      "    optimizer: Adam\n",
      "    lr: 0.0005161449102180434\n",
      "    scheduler: CosineAnnealingLR\n",
      "    T_max: 11\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, abs_dataset), n_trials=50)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Macro F1 Score: {trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1203e0-5eeb-4a36-b123-ab7402a528bd",
   "metadata": {},
   "source": [
    "We can now train and test using the obtained hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb9680a2-8b2f-41e7-8fd3-139ee4e96e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=27 tr-loss=0.03266 tr-acc=99.41% tr-macroF1=99.30% patience=1/10 val-loss=0.7C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:671: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=27 tr-loss=0.03266 tr-acc=99.41% tr-macroF1=99.30% patience=1/10 val-loss=0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoints/relations/classifier_net.dat from epoch 17\n",
      "Performing a final training pass over the validation set...\n",
      "[Training complete] - Best loss on validation set: 0.619984969496727 - Best f1 on validation set: 0.800266397985613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<experiment_4_code.ScheduledNeuralClassifierTrainer at 0x1ea0591dc10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "embedding_size = 195\n",
    "hidden_size = 278   \n",
    "lr = 0.0005161449102180434\n",
    "\n",
    "cnn_module = CNNnet(\n",
    "    abs_dataset.vocabulary_size,\n",
    "    abs_dataset.training.n_classes,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size\n",
    ")\n",
    "\n",
    "optimizer = Adam(cnn_module.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=11)\n",
    "\n",
    "cnn_classifier = ScheduledNeuralClassifierTrainer(\n",
    "    cnn_module,\n",
    "    lr_scheduler=scheduler,\n",
    "    optim = optimizer,\n",
    "    device='cpu',\n",
    "    checkpointpath='../checkpoints/relations/classifier_net.dat',\n",
    "    padding_length=107,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "cnn_classifier.fit(*abs_dataset.training.Xy, *abs_dataset.val.Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb1a8898-7f41-4965-8b99-8042e1312f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "\tF1: 0.9993651833835607\n",
      "\tAccuracy: 0.9994662396583934\n",
      "- Glaucoma test set:\n",
      "\tF1: 0.7257252862082719\n",
      "\tAccuracy: 0.7492236024844721\n",
      "- Neoplasm test set:\n",
      "\tF1: 0.7793549649737808\n",
      "\tAccuracy: 0.8055555555555556\n",
      "- Mixed test set:\n",
      "\tF1: 0.7357074017642404\n",
      "\tAccuracy: 0.7690397350993378\n"
     ]
    }
   ],
   "source": [
    "f1_train = 1-qp.error.f1e(abs_dataset.training.labels, cnn_classifier.predict(abs_dataset.training.instances))\n",
    "accuracy_train = 1-qp.error.acce(abs_dataset.training.labels, cnn_classifier.predict(abs_dataset.training.instances))\n",
    "print('- Train set:')\n",
    "print(f'\\tF1: {f1_train}')    \n",
    "print(f'\\tAccuracy: {accuracy_train}')    \n",
    "\n",
    "f1_test_glaucoma = 1-qp.error.f1e(glaucoma_collection.labels, cnn_classifier.predict(glaucoma_collection.instances))\n",
    "accuracy_test_glaucoma = 1-qp.error.acce(glaucoma_collection.labels, cnn_classifier.predict(glaucoma_collection.instances))\n",
    "\n",
    "print('- Glaucoma test set:')\n",
    "print(f'\\tF1: {f1_test_glaucoma}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_glaucoma}')    \n",
    "\n",
    "f1_test_neoplasm = 1-qp.error.f1e(neoplasm_collection.labels, cnn_classifier.predict(neoplasm_collection.instances))\n",
    "accuracy_test_neoplasm = 1-qp.error.acce(neoplasm_collection.labels, cnn_classifier.predict(neoplasm_collection.instances))\n",
    "\n",
    "print('- Neoplasm test set:')\n",
    "print(f'\\tF1: {f1_test_neoplasm}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_neoplasm}')\n",
    "\n",
    "f1_test_mixed = 1-qp.error.f1e(mixed_collection.labels, cnn_classifier.predict(mixed_collection.instances))\n",
    "accuracy_test_mixed = 1-qp.error.acce(mixed_collection.labels, cnn_classifier.predict(mixed_collection.instances))\n",
    "\n",
    "print('- Mixed test set:')\n",
    "print(f'\\tF1: {f1_test_mixed}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_mixed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c873988-2b70-4de9-8af6-2863d0e55bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 9643663 - Relations: [0] - Text:\n",
      " To evaluate the efficacy and tolerability of 'Casodex' monotherapy (150 mg daily) for metastatic and locally advanced prostate cancer. A total of 1,453 patients with either confirmed metastatic disease (M1), or T3/T4 non-metastatic disease with elevated prostate-specific antigen (M0) were recruited into one of two identical, multicentre, randomised studies to compare 'Casodex' 150 mg/day with castration. The protocols allowed for combined analysis. At a median follow-up period of approximately 100 weeks for both studies, 'Casodex' 150 mg was found to be less effective than castration in patients with metastatic disease (M1) at entry (hazard ratio of 1.30 for time to death) with a difference in median survival of 6 weeks. In symptomatic M1 patients, 'Casodex' was associated with a statistically significant improvement in subjective response (70%) compared with castration (58%). Analysis of a validated quality-of-life questionnaire proved an advantage for 'Casodex' in sexual interest and physical capacity. 'Casodex' had a substantially lower incidence of hot flushes compared to castration (6-13% compared with 39-44%) and the most commonly reported adverse events were those expected for a potent antiandrogen. However, in patients with M0 disease at entry, the data are still immature with only 13% of M0 patients having died. An initial analysis of this immature data has suggested that the results in these patients may be different to those obtained in patients with M1 disease. A further survival analysis in patients with M0 disease is therefore planned when the data are more mature. 'Casodex' 150 mg is less effective than castration in patients with M1 disease. However, 'Casodex' has shown a benefit in terms of quality of life and subjective response when compared to castration and has an acceptable tolerability profile. Thus 'Casodex' 150 mg monotherapy is an option for patients with M1 prostate cancer for whom surgical or medical castration is not indicated or is not acceptable. \n",
      "\n",
      "\n",
      "**************************************************\n",
      "Relation Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 12979.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tClassification:\n",
      "\t\t# Ground truth relations: (0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0)\n",
      "\t\t# Predicted relations labels: [0 0 0 0 1 0 1 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "infer(test_set, indexer, comp_quantifier=None, comp_classifier=None, rel_quantifier=None, rel_classifier=cnn_classifier, filename=None, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc37da-6239-4833-9c63-03cac51fdb78",
   "metadata": {},
   "source": [
    "We obtain worse results if compared with the components model: this is expected, as the number of positive labels is now a subgroup of the positive labels used to train the previous model, leading to an imbalanced dataset and a more difficult task.\n",
    "\n",
    "### 3. QuaNet \n",
    "We will include the description of `QuaNet` again, as it may prove useful. `QuaNet` observes the classification predictions to learn higher-order *quantification embeddings*, which are then refined by incorporating quantification predictions of simple classify-and-count-like methods.\n",
    "\n",
    "![architecture](./images/quanet_architecture.png)\n",
    "\n",
    "The QuaNet architecture (see Figure 1) consists of two main components: a **recurrent component** and a **fully connected component**.\n",
    "\n",
    "#### 3.1 Recurrent Component: Bidirectional LSTM\n",
    "- The core of the model is a **Bidirectional LSTM** (Long Short-Term Memory), a type of recurrent neural network. \n",
    "- The LSTM receives as input a **list of pairs** $⟨Pr(c|x), \\vec{x}⟩$, where:\n",
    "  - $Pr(c|x)$ is the probability that a classifier $h$ assigns class $c$ to document $x$.\n",
    "  - $\\vec{x}$ is the **document embedding**, a vector representing the document's content.\n",
    "- The list is **sorted by the value of $Pr(c|x)$**, meaning the documents are arranged from least to most likely to belong to class $c$.\n",
    "  \n",
    "The **intuition** behind this approach is that the LSTM will \"learn to count\" positive and negative examples. By observing the ordered sequence of probabilities, the LSTM should learn to recognize the point where the documents switch from negative to positive examples. The document embedding $\\vec{x}$ helps the LSTM assign different importance to each document when making its prediction.\n",
    "\n",
    "The output of the LSTM is called a **quantification embedding**—a dense vector representing the information about the quantification task learned from the input data.\n",
    "\n",
    "#### 3.2 Fully Connected Component\n",
    "- The vector returned by the LSTM is combined with additional information, specifically **quantification-related statistics**:\n",
    "  - $\\hat{p}_c^{CC}(D)$, $\\hat{p}_c^{ACC}(D)$, $\\hat{p}_c^{PCC}(D)$, and $\\hat{p}_c^{PACC}(D)$, which are quantification predictions from different methods.\n",
    "  - $tpr_b$, $fpr_b$, $tpr_s$, and $fpr_s$, aggregate statistics related to true positive and false positive rates, which are easy to compute from the classifier $h$ using a validation set.\n",
    "\n",
    "This combined vector then passes through the second part of the architecture, which is made up of **fully connected layers** with **ReLU activations**. These layers adjust the quantification embedding using the additional statistics from the classifier to improve the accuracy of the quantification.\n",
    "\n",
    "The final output is a prediction $\\hat{p}_c^{QuaNet}(c|D)$, which represents the probability of class $c$ for the dataset $D$, produced by a **softmax layer**.\n",
    "\n",
    "QuaNet could use quantification predictions from many methods, but it focuses on those that are **computationally efficient** (like CC, ACC, PCC, and PACC). This ensures that the process remains fast while still providing sufficient information for accurate predictions.\n",
    "\n",
    "### Details\n",
    "\n",
    "| Layer | Type | Dimensions | Activation | Dropout |\n",
    "|---|---|---|---|---|\n",
    "| Input | LSTM | 128 | N/A | N/A |\n",
    "| Dense 1 | Dense | 1024 | ReLU | 0.5 |\n",
    "| Dense 2 | Dense | 512 | ReLU | 0.5 |\n",
    "| Output | Dense | 2 | Softmax | N/A |\n",
    "\n",
    "- The LSTM has **64 hidden dimensions**, and since it’s bidirectional, the final LSTM output has **128 dimensions**.\n",
    "- This LSTM output is concatenated with the **8 quantification statistics** (giving a total of 136 dimensions), which is then fed into:\n",
    "  - **Two dense layers** with **1,024** and **512 dimensions**, each using **ReLU activation** and **0.5 dropout**.\n",
    "  - Finally, the output is passed through a **softmax layer** of size 2 to make the final class prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc527bb1-dec8-4f00-b777-f3d407f93165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuaNetModule(\n",
      "  (lstm): LSTM(102, 64, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (ff_layers): ModuleList(\n",
      "    (0): Linear(in_features=136, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/500 [00:00<?, ?it/s]C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\method\\_neural.py:233: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  ptrue = torch.as_tensor([sample_data.prevalence()], dtype=torch.float, device=self.device)\n",
      "[QuaNet] epoch=1 [it=499/500]\ttr-mseloss=0.00435 tr-maeloss=0.03707\tval-mseloss=-1.00000 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.10it/s]\n",
      "[QuaNet] epoch=2 [it=499/500]\ttr-mseloss=0.00083 tr-maeloss=0.02045\tval-mseloss=0.00020 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.14it/s]\n",
      "[QuaNet] epoch=3 [it=499/500]\ttr-mseloss=0.00081 tr-maeloss=0.01831\tval-mseloss=0.00007 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.06it/s]\n",
      "[QuaNet] epoch=4 [it=499/500]\ttr-mseloss=0.00031 tr-maeloss=0.01077\tval-mseloss=0.00007 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.51it/s]\n",
      "[QuaNet] epoch=5 [it=499/500]\ttr-mseloss=0.00013 tr-maeloss=0.00746\tval-mseloss=0.00012 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.74it/s]\n",
      "[QuaNet] epoch=6 [it=499/500]\ttr-mseloss=0.00028 tr-maeloss=0.01069\tval-mseloss=0.00003 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.47it/s]\n",
      "[QuaNet] epoch=7 [it=499/500]\ttr-mseloss=0.00025 tr-maeloss=0.01055\tval-mseloss=0.00050 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.22it/s]\n",
      "[QuaNet] epoch=8 [it=499/500]\ttr-mseloss=0.00027 tr-maeloss=0.01033\tval-mseloss=0.00002 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.89it/s]\n",
      "[QuaNet] epoch=9 [it=499/500]\ttr-mseloss=0.00037 tr-maeloss=0.01121\tval-mseloss=0.00035 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.70it/s]\n",
      "[QuaNet] epoch=10 [it=499/500]\ttr-mseloss=0.00041 tr-maeloss=0.01160\tval-mseloss=0.00009 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 50.98it/s]\n",
      "[QuaNet] epoch=11 [it=499/500]\ttr-mseloss=0.00002 tr-maeloss=0.00283\tval-mseloss=0.00002 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 53.93it/s]\n",
      "[QuaNet] epoch=12 [it=499/500]\ttr-mseloss=0.00018 tr-maeloss=0.00696\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.98it/s]\n",
      "[QuaNet] epoch=13 [it=499/500]\ttr-mseloss=0.00025 tr-maeloss=0.00880\tval-mseloss=0.00026 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.23it/s]\n",
      "[QuaNet] epoch=14 [it=499/500]\ttr-mseloss=0.00020 tr-maeloss=0.00815\tval-mseloss=0.00003 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.55it/s]\n",
      "[QuaNet] epoch=15 [it=499/500]\ttr-mseloss=0.00030 tr-maeloss=0.00864\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.04it/s]\n",
      "[QuaNet] epoch=16 [it=499/500]\ttr-mseloss=0.00026 tr-maeloss=0.00864\tval-mseloss=0.00018 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 45.13it/s]\n",
      "[QuaNet] epoch=17 [it=499/500]\ttr-mseloss=0.00021 tr-maeloss=0.00717\tval-mseloss=0.00014 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.09it/s]\n",
      "[QuaNet] epoch=18 [it=499/500]\ttr-mseloss=0.00016 tr-maeloss=0.00647\tval-mseloss=0.00009 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 49.77it/s]\n",
      "[QuaNet] epoch=19 [it=499/500]\ttr-mseloss=0.00020 tr-maeloss=0.00797\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 50.73it/s]\n",
      "[QuaNet] epoch=20 [it=499/500]\ttr-mseloss=0.00012 tr-maeloss=0.00619\tval-mseloss=0.00016 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 47.71it/s]\n",
      "[QuaNet] epoch=21 [it=499/500]\ttr-mseloss=0.00010 tr-maeloss=0.00462\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 42.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended by patience exhausted; loading best model parameters in ../checkpoints/relations\\Quanet-Relations for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\method\\_neural.py:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.quanet.load_state_dict(torch.load(checkpoint))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>QuaNetTrainer(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "              classifier__device=device(type=&#x27;cpu&#x27;), classifier__drop_p=0.5,\n",
       "              classifier__embedding_size=195, classifier__epochs=200,\n",
       "              classifier__hidden_size=278, classifier__kernel_heights=[3, 5, 7],\n",
       "              classifier__lr=0.001, classifier__padding_length=107,\n",
       "              classifier__patience=10, classifier__repr_size=100,\n",
       "              classifier__stride=1, classifier__weight_decay=0, qdrop_p=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;QuaNetTrainer<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>QuaNetTrainer(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "              classifier__device=device(type=&#x27;cpu&#x27;), classifier__drop_p=0.5,\n",
       "              classifier__embedding_size=195, classifier__epochs=200,\n",
       "              classifier__hidden_size=278, classifier__kernel_heights=[3, 5, 7],\n",
       "              classifier__lr=0.001, classifier__padding_length=107,\n",
       "              classifier__patience=10, classifier__repr_size=100,\n",
       "              classifier__stride=1, classifier__weight_decay=0, qdrop_p=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "QuaNetTrainer(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "              classifier__device=device(type='cpu'), classifier__drop_p=0.5,\n",
       "              classifier__embedding_size=195, classifier__epochs=200,\n",
       "              classifier__hidden_size=278, classifier__kernel_heights=[3, 5, 7],\n",
       "              classifier__lr=0.001, classifier__padding_length=107,\n",
       "              classifier__patience=10, classifier__repr_size=100,\n",
       "              classifier__stride=1, classifier__weight_decay=0, qdrop_p=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train QuaNet (alternatively, we can set fit_classifier=True and let QuaNet train the classifier)\n",
    "set_seed(42)\n",
    "\n",
    "quantifier = QuaNet(cnn_classifier, qp.environ['SAMPLE_SIZE'], qdrop_p=0, device='cpu', checkpointdir='../checkpoints/relations', checkpointname='Quanet-Relations')\n",
    "quantifier.fit(abs_dataset.training, fit_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcabfb51-13e9-4469-9060-e46085b29ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(collection, quantifier, n):\n",
    "    \"\"\"\n",
    "    Evaluates the estimated distributions of the model both by document (filename) and using standard sampling.\n",
    "\n",
    "    Parameters:\n",
    "        collection (object): The collection containing the samples.\n",
    "        quantifier (object): The method used to evaluate.\n",
    "        n (int or list): The maximum number of files to group together for evaluation. Can be a single integer or a list of integers.\n",
    "\n",
    "    Returns:\n",
    "        dict: Averaged error measures for the estimated distributions (both by document and standard).\n",
    "    \"\"\"\n",
    "    set_seed(42)\n",
    " \n",
    "    filename_to_instances = defaultdict(list)\n",
    "    filename_to_labels = defaultdict(list)\n",
    "\n",
    "    for i in range(len(collection.instances)):\n",
    "        filename = collection.filenames[i]\n",
    "        filename_to_instances[filename].append(collection.instances[i])\n",
    "        filename_to_labels[filename].append(collection.labels[i])\n",
    "\n",
    "    # Evaluate by standard sampling technique \n",
    "    true_dist_all = collection.prevalence()\n",
    "    estim_dist_all = quantifier.quantify(collection.instances)\n",
    "    \n",
    "    std_ae = qp.error.ae(true_dist_all, estim_dist_all)\n",
    "    std_rae = qp.error.rae(true_dist_all, estim_dist_all)\n",
    "    std_mse = qp.error.mse(true_dist_all, estim_dist_all)\n",
    "    std_mae = qp.error.mae(true_dist_all, estim_dist_all)\n",
    "    std_mrae = qp.error.mrae(true_dist_all, estim_dist_all)\n",
    "    std_mkld = qp.error.mkld(true_dist_all, estim_dist_all)\n",
    "\n",
    "    results = {\n",
    "        'Std_AE': std_ae,\n",
    "        'Std_RAE': std_rae,\n",
    "        'Std_MSE': std_mse,\n",
    "        'Std_MAE': std_mae,\n",
    "        'Std_MRAE': std_mrae,\n",
    "        'Std_MKLD': std_mkld,\n",
    "    }\n",
    "    \n",
    "    n_values = [n]  if isinstance(n, int) else n\n",
    "    \n",
    "    header = ['Error Metric', 'Standard'] + [f'ByDoc (n={i})' for i in n_values]\n",
    "    table_data = {\n",
    "        'AE': [std_ae],\n",
    "        'RAE': [std_rae],\n",
    "        'MSE': [std_mse],\n",
    "        'MAE': [std_mae],\n",
    "        'MRAE': [std_mrae],\n",
    "        'MKLD': [std_mkld]\n",
    "    }\n",
    "        \n",
    "    # Evaluate by filename, custom sampling method, in batches of size batch_size\n",
    "    # Iterate through each batch size from 1 to n\n",
    "    for batch_size in n_values:\n",
    "        total_ae, total_rae, total_mse = 0.0, 0.0, 0.0\n",
    "        total_mae, total_mrae, total_mkld = 0.0, 0.0, 0.0\n",
    "        \n",
    "        filenames = list(set(collection.filenames))\n",
    "        n_batches = (len(filenames) + batch_size - 1) // batch_size \n",
    "\n",
    "        for i in range(0, len(filenames), batch_size):\n",
    "            batch_filenames = filenames[i:i + batch_size]\n",
    "            \n",
    "            # Gather instances and labels for the current batch\n",
    "            batch_instances = []\n",
    "            batch_labels = []\n",
    "            for filename in batch_filenames:\n",
    "                batch_instances.extend(filename_to_instances[filename])\n",
    "                batch_labels.extend(filename_to_labels[filename])\n",
    "\n",
    "            # True distribution for the current batch\n",
    "            true_dist = np.bincount(batch_labels, minlength=2) / len(batch_labels)\n",
    "            \n",
    "            # Estimated distribution from quantifier\n",
    "            print(batch_filenames)\n",
    "            print(batch_instances)\n",
    "            print(batch_labels)\n",
    "            estim_dist = quantifier.quantify(batch_instances)\n",
    "            \n",
    "            total_ae += qp.error.ae(true_dist, estim_dist)\n",
    "            total_rae += qp.error.rae(true_dist, estim_dist)\n",
    "            total_mse += qp.error.mse(true_dist, estim_dist)\n",
    "            total_mae += qp.error.mae(true_dist, estim_dist)\n",
    "            total_mrae += qp.error.mrae(true_dist, estim_dist)\n",
    "            total_mkld += qp.error.mkld(true_dist, estim_dist)\n",
    "\n",
    "        # Average errors for the current batch size\n",
    "        avg_ae = total_ae / n_batches if n_batches > 0 else 0\n",
    "        avg_rae = total_rae / n_batches if n_batches > 0 else 0\n",
    "        avg_mse = total_mse / n_batches if n_batches > 0 else 0\n",
    "        avg_mae = total_mae / n_batches if n_batches > 0 else 0\n",
    "        avg_mrae = total_mrae / n_batches if n_batches > 0 else 0\n",
    "        avg_mkld = total_mkld / n_batches if n_batches > 0 else 0\n",
    "\n",
    "        # Append average results to the corresponding metric\n",
    "        table_data['AE'].append(avg_ae)\n",
    "        table_data['RAE'].append(avg_rae)\n",
    "        table_data['MSE'].append(avg_mse)\n",
    "        table_data['MAE'].append(avg_mae)\n",
    "        table_data['MRAE'].append(avg_mrae)\n",
    "        table_data['MKLD'].append(avg_mkld)\n",
    "\n",
    "        # Update results dictionary\n",
    "        results.update({\n",
    "            f'ByDoc_AE_{batch_size}': avg_ae,\n",
    "            f'ByDoc_RAE_{batch_size}': avg_rae,\n",
    "            f'ByDoc_MSE_{batch_size}': avg_mse,\n",
    "            f'ByDoc_MAE_{batch_size}': avg_mae,\n",
    "            f'ByDoc_MRAE_{batch_size}': avg_mrae,\n",
    "            f'ByDoc_MKLD_{batch_size}': avg_mkld\n",
    "        })\n",
    "\n",
    "    print(\"\\t\".join(header))\n",
    "    print(\"\\t\".join([\"-\" * len(h) for h in header]))\n",
    "\n",
    "    for metric in table_data:\n",
    "        row = [metric] + table_data[metric]\n",
    "        print(\"\\t\".join(f\"{value:<15.4f}\" if isinstance(value, float) else f\"{value:<15}\" for value in row))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48ebae5b-2648-4879-a40f-ab088ea428cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.str_('18809617')]\n",
      "[[4353, 2203, 2410, 5153, 3803, 4781, 3661, 842, 0, 3272, 2832, 5264, 5374, 5667, 1297, 4638, 5303, 1018, 1154, 3536], [100, 4095, 1275, 2832, 4353, 4745, 2446, 1018, 5614, 1499, 1338, 4840], [4353, 5614, 946, 5667, 5240, 2167, 3862, 2410, 4577, 842, 5376, 3803, 1180, 4360, 3803, 3217, 4369, 1156, 842, 5383, 4936, 1287, 963, 1018, 842, 963, 5632, 842, 2410, 5264, 5652, 4442, 1297, 842, 963, 2403, 5489, 3536, 130, 191, 272, 322, 842, 363], [5240, 2353, 3803, 5261, 5383, 2869, 5238, 5240, 3641, 1992, 3803, 697, 1297, 3822, 4353, 893, 5303, 1028, 5217, 842, 5238, 2827, 5667, 4637, 5303, 1018, 2461, 917, 3223, 2832, 3544, 3966], [2304, 3625, 842, 5600, 5681], [697, 1297, 2410, 1963, 4976, 3698, 4888, 1253, 3286, 1180, 3755, 3054, 3748, 5240, 4983, 3803, 1205, 1152, 5251, 3054, 3245, 2906, 4493, 3074, 2802, 3822, 4360, 3803, 3217, 4353]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('22120301')]\n",
      "[[700, 3803, 5240, 5526, 5614, 0, 5672, 5303, 3536, 4123, 5667, 100, 0, 2533, 3899, 3536], [5240, 5125, 2428, 5614, 4841, 2663, 2832, 5240, 5526, 2572, 1017, 3822, 3099, 3407, 830], [3966, 156, 5303, 526, 5693, 3803, 737, 5667, 3673, 1791, 2495, 842, 3101, 4006, 4723, 3123, 4743, 526, 5652, 5637, 4443, 2921, 5376, 5637, 2074, 842, 5637, 4395, 3005, 2573, 1940, 5240, 5692, 5042, 4012], [5240, 3391, 3875, 2410, 5240, 5526, 2572, 5614, 327, 3536, 842, 2410, 5240, 1533, 2572, 5614, 164, 3536, 13], [5240, 3391, 4261, 2439, 5122, 4031, 2410, 5240, 5526, 2572, 5614, 3536, 842, 3536, 2410, 5240, 1533, 2572, 90], [2403, 5489, 5287, 4399, 2446, 156, 5303, 467, 3536, 3391, 341, 3536], [3069, 4760, 5303, 1028, 4698, 842, 3074, 3267, 5222, 1991, 3054, 0, 3803, 2471, 3028]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('18064563')]\n",
      "[[4888, 3759, 3803, 4395, 1534, 5384, 2614, 2198, 5240, 1994, 3803, 872, 1403, 5303, 5238, 3803, 4068, 2832, 1180, 3966, 1152, 4916, 4623, 2614, 1035, 1818, 5303, 2990, 1935, 5303, 2659, 3965, 2572], [5041, 2614, 4833, 5238, 5251, 3054, 2662, 4207, 3803, 1741, 2832, 1180, 3966], [2832, 5240, 1624, 3028, 5625, 4747, 3673, 1791, 1963, 4976, 1135, 1180, 3966, 2410, 1742, 5153, 4224, 5303, 5240, 2925, 3803, 697, 5250, 842, 3026, 5646, 5240, 3857, 871, 2388, 726, 1742, 5153, 1420, 3803, 697, 5376, 842, 4360, 3803, 3217]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15888157')]\n",
      "[[4395, 1534, 5383, 3803, 2364, 2662, 0, 5377, 2998, 5601, 2364, 3274, 0, 5377, 1533, 5614, 4007, 3899, 3536, 2832, 3622, 2631, 4797, 0, 2695], [4322, 4360, 3803, 3217, 789, 2825, 3539, 2832, 5240, 2998, 2572, 2446, 434, 5303, 486, 5601, 437, 5303, 442, 2832, 5240, 1533, 2572, 567, 100, 77], [4769, 4563, 5287, 0, 3822, 2598, 4551, 1683, 2446, 122, 5303, 246, 3465, 3995, 5629, 2832, 5240, 2998, 2572, 842, 2446, 590, 5303, 467, 3465, 2832, 5240, 1533, 2572, 556, 100, 77], [5303, 2174, 5240, 2802, 3803, 3152, 5376, 3822, 5240, 4810, 3803, 2275, 2669, 842, 3822, 4322, 3538, 2832, 5675, 5667, 3978], [5240, 1275, 5614, 4841, 2567, 2832, 5240, 2998, 2572, 0, 566, 272, 77], [3152, 5376, 895, 5303, 4476, 5240, 4810, 3803, 2275, 2598, 842, 5287, 0, 3822, 2598, 4551, 932, 5635, 932, 775, 1741, 842, 888, 2832, 5675, 5667, 3978]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('9890172')]\n",
      "[[3033, 1097, 5303, 5240, 4623, 5019, 5240, 5383, 1963, 1031, 3803, 3274, 2076, 4409], [2446, 914, 217, 5303, 3747, 218, 201, 3803, 5240, 356, 5190, 3966, 5637, 4397, 952], [5122, 1627, 5637, 0, 842, 5637, 1403, 5667, 5240, 3342, 2597, 5230], [4527, 2617, 3803, 1672, 842, 597, 1466, 2997, 1320, 5637, 2148, 1154, 5240, 1587, 3503], [5625, 2253, 5240, 4360, 3803, 3217, 4353, 3803, 5077, 3966, 2832, 3575, 4395, 5383, 5238, 1403, 5581, 5376, 5667, 5105, 1205, 783], [2028, 3966, 5637, 526, 5693, 3803, 737, 3856, 3816, 2591, 4976, 3077, 3856, 2773, 3755, 5238, 5614, 2890, 2410, 4386, 842, 2591, 4006, 4998, 3803, 4998, 3803, 2459, 662, 5303, 4998, 3803, 0, 3803, 772, 4769, 1205, 1152, 5432, 5303, 5676], [5240, 4527, 2616, 3803, 1672, 2410, 5581, 5374, 3966, 5614, 502, 597, 1313, 417, 593]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('22412149')]\n",
      "[[814, 2987, 1017, 1984, 4255, 1017, 3822, 0, 2579, 842, 0, 3370, 2641, 3966, 3331, 1597], [5625, 1017, 5240, 4255, 3822, 5240, 1597, 2579, 3803, 5240, 3622, 1433, 1180, 0, 0, 842, 2853, 5240, 0, 3503, 0], [2987, 1017, 5176, 1984, 4255, 3054, 1989, 2410, 1850, 2439, 1180, 5126, 5667, 1180, 4523, 2304, 1597]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16416368')]\n",
      "[[2832, 1530, 5240, 2065, 3640, 3803, 3328, 5404, 917, 781, 0, 3423, 1154, 2542], [2301, 2439, 3360, 4831, 814, 2979, 1811, 2832, 2306, 3803, 2572, 733, 2006, 5632, 77], [4808, 942, 3803, 5240, 4360, 3803, 3217, 5637, 4410, 5303, 1028, 1059, 2832, 2572, 5237, 2832, 2572], [5411, 5270, 3509, 0, 3966, 5667, 2492, 1199, 5637, 4395, 5303, 4441, 2009, 1540, 1804, 0, 355, 3716, 3103, 842, 3803, 4294, 3109, 3995, 1663, 2572, 119, 3856, 2301, 2073, 931, 3241, 1804, 224, 3716, 3103, 3109, 3995, 1663, 4087, 3731, 0, 2572, 130, 2410, 4012, 3803, 2006, 5632, 2446, 5240, 2358, 5303, 5240, 5259, 1297, 1634]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('19826172')]\n",
      "[[5240, 2508, 3234, 3503, 5614, 5509, 2410, 1526, 3887, 5649, 830, 3803, 0, 1060, 1225, 3888, 5614, 4007, 932, 830, 3803, 0, 0, 2832, 0, 0], [0, 3966, 5667, 1180, 535, 3414, 206, 5675, 3379, 737, 422, 5693, 4398, 224, 502, 4569, 246, 1792], [5098, 3583, 2217, 2998, 2844, 2662, 842, 3274, 2962, 1427, 5614, 2318, 842, 1573, 0, 1028, 5509, 2832, 3966, 5667, 5542, 1181, 5652, 5637, 4443, 697, 1297, 3856, 5376, 2410, 711, 1850]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21149658')]\n",
      "[[2711, 2175, 5303, 5693, 733, 5376, 4831, 814, 712, 2410, 1148, 2832, 5499, 842, 4812, 1891, 842, 2832, 3965, 4716], [4605, 4409, 5614, 582], [5240, 803, 0, 3803, 5113, 3827, 2572, 4039, 2771, 5116, 4291, 5562, 2995, 4377, 2998, 5383, 1405, 4378, 4291, 4680, 842, 1129, 1148, 1347, 733, 5693, 1935, 5303, 4103, 638]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23549581')]\n",
      "[[5042, 2403, 5489, 5614, 1416, 3093, 0], [0, 4572, 5238, 3966, 2614, 2559, 3856, 2663, 4781, 3661, 635, 5303, 5240, 3627, 1389, 0, 1598, 2410, 715, 2183, 842, 963, 3180, 3822, 4723, 3803, 5303, 100, 4569, 994, 1297, 2886, 3921, 733, 3917, 3881, 5196, 3856, 3908, 5376]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('20885302')]\n",
      "[[3691, 1207, 5539, 1276, 5648, 3370, 2869, 3637, 2410, 2395, 3822, 1208], [3691, 3881, 3965, 3888, 5150, 1865, 4948, 3640, 3856, 1207, 3888, 1741, 5103, 4948, 3640, 5637, 4841, 1813], [5161, 948, 3803, 1741, 3054, 3638, 2832, 2693, 3966]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16823511')]\n",
      "[[3966, 3822, 1911, 1906, 923, 5637, 4841, 5680, 2832, 5226, 3803, 994, 842, 3391, 3921, 1275, 4745], [1602, 4757, 1407, 4831, 4995, 4840, 1812, 2410, 4916, 1891, 3072, 963, 4936, 948, 5289, 5667, 772, 1812, 0, 5240, 1906, 783, 923], [1911, 5228, 4087, 1908, 1906, 1725, 4840, 2826, 2832, 5122, 5562, 1906, 2832, 4039, 2771, 1337, 5383, 2832, 711, 1135, 1180], [1813, 828, 5695, 4877, 1813, 1453]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('23439759')]\n",
      "[[4224, 5041, 2614, 5083, 5238, 3409, 2444, 5509, 0, 3401, 1176, 0, 5634, 3272, 854, 842, 2304, 2832, 3966, 5667, 1180], [5625, 4007, 4395, 1901, 1096, 304, 1663, 5383, 3803, 3409, 224, 3445, 5562, 4068, 2832, 3966, 5667, 711, 3286, 3856, 2528, 1180, 897, 4745, 3822, 5303, 100, 4723, 100, 5683, 897, 842, 2678, 3803, 5634, 3272], [5254, 5041, 5637, 3230, 1154, 3131, 3803, 1098, 842, 620, 3803, 4068, 1536], [949, 2842, 5634, 5153, 1154, 5240, 1979, 5150, 948, 4723, 842, 4360, 3803, 3217, 1154, 5240, 2462, 948, 3803, 854, 1161, 5250, 2273, 4369], [5240, 4217, 4337, 3803, 5261, 5042, 5614, 5303, 1402, 3409, 5667, 4068, 2410, 897, 2826, 2832, 3966, 5667, 1180, 1161], [733, 2980, 830, 3803, 427, 3966, 5240, 5042, 5614, 1347, 2410, 2473]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('9554521')]\n",
      "[[5251, 5637, 3946, 4606, 191, 597, 1466, 2996, 1313, 144, 2832, 5042, 842, 3946, 4606, 246, 597, 1313, 156, 2832, 5042], [5260, 4862, 3966, 2832, 5042, 4442, 1114, 524, 3289, 101, 3445, 3303, 2404, 1154, 1114, 2453, 370, 3445, 3303, 3822, 1664, 842, 363, 3882, 2832, 5042, 4442, 2098, 322, 3445, 3303, 3822, 1664, 842, 2160, 101, 3445, 3303, 3822, 1664, 842, 1321, 322, 3445, 3303, 3822, 1664, 842, 842, 3271, 165, 3445, 1663], [5251, 3054, 3691, 4983, 5250, 2410, 3966, 5667, 711, 2486, 1198], [797, 3344, 1927, 1378, 5248, 2614, 1035, 4285], [2832, 5240, 5653, 4105, 3803, 3966, 5122, 5614, 4841, 2663, 2832, 3966, 5667, 4312, 3803, 77]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('17912635')]\n",
      "[[4121, 5675, 5667, 2692, 4446, 4108, 1135, 1180, 197, 5637, 4395, 5303, 3202, 3445, 3856, 4068, 3823, 1656, 2410, 5693], [1337, 1053, 2844, 814, 3900, 5122, 712, 5637, 789, 4761, 2832, 5675, 5652, 1603, 3899, 2446, 4068, 5303, 3202, 733, 5438, 2872, 5238, 5404, 4541, 4779, 5303, 2692, 5250, 1760, 4269, 4012, 4850, 1843, 3803, 5184]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('16384850')]\n",
      "[[733, 5240, 700, 3803, 5240, 146, 1896, 5240, 4409, 3803, 4377, 1331, 2446, 1104, 5281, 4549, 842, 5653, 1111, 5614, 3385], [4655, 4184, 3966, 3314, 2663, 4360, 3803, 3217, 842, 4442, 3197, 4377, 2254, 5303, 5240, 1104], [4755, 1599, 2410, 616, 814, 4655, 5010, 4795, 0, 1445, 3197, 5237, 3675, 3490, 5614, 0, 1154, 265, 3803, 272, 599, 2170, 3966, 842, 191, 3803, 246, 574, 2740, 3966, 0], [5240, 4158, 4217, 1599, 2410, 5075, 616, 5614, 3691, 0, 5494, 2832, 5240, 5281, 0, 3856, 2759, 0, 0, 5494, 3197, 5237, 3822, 3635, 4728, 4007, 3536, 733, 5250, 842, 5614, 4717, 2832, 101, 3803, 3966, 2832, 1121, 2573], [733, 5115, 2410, 1816, 5281, 1198, 3344, 3966, 917, 5374, 5667, 4382, 5303, 615, 4549, 5281, 5291]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('18786981')]\n",
      "[[3966, 2533, 2497, 4442, 3539, 1297, 932, 3892, 584, 2497, 5601, 505, 3984, 3803, 5376, 1635], [2497, 3054, 932, 1989, 932, 3984, 2832, 5226, 3803, 3900, 5122, 842, 4261, 2439, 5122, 842, 2613, 5337, 4248, 3539, 629, 5303, 3966], [1152, 5254, 5637, 3741, 957, 5667, 2857, 2695, 702, 2894, 3856, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('21831722')]\n",
      "[[3691, 4840, 1811, 2832, 889, 3803, 5254, 3388, 5614, 3744, 2832, 5240, 1533, 2572], [5411, 3966, 1416, 5240, 5042]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('10653877')]\n",
      "[[5667, 5240, 2205, 3803, 2857, 2564, 3822, 5240, 3274, 1896, 3917, 4495, 842, 2857, 0, 3666, 842, 4112, 2857, 5376, 4523, 1201, 2183, 5667, 2662, 1896, 3917, 5337, 5614, 4843, 654, 772, 5270, 924], [5096, 5122, 5614, 3781, 5667, 5240, 1380, 3917, 4496, 3391, 5122, 5287, 3536, 5692, 5122, 4409, 374, 1403, 5667, 2160, 4087, 1321, 3391, 5122, 5287, 3536, 5692, 5122, 4409, 327, 75], [1960, 4495, 5614, 4556, 2186, 246, 1664, 842, 1960, 2842, 1321, 543, 3445], [5240, 5042, 5614, 1215, 3886, 1154, 3574, 2943, 1548, 2572, 2832, 1297, 3617, 4976, 2773, 5303, 3077, 3755, 3966, 4395, 5303, 4441, 3917, 4087, 1321, 3856, 2160, 4087, 1321]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('18802161')]\n",
      "[[5303, 1773, 5240, 1991, 3803, 5240, 775, 1741, 808, 3966, 5667, 1180, 673, 1368, 1205, 3334, 2410, 3317, 1741, 3856, 1955], [2998, 3966, 789, 2241, 2567, 4411, 3803, 1741, 5376, 532, 100, 3803, 2162, 3966, 4, 842, 4841, 1059, 4360, 3803, 3217, 3888, 2844, 4902, 2293, 693, 3379, 1811, 1060, 2573, 597, 1313, 257, 5303, 183, 9, 2045, 693, 3379, 1811, 311, 597, 1313, 292, 5303, 257, 32, 2462, 693, 3379, 1811, 348, 597, 1313, 94, 5303, 477, 67, 842, 4048, 5635, 1045, 693, 3379, 1811, 553, 597, 1313, 434, 5303, 45], [963, 130, 3536, 493, 3803, 2998, 3966, 2591, 437, 3856, 2567, 4480, 2832, 1742, 5153, 2446, 1018, 932, 946, 1154, 5240, 3965, 2631, 4369, 4047, 1741, 4723, 1403, 5667, 437, 3803, 2162, 3966, 3796, 4415, 3856, 602, 597, 1313, 173, 5303, 374, 32]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('17947724')]\n",
      "[[5153, 5637, 1389, 2832, 1121, 2573], [2216, 3966, 4563, 3539, 1115, 3590, 644, 4, 5527, 1930, 7, 842, 1820, 4875, 58], [963, 130, 3536, 2216, 5614, 957, 5667, 2336, 2701, 2370, 842, 3197, 5527, 1840, 5237, 5184, 1152, 5667, 3539, 5527, 1930, 1115, 3590, 644, 842, 1820, 4875], [1960, 5287, 4012, 5614, 5231, 0, 736, 5240, 1018, 814, 3900, 5535, 5614, 952, 2410, 1960, 4563, 5150], [2691, 1135, 1180, 5376, 2858, 3415, 5153, 2832, 5675], [2701, 2369, 4743, 5614, 1166, 963, 1960, 5287, 4095]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('11830607')]\n",
      "[[711, 737, 783, 4828, 3741, 0, 910, 3755, 5376], [2111, 1684, 3899, 5287, 2832, 2462, 5635, 1045, 3791, 2832, 1121, 2573], [3207, 9, 842, 3662, 5337, 13, 5637, 3539, 1389, 2832, 2012, 3414, 5237, 2832, 5698, 3414]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('19436289')]\n",
      "[[5261, 5042, 1404, 5240, 1337, 1991, 842, 1569, 1991, 3803, 4769, 0, 0, 0, 4772, 5667, 3881, 3929, 5248, 5303, 751, 1341, 2832, 3321, 814, 2188, 1017, 5376, 1307], [4755, 3887, 3388, 2842, 5376, 957, 3538, 3542, 5122, 842, 1569, 1991], [1152, 3156, 1425, 5637, 3539, 2443, 733, 4658, 0, 4662, 4415, 422, 597, 1313, 582, 67], [5251, 5637, 3691, 1812, 2832, 5240, 2832, 2695, 3542, 3856, 1963, 1424, 4411], [5625, 1463, 4287, 3575, 4395, 1534, 1337, 5383, 5667, 251, 3966, 2404, 5486, 1672, 3856, 5042, 1350]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('23589316')]\n",
      "[[3011, 700, 3232, 0, 3803, 5557], [5376, 5337, 842, 5240, 4409, 3803, 715, 1992, 842, 3004, 0, 5637, 3741, 4841, 1813, 1060, 5042, 2573], [5240, 5376, 3803, 1533, 2572, 3182, 5303, 1218, 3803, 1415, 4547, 842, 1219, 3803, 3946, 4547, 3321, 5240, 4409, 3803, 1994, 1415, 3946, 4547, 3803], [5512, 3123, 4745, 1277, 2832, 4360, 3803, 3217, 5637, 1403, 1036, 842, 733, 5240, 5376], [5260, 1664, 733, 5240, 5376, 1994, 5337, 842, 715, 1992, 5637, 946, 2832, 1121, 5042, 2573], [5240, 1380, 5376, 2572, 1725, 144, 1219, 3803, 1415, 4547, 842, 257, 1219, 3803, 3946, 4547], [5279, 5240, 4409, 3803, 1994, 5614, 4841, 2663, 2832, 5240, 1380, 5376, 2572, 570, 363, 77, 5601, 1533, 2572]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('12560436')]\n",
      "[[3966, 5637, 2175, 3822, 2364, 3789, 3899, 5240, 2244, 4012], [797, 5240, 2217, 842, 1533, 2572, 3382, 5637, 4841, 1813, 9], [5240, 3397, 3888, 0, 4824, 2412, 363, 5120, 5614, 5509, 5303, 3384, 4360, 3803, 3217, 1036, 842, 733, 5240, 2998], [3419, 2631, 2857, 797, 3741, 4841, 2410, 772, 5054, 0]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('17921185')]\n",
      "[[3691, 0, 4701, 3779, 5637, 4563, 1940, 3837, 3129, 2254], [963, 3536, 842, 5693, 5512, 4113, 2683, 932, 3781, 828, 5240, 3998, 3803, 3966, 597, 1466, 2996, 648, 1658, 4433, 4294, 3274, 1850, 664, 4743, 3856, 842, 1658, 4433, 4294, 1703, 4547, 2857, 2446, 191, 144, 265, 5303, 333, 272, 376, 842, 119, 164, 5303, 224, 144, 292], [4701, 842, 1994, 803, 0, 3803, 0, 652, 1598, 4606, 1658, 4433, 4294, 2608, 1788, 4815, 363, 3397, 3888, 5042, 4873, 4235, 2866, 2304, 5545, 5637, 946, 5277, 5693], [4411, 3803, 4792, 715, 2183, 5637, 282, 842, 265, 3995, 101, 3965, 5693, 2832, 5240, 1901, 1096, 5562, 1620, 4012]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15120769')]\n",
      "[[3691, 1060, 5376, 1812, 5637, 3781, 2410, 1277, 2832, 4360, 3803, 3217, 3856, 4006, 4998], [5240, 5679, 2631, 3863, 5652, 2579, 2410, 5240, 5376, 3803, 1180, 3921, 4454, 0, 821, 932, 2358, 3233, 5250, 4900, 0, 0, 821, 1380, 5667, 0, 821, 932, 4754, 3233, 5250, 842, 4900, 0, 5036, 3845, 5667, 0, 821, 3833, 932, 5259, 3233, 5250], [2175, 3888, 2842, 3921, 2962, 3637, 2410, 1275, 2832, 5250, 4360, 3803, 3217, 3101, 4006, 4998, 2508, 1459, 3803, 5240, 3965, 842, 715, 2183], [5036, 3845, 5637, 4698, 842, 5635, 5316, 5667, 3691, 1779, 3803, 5315, 3856, 4792, 715, 2183], [1152, 3966, 4987, 3822, 5036, 3845, 2591, 4841, 1059, 3921, 4536, 5237, 3966, 5374, 635, 5303, 5652, 2579, 68]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('12431971')]\n",
      "[[5240, 0, 3803, 4687, 2613, 3470, 2802, 3822, 5240, 2461, 3803, 5038, 3966, 2832, 5240, 2358, 5692, 733, 5115]]\n",
      "[np.int64(0)]\n",
      "[np.str_('21439726')]\n",
      "[[5240, 2826, 2832, 3921, 4523, 2807, 5614, 3741, 5133, 963, 842, 130, 5632], [963, 5632, 954, 5303, 5206, 5614, 957, 5667, 2826, 2832, 3921, 4523, 2807, 282, 4096, 3822, 4095, 4723, 597, 1466, 2996, 404, 5303, 84, 32], [5251, 5637, 3691, 4840, 2998, 1154, 5049, 2968, 100]]\n",
      "[np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('17187405')]\n",
      "[[5240, 1344, 4743, 4760, 5303, 4160, 5122, 1059, 5237, 1121, 3814, 842, 1618, 4743], [4915, 2613, 1035, 4833, 5303, 2614, 0, 664, 2832, 2054, 932, 5635, 932, 2832, 0, 3803, 0, 5404], [5240, 5435, 4527, 4662, 2410, 3542, 2832, 5240, 3795, 2572, 1403, 5667, 3966, 2832, 5240, 1533, 2572, 5614, 119, 597, 1313, 546, 493, 477], [797, 0, 5376, 3501, 2614, 1035, 2253, 2832, 3966, 5667, 711, 2621, 5240, 5247, 3854, 917, 5009, 3230]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('24001635')]\n",
      "[[3816, 709, 3966, 4563, 1059, 3900, 4353, 3151, 539, 5601, 490, 0, 2663, 4902, 5635, 1045, 2279, 589, 5601, 566, 72, 842, 2336, 4235, 5667, 848, 4102, 848, 2700, 597, 5601, 574, 55], [3888, 3385, 963, 1018, 842, 5632, 300, 842, 446, 2842, 4353, 3234, 824, 4769, 948, 3151, 2462, 948, 3803, 1180, 5250, 2508, 2279, 842, 3537, 4248, 3803, 3537, 4991, 4102], [3803, 146, 3966, 2832, 5240, 3147, 4395, 1534, 5042, 5625, 4562, 1659, 3822, 454, 2173, 3966, 173, 3816, 0, 842, 374, 5698, 0, 4395, 5303, 5240, 2998], [2832, 5261, 4265, 5240, 4353, 1812, 1060, 3816, 709, 737, 502, 842, 5698, 709, 737, 191, 497, 711, 1180, 3966, 2832, 4605, 5303, 3580, 2998, 1756, 5303, 2824, 4353, 5637, 2198], [3267, 5222, 1053, 2410, 3816, 3966, 5637, 4761, 2832, 5240, 848, 2700, 4723, 963, 5629, 300, 590, 5601, 567, 54, 842, 5629, 446, 599, 5601, 570, 24], [3966, 2240, 4481, 2832, 4360, 3803, 3217, 4353, 5649, 4443, 1180, 5376, 842, 4808, 908, 2614, 1035, 4285, 5303, 681, 4353, 3068], [5264, 2832, 5240, 2998, 2572, 4442, 4862, 3580, 586, 3479, 4801, 1756, 5303, 681, 5240, 2364, 3317, 1891, 3803, 4353]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('16598426')]\n",
      "[[2832, 2559, 3077, 5240, 1563, 3391, 5122, 5287, 5614, 130, 584, 842, 100, 589, 3536, 4597]]\n",
      "[np.int64(1)]\n",
      "[np.str_('15659490')]\n",
      "[[3966, 5637, 5025, 1154, 3415, 842, 2692, 4446, 4998], [2832, 814, 2964, 5303, 5372, 830, 5251, 5637, 3691, 4840, 1812, 1060, 5240, 5413, 5377, 2832, 3391, 5287, 5303, 4261, 2625, 119, 3536, 963, 100, 3536, 508, 1939, 3803, 4605, 2625, 144, 3536, 963, 156, 3536, 602, 842, 3900, 5122, 2625, 292, 3536, 963, 265, 3536, 483]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('19621686')]\n",
      "[[2705, 3009, 1104, 3272, 842, 4122, 2695, 5000, 5637, 3741, 4841, 1813, 5039, 5230], [5368, 842, 5358, 2134, 917, 5413, 1389, 4237, 2410, 2133, 1180, 4581], [4287, 4395, 5042, 5614, 1463, 2446, 3081, 230], [4287, 5041, 1405, 5240, 5413, 3437, 2832, 940, 3994, 917, 2335], [5240, 3841, 5287, 5614, 4841, 3268, 842, 5240, 3176, 4409, 5614, 2663, 2832, 5240, 5368, 2572, 5039, 5230, 842, 0, 2194, 5230, 4597]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('18086760')]\n",
      "[[3540, 0, 1380, 721, 842, 4588, 2217, 4255, 733, 1135, 1180, 1297, 5614, 957, 5667, 2471, 2827, 2832, 3965, 4410, 3888], [2656, 5625, 4562, 3534, 2403, 5489, 3803, 2217, 1039, 842, 3965, 4410, 3888, 2446, 814, 2217, 5383, 2832, 1135, 1180, 3966], [963, 3534, 2403, 5489, 3948, 5637, 3309, 4369, 5238, 946, 4360, 3803, 3217, 4769, 2145, 2304, 888, 1741, 842, 2217, 1039], [3540, 1403, 5667, 3948, 4564, 3691, 4506, 2217, 1940, 5240, 2403, 5489, 4012, 5264, 4564, 4506, 721, 842, 4588, 2217, 789, 4563, 1059, 3965, 4410, 3888, 2844, 4360, 3803, 3217, 693, 3379, 1811, 597, 1313, 183, 52], [2827, 2832, 4769, 2145, 3781, 5667, 4626, 1940, 1135, 1180, 1297, 5637, 3314, 963, 3534, 2403, 5489]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('20003966')]\n",
      "[[3888, 5637, 3397, 1992, 4360, 3803, 3217, 842, 1570], [5261, 5376, 3054, 4169, 2410, 3966, 2237, 5303, 3246, 3197, 5237, 3536], [1031, 3803, 5240, 4888, 5042, 4105, 3833, 2921, 2695, 1570, 5684, 2614, 1035, 4995, 4840, 2759, 5240, 0, 0, 2410, 3585, 5232, 2591, 1035, 901], [1152, 3267, 5222, 4536, 5614, 1059, 733, 2535, 5667, 3539, 3966, 3249, 3539, 1664, 5667, 2552, 4746, 5160, 4743, 3803, 3856, 3539, 5237, 733, 5004, 4069, 532, 5601, 437, 1664, 4597, 77], [5411, 3828, 1259, 2832, 5240, 3652], [5251, 5637, 789, 3691, 1812, 2832, 3391, 5122, 5004, 467, 1664, 5601, 2535, 549, 1664, 842, 4360, 3803, 3217]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('12131085')]\n",
      "[[772, 1425, 5637, 4461, 842, 5254, 4180, 949, 5637, 4556, 963, 3536], [963, 4193, 5240, 4136, 4544, 5240, 0, 2410, 4677, 1337, 4148], [1121, 2573, 5637, 5635, 3364, 2410, 737, 2506, 4979, 697, 5250, 842, 3379, 2403, 5489], [1519, 4746, 853, 3340, 842, 2053, 5429, 949, 5637, 3306, 1036, 5115], [2244, 5115, 3822, 0, 2613, 5083, 5238, 1376, 4136, 1588, 3370, 1028, 5510, 796]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21236344')]\n",
      "[[1811, 1060, 2573, 28, 2410, 1106, 963, 0, 5614, 4840], [2462, 948, 3803, 1180, 5250, 1555, 4369, 2279, 5667, 3074, 679, 1135, 1180, 5060, 5637, 4007, 963, 1018, 842, 130, 5632, 733, 3679], [322, 3966, 4442, 2216, 2214, 282, 3445, 1656, 842, 1252, 0, 386, 3445, 5412, 1656, 2572, 272, 3966, 4442, 2214, 282, 3445, 1656, 2572, 842, 304, 3966, 4442, 3202, 0, 3445, 1656, 2572], [4840, 1812, 3803, 2279, 4743, 29, 842, 2279, 4743, 0, 5637, 3781, 963, 5238, 5287, 4095], [5240, 3379, 0, 2857, 2446, 1018, 2832, 2572, 3966, 1152, 1688, 2446, 1018, 2832, 2572, 3966, 963, 3536, 842, 164, 3536], [863, 925, 5250, 3054, 2821, 2832, 5240, 5376, 3803, 1135, 1180, 2832, 4121, 5675], [0, 1587, 2919, 2614, 1035, 4833, 5303, 1028, 1989, 2832, 1292, 2832, 0, 842, 1337, 5041], [5240, 1277, 2832, 5240, 1106, 1115, 0, 0, 842, 4360, 3803, 3217, 4353, 5637, 834, 842, 4195, 2656], [3691, 4994, 4839, 5614, 2424, 2832, 5240, 2279, 4745, 842, 2279, 4745, 808, 1813, 2573, 963, 1018, 5629, 5629, 842, 5629, 130, 733, 3679], [1106, 3803, 427, 3966, 5637, 834, 265, 0, 5303, 2572, 100, 5303, 2572, 842, 164, 5303, 2572, 5240, 4795, 1115, 0, 0, 1115, 4936, 0, 0, 0, 842, 0, 0, 0, 3803, 5414, 1369, 0, 5637, 3385, 5667, 0, 993, 5230, 0, 1036, 5376, 3536, 842, 164, 3536, 733, 5376]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23254324')]\n",
      "[[737, 526, 119, 5693], [3379, 3921, 4745, 3822, 5240, 1663, 3803, 1840, 5637, 2832, 5240, 2097, 2572, 842, 2832, 5240, 5657, 2572], [1060, 914, 238, 842, 3370, 239, 3966, 5447, 2014, 3168, 2832, 5413, 1263, 5637, 4395, 5303, 819, 5566, 2097, 3856, 5657], [5240, 5074, 3803, 1098, 5614, 946, 5512, 3488, 3437, 907], [3379, 5508, 3803, 679, 819, 3014, 3541, 0, 5614, 130, 3445, 2832, 5240, 5657, 923, 842, 3445, 2832, 5240, 2097, 923], [3965, 1098, 5614, 5075, 2832, 1121, 924], [2421, 2364, 3966, 5637, 2028, 3803, 5654, 348, 5637, 4395, 3379]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('22198049')]\n",
      "[[5240, 5376, 1939, 5614, 3536], [5247, 4297, 957, 5667, 910, 842, 1989, 4322, 842, 4902, 5103, 0, 917, 0, 5303, 0, 5240, 5153, 3803, 3648, 1850, 2832, 0, 3966], [5240, 1380, 5376, 2825, 1121, 0, 793, 842, 3965, 4353], [0, 5248, 2410, 1161, 0, 4828, 1028, 3018, 5672, 1518, 3803, 1057, 5105, 1205, 5238, 2843, 3849, 5150, 3334, 842, 1206, 4325, 1574]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23379385')]\n",
      "[[5625, 2198, 3020, 5267, 618, 1180, 963, 5240, 1018, 948, 932, 4164, 3803, 2304, 4873, 4235, 3921, 1135, 1180, 4936, 5153, 1742, 5153, 3641, 725, 842, 4360, 3803, 3217, 5512, 2574, 1626, 3504, 1535, 2410, 5042, 1459, 842, 3881, 1585], [1659, 0, 2446, 5240, 3552, 1062, 1180, 4320, 2998, 5383, 754, 963, 0, 5240, 5359, 2446, 3965, 5303, 0]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('8229122')]\n",
      "[[5473, 679, 2403, 5489, 1659, 1724, 5122, 712, 963, 2663, 1898, 5240, 174, 3445, 1896, 3054, 3849, 648, 0, 5376, 1988, 5667, 5240, 0, 4835, 1992, 842, 1059, 4360, 3803, 3217], [3966, 5652, 4442, 5240, 557, 3445, 1896, 2330, 1060, 5240, 3274, 842, 2662, 1896, 924, 2832, 4563, 2962, 3803, 1927, 4835, 1992], [963, 3536, 5675, 5374, 5667, 174, 3445, 4563, 3197, 4809, 4835, 1992, 0, 1059, 4048, 2465, 0, 0, 3197, 4321, 1865, 3448, 29, 842, 814, 2826, 2832, 3900, 4360, 3803, 3217, 2378, 16, 2446, 5240, 5287, 3803, 5042, 2084, 932, 1403, 5667, 5264, 5374, 5667, 484, 3445], [5240, 2802, 3803, 5240, 4835, 1992, 3803, 3406, 643, 3822, 5240, 4360, 3803, 3217, 3803, 0, 5675, 5667, 711, 1135, 1180, 5614, 5040, 2832, 1896, 4605, 1337, 5383, 3803, 5240, 1180, 842, 3206, 2572, 0, 0]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('9850014')]\n",
      "[[3691, 4840, 1811, 2832, 5287, 5303, 4261, 3856, 5122, 5614, 2424, 1060, 5240, 5413, 924], [3966, 5374, 5667, 4851, 740, 2404, 1154, 4851, 740, 2591, 4843, 5122, 1152, 3197, 5376, 4523, 5337, 842, 1059, 4353, 932, 1403, 5667, 5264, 5374, 5667, 1251, 2404, 1154, 3599]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('15750360')]\n",
      "[[4226, 3397, 1336, 0, 0], [2701, 2369, 2442, 842, 4810, 0, 3415, 2866, 0, 4824, 2412, 363, 4815, 363], [3147, 5041, 4828, 1028, 1215, 3886, 5238, 789, 2841, 2632, 5675, 5652, 0, 5303, 995, 2692, 4560, 5250], [4888, 4711, 4866, 0, 1706, 0], [3966, 5637, 4761, 1154, 2686, 4304, 2186, 3536, 2410, 5692]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('22097195')]\n",
      "[[5240, 4217, 2051, 4095, 5614, 5400, 842, 5240, 4755, 2051, 4095, 5614, 4353], [1960, 1634, 1501, 3803, 246, 1664], [3069, 3054, 3634, 5303, 0, 5240, 4711, 4866, 5303, 2471, 1467, 5240, 5247, 1994, 3803, 1352, 1433, 4495, 932, 3565, 2832, 5376, 3803, 3966, 5667, 711, 3755], [2203, 1363, 842, 4902, 2466, 5240, 5153, 3803, 1951, 842, 3921, 2832, 3881, 0], [3069, 5614, 713, 3899, 2829, 5240, 4353], [5400, 3803, 2344, 3966, 842, 4353, 3803, 404, 3966, 2591, 1035, 4995, 834], [5240, 5042, 5614, 4287, 4395, 842, 1534, 1337, 5383], [1635, 5637, 4556, 5486, 5240, 1850, 4259, 3856, 0, 5335, 3856, 715, 4431, 3791, 3856, 3966, 0, 5303, 1522, 5240, 5376]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('11843249')]\n",
      "[[4039, 2771, 4393, 1337, 5383, 0, 539, 3966, 5614, 5453, 3822, 3966, 5667, 0, 0, 3698, 4579, 3247, 3430, 2446, 4217, 684, 3803, 5240, 3145, 1126], [5261, 5383, 5614, 1756, 5303, 3384, 889, 2857, 3965, 1051, 1154, 677, 4851, 700, 3803, 4855, 4943, 5303, 4495, 3803, 4499, 2652, 927, 1297, 2590, 698, 932, 130, 1663, 2910, 3803, 0, 842, 4556, 963, 3535, 2997, 5601, 5240, 4709, 1297, 783]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('20564162')]\n",
      "[[5240, 5042, 2842, 535, 832, 3966, 2074, 1060, 219, 842, 228, 2844, 363, 2832, 5240, 3917, 923, 842, 369, 2832, 5240, 4082, 923, 535, 3803, 3966, 4442, 2666, 662, 885, 5250, 2588, 842, 333, 2591, 814, 5458, 0, 3252, 386, 0, 3490], [1405, 5240, 3917, 842, 4082, 924, 4641, 1398, 4605, 4411, 467, 5601, 419, 434, 3391, 4261, 2439, 5122, 183, 3536, 5601, 130, 3536, 505, 842, 5692, 5122, 4411, 553, 5601, 549, 543], [1152, 0, 3539, 2559, 5303, 5337, 2410, 3917, 567, 5601, 505, 92], [5376, 5614, 957, 5667, 4840, 2827, 2832, 3921, 51, 842, 5139, 9]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('21396307')]\n",
      "[[3803, 437, 2074, 3966, 282, 3966, 5637, 0, 932, 5240, 2998, 2572, 842, 5240, 1533, 2572, 789, 1435, 282, 3966], [3540, 4840, 3275, 4795, 4433, 4294, 3208, 5614, 1767, 2832, 2543, 2073, 5339, 1403, 5667, 4983, 5339, 37], [3885, 4193, 5042, 4834, 5238, 2543, 2073, 5339, 3370, 1028, 1049, 2832, 2829, 5240, 2901, 4998, 842, 1690, 5240, 2895, 3538, 2832, 4122, 2528, 1180, 3966], [4841, 3275, 4795, 2981, 3208, 5614, 2424, 2832, 1405, 2543, 2073, 5667, 4983, 5339, 32]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('9790546')]\n",
      "[[3966, 1416, 1433, 1021, 3803, 4353, 4370, 963, 4391, 954, 5303, 5376, 842, 963, 842, 3536, 3158], [2826, 3899, 5287, 5614, 2190, 2832, 1121, 5376, 2573, 1152, 3539, 4900, 2410, 3966, 4443, 4068], [772, 5536, 917, 5413, 4836], [5625, 2424, 1502, 3967, 3803, 1059, 4353, 3888, 963, 1960, 2403, 5489, 948, 1940, 5240, 2358, 3536, 3803, 5376, 2410, 0, 3966, 5667, 3433, 4290, 1180, 5652, 4442, 4068, 5562, 2391], [3270, 828, 4561, 5254, 2353]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23182307')]\n",
      "[[4755, 3888, 5637, 3881, 1891, 3803, 5240, 4742, 4322, 4998, 0, 3803, 1205, 4360, 3803, 3217, 842, 4594, 5508], [4510, 755, 5303, 681, 5254, 3640, 0, 4322, 842, 4048, 2461, 842, 2048, 3474, 0, 4494, 3803, 3217, 2235], [5240, 2861, 1569, 1991, 4415, 5614, 199, 0, 3995, 4360, 693, 3217, 5692], [5255, 3370, 2614, 5475, 3640, 733, 662, 5376], [1760, 4888, 3760, 5240, 3310, 1988, 4866, 5614, 4665]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('7502428')]\n",
      "[[5667, 1066, 2701, 2390, 3791, 3197, 3813, 842, 1135, 0, 842, 2587, 3539, 3813], [797, 1894, 3803, 437, 3445, 3803, 1066, 3823, 1656, 5614, 3741, 932, 1989, 932, 1223], [1277, 2446, 1018, 2832, 4808, 4360, 3803, 3217, 5539, 5637, 4841, 1813, 3856, 32, 1060, 5376, 2573, 4013, 2446, 3536, 5303, 842, 772, 2309, 1066], [5376, 1992, 2309, 1223, 2410, 1121, 2061, 3856, 13, 5667, 2616, 4419, 1066, 1223, 3803, 454, 597, 1466, 2996, 1313, 191, 5303, 5705, 2410, 5287, 5303, 5376, 2286, 842, 597, 1313, 199, 5303, 164, 2410, 5287, 5303, 1850, 4261], [4217, 1994, 2061, 5637, 5289, 5303, 5376, 2286, 842, 3773, 1850, 4261, 842, 5122]]\n",
      "[np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('19652072')]\n",
      "[[3966, 5637, 4397, 952, 5303, 5094, 4988, 1896, 3803, 369, 3445, 2832, 5240, 0, 798, 3856, 0, 4088], [3828, 2717, 4805, 3966, 5637, 4397, 952, 5303, 798, 454, 3856, 4088, 450, 1900, 842, 3822, 5042, 2410, 3391, 3536], [0, 1598, 2842, 2673, 4300, 3560, 5667, 3383, 1850, 2286, 3803, 3828, 4224, 1639, 4495, 842, 2553, 4006, 4998], [5240, 3544, 1389, 2559, 5376, 4523, 722, 5637, 960, 2304, 173, 1798, 119, 2729, 119, 2604, 0, 5155, 842, 854], [1900, 5614, 4477, 5303, 282, 3445, 2832, 419, 3966, 404, 1935, 5303, 2559, 722]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('12610178')]\n",
      "[[3047, 4734, 3803, 5631, 842, 3803, 3823, 2186, 5632, 1725, 4843, 1994, 842, 4360, 3803, 3217, 2832, 3966, 5667, 2453, 4490, 3433, 1377, 1180], [5625, 4288, 1403, 5240, 1994, 842, 5313, 3803, 5413, 3047, 4496, 3823, 5629, 2410, 5632, 2404, 1154, 5629, 4609, 4012, 5631, 3823, 2186, 5632, 2832, 5077, 3966], [5261, 3575, 3837, 3129, 4039, 2771, 5042, 4397, 952, 3966, 2832, 4415, 5303, 3047, 2533, 2009, 5631, 138, 3445, 3856, 3823, 2186, 5632, 356, 3445, 3856, 323, 3445, 2832, 3966, 5652, 5637, 526, 5693, 3803, 737, 5652, 2591, 1968, 1548, 3827, 2572, 4006, 4998, 2105, 5303, 3856, 5652, 2591, 4224, 3991, 3050], [2559, 3667, 3791, 2832, 311, 3803, 3966, 5374, 5631, 842, 348, 3803, 5264, 5374, 3823, 2186, 5632, 355]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('9849452')]\n",
      "[[3691, 4840, 1812, 5637, 4761, 2832, 2311, 3803, 5240, 5105, 1205, 2572], [3966, 2832, 5240, 1297, 2572, 4563, 1059, 3900, 4048, 2465, 842, 5150, 1533, 1403, 5667, 5240, 5105, 1205, 2572], [4360, 3803, 3217, 5614, 3385, 963, 4392, 842, 4224, 5303, 1960, 5376, 1581, 842, 963, 1563, 5629, 2997, 2832, 5240, 1533, 923, 5512, 5240, 2090, 4351, 1156, 3165, 4369], [2572, 1812, 5637, 4889, 5672, 5240, 4325, 1890], [427, 3966, 5637, 4393, 5105, 1205, 292, 1297, 257, 1045, 2028, 2410, 1399, 828]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('18386580')]\n",
      "[[5240, 5247, 1994, 5614, 2175, 733, 1635, 3803, 5376, 1154, 0, 5240, 1277, 3803, 4824, 5222, 1994, 5202, 5155, 4360, 3803, 3217, 842, 2792, 2461, 932, 5635, 932, 5240, 715, 4431], [5240, 3792, 3803, 715, 4431, 2832, 5240, 5376, 2572, 5614, 3275, 5237, 5238, 2832, 5240, 1533, 2572, 77], [772, 3966, 5637, 5374, 5667, 5240, 4709, 0, 4495, 3803, 1297, 2410, 246, 1664, 932, 3828, 5247, 1634, 5649, 5264, 2832, 5240, 5376, 2572, 5637, 2533, 4817, 680, 2832, 5240, 0], [5240, 3208, 3803, 1244, 1245, 842, 1245, 1246, 4415, 2857, 77, 842, 1246, 1688, 2832, 5240, 5376, 2572, 32, 5649, 5255, 4831, 2935, 1275, 2832, 5240, 1533, 2572]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('16921047')]\n",
      "[[5240, 4217, 2051, 4095, 5614, 3900, 5122, 4755, 2051, 4096, 5637, 4261, 2439, 5122, 4605, 4409, 4701, 842, 4360, 3803, 3217], [3828, 2717, 3683, 2364, 3966, 5637, 2074, 842, 4831, 1018, 1279, 5635, 1011, 1060, 5376, 924], [797, 5240, 1811, 1802, 3741, 967, 4994, 4839], [3391, 3900, 5122, 842, 4261, 2439, 5122, 5637, 3539, 2307, 2832, 5240, 1378, 923, 932, 1403, 5667, 2505, 783], [789, 3391, 3900, 5122, 5614, 5096, 2410, 3966, 5374, 2832, 5240, 2504, 923, 932, 1403, 5667, 5240, 2503, 923, 3536], [1378, 5376, 2832, 5240, 2504, 923, 5614, 957, 5667, 4269, 3391, 4261, 2439, 5122, 3536, 3536, 2616, 4415, 2708, 543, 0], [5403, 4605, 4411, 5637, 1398, 1060, 5376, 924, 100], [814, 712, 5648, 1802, 3741, 2705, 4429, 4994, 4839, 2708, 556, 164]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('19690306')]\n",
      "[[1152, 1802, 3741, 2614, 2827, 2832, 5150, 2962, 4745, 3856, 4477, 1664, 2832, 5240, 2695, 3856, 0, 3856, 2041, 1733, 5585], [2962, 3803, 4797, 5614, 3385, 932, 5240, 3759, 3803, 1664, 2832, 5240, 2695, 842, 2832, 5240, 2963, 1205, 5466, 0, 842, 5240, 3759, 3803, 2041, 1733, 5585, 4461, 2832, 5240, 2015, 3397, 4460], [4360, 3803, 3217, 5614, 3385, 1154, 5240, 2462, 948, 3803, 1312, 2779, 5250, 2410, 3929, 1205, 4743, 4398, 195], [5303, 1773, 5240, 1988, 3803, 3766, 3182, 2998, 3822, 4360, 3803, 3217, 5150, 2962, 3537, 842, 4594, 5508, 2832, 3966, 5667, 711, 1180]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('19347979')]\n",
      "[[3966, 1217, 2499, 547, 0, 2591, 5680, 3900, 5122, 2616, 4415, 2708, 77], [5254, 4712, 5637, 834, 5512, 4862, 0, 1880, 3355, 5238, 0, 2364, 2821, 2088, 2832, 5240, 2544, 3425, 0], [5625, 2175, 5240, 4669, 3803, 2544, 4523, 2519, 3822, 3900, 5122, 5287, 5303, 4261, 715, 2183, 842, 4360, 3803, 3217, 4353, 2832, 4976, 2773, 3077, 3698, 4888, 1253, 3286, 1180, 3966, 5652, 5637, 4974, 3856, 4604, 2446, 2921, 5376, 5667, 4079, 1017, 1297, 842, 5063, 4395, 5303, 4441, 1656, 3857, 0, 3856, 4068], [3588, 828, 5637, 5509, 5303, 2174, 5240, 958, 3803, 5240, 2519, 3822, 5240, 2426, 2061], [3966, 1217, 5240, 2558, 1240, 0, 2591, 1338, 4840, 1682, 2832, 5240, 0, 3796, 4415, 3856, 67, 5330, 2462, 948, 3803, 1180, 5250, 3286, 4743, 3856, 119, 67, 4048, 3856, 58, 2462, 3856, 67, 842, 2045, 5635, 1045, 0, 3856, 265, 32], [3965, 4353, 5614, 946, 5512, 5240, 2462, 948, 3803, 1180, 5250, 3286, 842, 5240, 0, 4353, 4370]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21300930')]\n",
      "[[802, 4411, 1810, 4841, 1154, 5376, 923, 5667, 5240, 963, 923, 2615, 5240, 3277, 4409], [5240, 3622, 5116, 697, 1135, 842, 1126, 4265, 322, 5383, 2842, 3418, 2678, 3447, 842, 4360, 3803, 3217, 4353, 5041, 5303, 1402, 5377, 3822, 5254, 3888]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('14698542')]\n",
      "[[5240, 5413, 1297, 4495, 4831, 4843, 1991, 2832, 5150, 3928, 5642, 2175, 5667, 1156, 0, 3803, 2090, 4353, 4369], [5240, 1850, 4523, 5153, 2825, 5667, 5287], [3966, 5637, 4395, 5303, 4441, 3601, 4495, 3486, 3445, 3303, 1649, 5577, 3445, 3303, 1321, 101, 3445, 3303, 1649, 3856, 3600, 4495, 3486, 3445, 3303, 1649, 5577, 3445, 3303, 1195, 323, 3445, 3303, 1649, 2186, 5632], [2446, 4786, 215, 5303, 3093, 218, 168, 1485, 3966, 5637, 4395, 5303, 3601, 543, 3966, 3856, 3600, 923, 549, 3966], [5667, 5240, 4951, 4370, 5378, 5334, 814, 2825, 4360, 3803, 3217, 2866, 5614, 3781, 1940, 5376, 5667, 5240, 1195, 1378, 2832, 1406, 5303, 5240, 1321, 1378]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('9496390')]\n",
      "[[4754, 2910, 3803, 3931, 1802, 3741, 1687, 4593, 2471], [5240, 753, 3803, 5261, 5042, 5614, 5303, 2174, 3931, 2410, 1115, 3921, 2832, 4393, 1901, 1096, 5383, 842, 5303, 2174, 5240, 0, 3803, 3672, 3355, 3803, 1115, 4593, 2832, 3966, 5667, 1115, 3430], [4593, 3355, 1802, 3741, 1687, 733, 4068]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23070101')]\n",
      "[[5330, 3803, 171, 3966, 5637, 4395, 5303, 5417, 3856, 5115, 2743, 3856, 3611], [5303, 4562, 5692, 1530, 2067, 3307, 4592, 2785, 2353, 3803, 5240, 4609, 5383, 0, 5652, 5456, 2009, 5515, 927, 2035, 5417, 3856, 3611], [2340, 2892, 963, 3536, 5614, 1415, 2832, 355, 3803, 5675, 781, 1415, 2832, 311, 842, 3946, 2832, 363], [3691, 3611, 1219, 2591, 2471, 2998], [5251, 3054, 2471, 4840, 4480, 2832, 1121, 5515, 5598, 842, 1892, 2340, 1795, 1060, 3536, 842, 5693, 733, 5417], [1415, 2340, 2892, 1888, 3741, 5363, 3005, 5330, 0, 2446, 5062, 4517], [3672, 2340, 2414, 3054, 4841, 2663, 733, 3611]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21871978')]\n",
      "[[5240, 4122, 1337, 5097, 3803, 5240, 2989, 3803, 0, 4459, 2952, 5303, 0, 0, 4459, 4694, 733, 5330, 2485, 2613, 3741, 1035, 0], [733, 4553, 3966, 5652, 1802, 3741, 1415, 5240, 2403, 5489, 5120, 3856, 0, 1219, 272, 3966, 2832, 5240, 4694, 2572, 842, 191, 3966, 2832, 5240, 2952, 2572, 5637, 1338, 993, 842, 5241, 4122, 4998, 5614, 946], [5330, 3803, 107, 3966, 5667, 2486, 1180, 5637, 4288, 4397, 1875, 3005, 2573, 2410, 4694, 442, 3856, 2952, 4459, 446, 733, 5330, 2485]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16934909')]\n",
      "[[3829, 5041, 917, 0, 0, 4734, 5303, 0, 0, 1060, 5254, 741], [3966, 3822, 924, 842, 2591, 3391, 0, 3803, 327, 842, 282, 5632, 130, 3534, 0, 3803, 311, 842, 272, 842, 191, 3534, 0, 3803, 191, 842, 4597], [4353, 828, 2309, 923]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('22172923')]\n",
      "[[1152, 4840, 4481, 5637, 3781, 2832, 3921, 2635, 842, 5286, 3379, 3803, 1275, 144, 173, 842, 3492, 4597, 842, 2279, 0, 2825, 4841, 5383, 3887, 2866, 3379, 3803, 1275, 0], [3274, 2442, 3274, 2962, 2018, 1802, 3741, 4476, 3297, 5598, 3379, 3803, 1275, 199, 547, 3490, 363]]\n",
      "[np.int64(1), np.int64(1)]\n",
      "[np.str_('21592754')]\n",
      "[[1563, 3391, 2286, 2439, 5122, 5289, 5637, 842, 3536, 224], [5261, 4393, 3698, 2898, 5383, 5614, 1756, 5303, 944, 5646, 4385, 4087, 697, 5653, 1130, 4386, 4685, 5623, 3054, 932, 1989, 932, 5115, 4087, 5653, 1130, 4386, 5623, 2410, 1180, 3966, 5667, 4910, 1130, 3431, 5667, 4596, 5303, 3900, 5122, 842, 4360, 3803, 3217], [5240, 5383, 5614, 1347, 1963, 1935, 5303, 4881, 638], [1060, 2321, 230, 842, 914, 236, 385, 3966, 5637, 1499, 2028, 257, 1489, 5303, 4392, 842, 246, 5637, 827, 119, 4685, 5623, 100, 5623], [5261, 4393, 5383, 2049, 5240, 638, 1819, 842, 0, 3274, 4994, 4140, 1390, 957, 5667, 2972, 5041, 1915, 2446, 4888, 2028, 4105]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('15753541')]\n",
      "[[5303, 944, 5240, 4701, 842, 1994, 3803, 5236, 2832, 973, 5634, 3272, 2832, 3966, 5667, 1161, 4755, 5303, 711, 3933, 1180], [5240, 4217, 3887, 5614, 1275, 2832, 5634, 842, 3769, 4998], [4264, 1640, 2139, 5405, 3636, 2280, 786, 5301, 786, 4080, 0, 4669, 2832, 5240, 0, 3803, 1180, 1161]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23686277')]\n",
      "[[5649, 3470, 3022, 5208, 2614, 1035, 901, 2860, 1330, 2188, 3803, 5097, 2410, 3140, 4136, 4237, 3054, 3741, 5694, 993], [5240, 5383, 2591, 5303, 1028, 5019, 4176, 1935, 5303, 2951, 3965, 4468], [5330, 3803, 396, 3966, 246, 3139, 437, 246, 1443, 437, 5637, 4395], [5240, 3139, 907, 3054, 2318, 2410, 4613, 4241, 842, 3040, 4760, 963, 3180, 932, 4698, 932, 1443, 5115], [5240, 5383, 5614, 1756, 932, 4851, 1261, 4154, 0, 4395, 1534, 5383, 5512, 5413, 2572, 3939, 5097, 1754], [5251, 5637, 3691, 0, 2832, 3189, 3803, 2695, 5000, 4122, 3921, 1126, 2461, 842, 4360, 3803, 3217, 1060, 1121, 908], [5240, 4217, 2060, 5614, 1703, 932, 5240, 810, 3803, 1104, 3272], [2832, 1530, 5240, 1540, 907, 5614, 5096, 2832, 1939, 3803, 3841], [4755, 2061, 1809, 2832, 1121, 2573], [4994, 828, 5637, 2250, 4850, 5240, 5383, 2591, 5303, 1028, 5019, 4176]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23962028')]\n",
      "[[5254, 828, 1467, 5238, 2179, 2214, 4305, 1337, 1051, 5673, 716, 0, 2711, 2832, 3966, 5667, 2708, 609, 5652, 4471, 4259, 3822, 4224, 0, 5562, 2054, 5250, 783], [2832, 5240, 2230, 3967, 3489, 3503, 2179, 2214, 5374, 3966, 5652, 1802, 3741, 1921, 3886, 1963, 2591, 4974, 1132, 842, 1131, 4527, 5303, 3972, 2214], [2711, 1659, 5637, 3741, 1370, 733, 1850, 4261], [4261, 2439, 5122, 5122, 4605, 4409, 4701, 842, 2711]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('17145528')]\n",
      "[[5010, 5653, 5146, 842, 0, 4959, 4705, 2383, 5637, 3385, 842, 3397, 3888, 4824, 2412, 363, 4815, 363, 2167, 3863, 2410, 4577, 842, 5376, 3803, 1180, 2090, 1555, 0, 842, 2090, 2627, 842, 3635, 3520, 4351, 812, 0, 5637, 1416, 963, 1018, 842, 842, 130, 3536, 733, 4386], [2831, 5614, 4841, 1059, 5237, 1610, 2832, 5226, 3803, 0, 0, 842, 2825, 4353, 2410, 1963, 4976, 1850]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('19074911')]\n",
      "[[2832, 5261, 2986, 3534, 3837, 3129, 5383, 3966, 2591, 662, 4375, 814, 2836, 4605, 5303, 863, 5301, 5250, 2410, 3536, 3856, 3268, 842, 1850, 664, 4743, 2832, 304, 0, 1658, 4433, 4294, 1606, 3803, 3856, 2567]]\n",
      "[np.int64(0)]\n",
      "[np.str_('22282373')]\n",
      "[[2408, 2953, 842, 897, 5545, 4745, 5637, 4841, 2663, 2832, 5240, 2524, 2572, 5237, 2832, 5240, 4068, 2572, 191, 3103, 3109, 1663, 5601, 130, 3103, 3109, 1663, 9, 842, 5601, 4, 4597], [4840, 1770, 5614, 3744, 733, 1297, 2832, 5240, 4068, 2572, 2832, 4353, 4745, 897, 3625, 842, 5600, 842, 2541, 2631, 4998], [1321, 4478, 4075, 2524, 3209, 5277, 5240, 0, 0, 4446], [5240, 4217, 2060, 5614, 1277, 2832, 3857, 0, 2953, 842, 5240, 4755, 2061, 5637, 1297, 4523, 715, 2183, 897, 5587, 822, 4723, 5545, 4745, 1277, 2832, 2492, 0, 842, 3769, 4998, 2844, 4404, 0, 0, 842, 4360, 3803, 3217, 4353, 2148, 5667, 5240, 2167, 3863, 2410, 4577, 842, 5376, 3803, 1180, 4353, 1555, 4369, 4351, 1156], [5255, 4442, 2009, 3014, 2912, 3803, 5159, 2716, 2524, 5703, 3109, 3856, 4704, 5412, 1656, 2410, 5629, 5667, 1321, 700]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23075680')]\n",
      "[[5240, 5403, 4261, 1939, 2832, 5254, 5413, 2573, 5614, 842, 3536, 4597], [5240, 5330, 1994, 3803, 3698, 4962, 1253, 1198, 2832, 5240, 4143, 2572, 5614, 4841, 2663, 5237, 5238, 2832, 2556, 2572], [5240, 5330, 0, 3803, 3698, 4962, 1253, 1198, 5637, 300, 490, 842, 173, 5705], [5240, 4409, 3803, 715, 4432, 2844, 5650, 1104, 1253, 4480, 3275, 4078, 1576, 3275, 2649, 842, 2598, 3272, 2832, 5240, 4143, 2572, 5614, 4841, 3275, 5237, 5238, 2832, 5240, 2556, 2572]]\n",
      "[np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('10735887')]\n",
      "[[2214, 4270, 5122, 5287, 5287, 5303, 5403, 4261, 842, 5287, 5303, 5376, 2286, 1403, 5667, 3304, 842, 3808, 5635, 5316, 5376, 3853, 2410, 4121, 5675, 5667, 4262, 711, 1135, 1180, 5652, 2241, 2286, 3803, 5184], [5330, 3803, 0, 3966, 5637, 4395, 5303, 2214, 282, 3445, 0, 3856, 3304, 0, 385, 3445, 2426, 5289, 1656], [4843, 5378, 5614, 3744, 2832, 3966, 5667, 5583, 3430, 144, 100]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('10930797')]\n",
      "[[4133, 4253, 2282, 5637, 5040, 2832, 197, 5675, 5667, 1018, 4349, 1659, 2446, 5383, 3803, 3917, 5562, 1908, 932, 2358, 3233, 1297], [5240, 753, 3803, 5240, 4265, 5614, 5303, 2754, 1337, 842, 4360, 3803, 3217, 4349, 2282, 5238, 5310, 4160, 5122, 842, 4605, 5303, 1297, 2832, 711, 1135, 1180], [5240, 2348, 3588, 3503, 4161, 4103, 5122, 5667, 3585, 4859, 3803, 5583, 1850, 16, 1786, 5693, 53, 842, 3921, 16]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('22113257')]\n",
      "[[5240, 2998, 5614, 1716, 5277, 5240, 5471, 3803, 0, 963, 0, 0, 3948, 4442, 5218, 4043, 1171, 3899, 5240, 1581, 3803, 5240, 173, 5629, 2998], [0, 0, 2832, 483, 3465, 3803, 0, 664, 5629, 1135, 842, 1377, 1180, 5126, 5637, 4395, 5303, 5213, 1017, 2217, 2998, 3856, 5513, 1205, 1533, 2572], [3828, 2717, 842, 5411, 3828, 3966, 5637, 2074, 5277, 5218, 1180, 842, 3206, 2572, 0, 0, 101, 3966, 2591, 1135, 1180, 842, 246, 2591, 1377, 1180]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('18293016')]\n",
      "[[3833, 369, 3803, 5240, 3966, 1416, 4295, 5376], [3178, 1111, 3360, 5614, 2148, 1154, 1080, 842, 4870, 2399, 3387], [4111, 0, 2282, 1062, 5240, 1994, 3803, 5240, 2998, 5637, 5240, 0, 3803, 3966, 5303, 1415, 814, 5629, 1581, 3803, 5376, 842, 4637, 2832, 0, 2297, 2410, 2403, 5489, 948, 842, 1031, 5240, 3966, 3370, 2614, 3833, 2591, 5634, 3272, 4111, 3741, 4523, 5303, 1161, 1152, 3881, 1235, 3803, 5634, 3272, 5077, 932, 1688, 897], [2426, 2717, 4807, 5413, 711, 1180, 3966, 5667, 1060, 842, 100, 5634, 3272, 5637, 4395, 5303, 3489, 3803, 1058, 2720, 1058, 3439, 1153, 2543, 842, 922, 3856, 814, 3059, 3057, 1533, 3489, 5179, 5412, 1663, 2410, 5632], [2705, 3272, 3803, 4140, 5614, 3741, 814, 3067, 1031, 3803, 5240, 4071, 3145, 5189, 4711, 4866], [1017, 3822, 814, 2965, 5303, 5372, 830, 5251, 5614, 3691, 4995, 4840, 1811, 2832, 5240, 5629, 3178, 1111, 3360, 1060, 5240, 5413, 924]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('19838842')]\n",
      "[[2998, 3966, 789, 2241, 2567, 4547, 4411, 0, 3539, 1741, 2439, 1664, 9, 3197, 2462, 2807, 35, 842, 2567, 4360, 3803, 3217, 66, 963, 130, 3536, 5237, 5513, 1205, 3948], [5261, 830, 0, 5240, 1991, 3803, 5240, 2829, 3537, 4274, 632, 5303, 1368, 5376, 2802, 4255, 0, 1205, 3334, 4255, 2410, 1741, 2832, 4217, 1205, 3966, 5652, 2591, 814, 0, 1180, 1793], [5413, 2717, 0, 1180, 3966, 5637, 2753, 2446, 5240, 0, 3948, 2832, 5240, 0, 5042]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('12453852')]\n",
      "[[2426, 5335, 1673, 3791, 2832, 5240, 3750, 923, 842, 2006, 2832, 5240, 3684, 923], [5240, 4983, 1903, 5581, 1321, 5614, 1403, 5667, 0, 3803, 5581, 2765, 1321, 2832, 5226, 3803, 5122, 2832, 3966, 5667, 711, 3698, 4888, 1253, 3286, 1180, 3755], [2446, 2321, 219, 5303, 3095, 220, 290, 0, 3966, 2082, 5240, 5042, 842, 5637, 4393, 5303, 4441, 2009, 5581, 1321, 3750, 5581, 322, 3445, 3822, 1664, 842, 164, 5667, 1321, 556, 3445, 3822, 1663, 3856, 5581, 2765, 1321, 3684, 5581, 282, 3445, 3822, 1664, 842, 2765, 3822, 1663, 842, 1321, 543, 3445, 3822, 1663, 5667, 1121, 4496, 1045, 4556, 2186, 5632], [3391, 842, 5692, 5122, 4411, 5637, 100, 3536, 842, 374, 2410, 3750, 842, 3536, 842, 341, 2410, 3684, 4597], [797, 5261, 3054, 3741, 4995, 4840]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('17664467')]\n",
      "[[1668, 4841, 4269, 5287, 5303, 1706, 5682, 3803, 3123, 1403, 5667, 1269, 3391, 3536, 2616, 4415, 374, 597, 1313, 94, 5303, 546, 3263, 4402, 31], [2705, 4287, 2177, 3803, 1337, 1051, 2832, 743, 3966, 2613, 3670, 1036, 1035, 4563, 2832, 4039, 2771, 4803], [5240, 678, 3803, 5303, 1269, 3741, 3833, 4841, 2825, 1337, 1051, 1152, 789, 2825, 4360, 3803, 3217, 5287, 5303, 4261, 842, 3900, 5122, 1403, 5667, 1269], [1337, 1051, 949, 5637, 4461, 963, 1960, 1336, 5584]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('10735891')]\n",
      "[[2832, 678, 5303, 3921, 842, 3844, 820, 2953, 4290, 4936, 877, 4313, 4605, 5287, 5303, 1850, 4261, 4360, 3803, 3217, 4006, 4998, 842, 5122, 5637, 1403], [3644, 4360, 3803, 3217, 3729, 4006, 4998, 5614, 1688, 1154, 5110, 5376]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('15452188')]\n",
      "[[4353, 5614, 946, 963, 1018, 842, 5629, 130], [3998, 5355, 5614, 4841, 3275, 2832, 5240, 2103, 766, 2572, 1403, 5667, 4901, 257], [2649, 4606, 1703, 932, 3379, 2649, 3856, 130, 1877, 3856, 3856, 1877, 2856, 1403, 5667, 1018, 5637, 4841, 2663, 2832, 5240, 2103, 766, 2572, 5562, 4901, 446, 842, 502, 4597, 4, 2410, 1121, 1407]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('12467070')]\n",
      "[[5270, 5289, 3995, 5629, 1900, 4732, 5614, 3744, 5303, 1028, 3539, 1989, 1403, 5667, 3823, 5629, 4732, 5673, 889, 4840, 1811, 2832, 5337, 4563], [2426, 5632, 3803, 5250, 0, 1634, 3966, 1573, 4441, 5489, 5303, 1635, 842, 5264, 5652, 1416, 1635, 5637, 2533, 5240, 3853, 3803, 4443, 2259, 5250], [1454, 3856, 4224, 5508, 3803, 0, 2919, 1802, 3741, 893, 5303, 725, 5240, 3965, 4605, 5303, 5376, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23404211')]\n",
      "[[2832, 0, 1180, 604, 0, 490, 235], [5240, 3544, 1389, 2559, 715, 2183, 5017, 845, 2022, 3247, 2088, 0, 842, 1952, 3791, 963, 4843, 2441, 2832, 940, 842, 3698, 940, 3966], [4360, 3803, 3217, 5614, 4843, 1060, 5376, 924, 2832, 940, 3966]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('23406994')]\n",
      "[[1378, 4495, 3803, 3304, 842, 5236, 3054, 3539, 1989, 5237, 3304, 783, 2832, 5240, 5376, 3803, 1162], [2832, 1530, 2832, 5240, 1533, 2572, 2061, 5667, 4840, 2826, 2446, 1018, 2842, 1111, 5634, 45, 842, 897, 45], [5376, 1939, 5614, 5632], [5240, 3379, 1277, 2832, 5240, 2061, 2446, 1018, 2832, 5240, 5383, 2572, 5637, 4841, 2567, 1403, 5667, 5240, 1533, 2572, 2832, 5240, 4217, 2061, 1111, 5634, 77, 2304, 32, 842, 4360, 3803, 3217, 32, 842, 2832, 5240, 4755, 2061, 2570, 5028, 77, 2536, 4253, 4743, 45, 1968, 1548, 3827, 2572, 4006, 4998, 45, 2775, 32, 842, 5403, 3636, 2280, 32]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('20479425')]\n",
      "[[5625, 2424, 3691, 2188, 5238, 5240, 678, 3803, 5341, 5303, 1295, 2832, 3966, 5667, 711, 2627, 842, 3635, 1180, 3741, 4764, 2410, 5240, 4192, 3803, 0, 2828, 3875]]\n",
      "[np.int64(0)]\n",
      "[np.str_('22357901')]\n",
      "[[820, 1569, 842, 2695, 5000, 5637, 4841, 3275, 2832, 5374, 2573, 5237, 2832, 5240, 4068, 2572], [5261, 4395, 1901, 1096, 4068, 1534, 4287, 1337, 5042, 4442, 2943, 4646, 1109, 912], [733, 5688, 2908, 1488, 5614, 3786, 247, 3966, 5667, 3247, 1180, 5637, 4395, 3005, 5270, 2573, 3803, 526, 3966], [3691, 4840, 1811, 5614, 3781, 2832, 4360, 3803, 3217, 842, 715, 2183, 1060, 5240, 5374, 2573, 842, 5240, 4068, 2572], [3762, 4413, 4723, 4745, 2410, 3921, 2962, 2832, 2572, 842, 2572, 5637, 4841, 3275, 5237, 5264, 2832, 2572, 963, 130, 9, 130, 272, 9, 842, 272, 427, 9]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('23245940')]\n",
      "[[5240, 2838, 3803, 4809, 926, 4688, 2090, 2559, 963, 5240, 2403, 5489, 5584, 5614, 265, 417, 2832, 5240, 1169, 2572, 842, 199, 374, 2832, 5240, 2141, 2572], [926, 0, 5303, 1028, 4528, 3230, 4233, 4232, 3539, 2903, 1154, 5376, 4523, 2282, 5237, 1154, 1307, 3803, 4870, 1205, 4246, 2832, 5261, 3965, 2572], [5240, 4217, 2060, 5614, 5240, 1811, 2832, 4280, 3803, 3966, 5667, 926, 946, 5667, 5240, 4377, 5250, 3827, 2572, 5240, 3863, 2410, 4577, 842, 5376, 3803, 1180, 670, 4377, 3538, 4746, 1598, 4688, 2090, 4723, 963, 2403, 5489], [5240, 4337, 3803, 5261, 1097, 4395, 1337, 5383, 5614, 5303, 1402, 5413, 5326, 741, 1169, 0, 1595, 5601, 2141, 1595, 2832, 4479, 5240, 4662, 3803, 4809, 670, 4377, 4870, 4432, 926, 2832, 4524, 5303, 697, 4386, 4687, 2410, 1135, 1180]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('11896110')]\n",
      "[[5240, 678, 3803, 3274, 1896, 2801, 1802, 3741, 4242, 4995, 4840, 712, 2832, 3875, 5287, 5303, 4261, 3856, 3856], [2832, 5240, 577, 1078, 923, 3966, 257, 3874, 282, 5270, 1609, 842, 199, 4310, 526, 5637, 4461], [5376, 4523, 5337, 5614, 0, 4843, 2832, 1121, 924], [2472, 3885, 5376, 4732, 5614, 1215, 3886, 3822, 3892, 842, 2591, 814, 629, 3208, 3803, 5337], [2705, 5240, 119, 3534, 3391, 3875, 2832, 5240, 1078, 923, 1888, 3741, 1809, 2569, 2446, 5240, 1057, 4623, 5667, 2662, 1896, 2775, 1515, 4496, 4563, 2832, 5240, 3243], [4039, 2768, 842, 2771, 5041, 2614, 4833, 5238, 5240, 678, 3803, 2981, 2775, 842, 2978, 786, 318, 2760, 786, 318, 2832, 0, 1297, 1614, 2410, 711, 3408, 2858, 3900, 4605, 3856, 762, 5673, 1330, 2188, 3803, 814, 2826, 2832, 3900, 5122, 3875]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('12525531')]\n",
      "[[1406, 3803, 5240, 4105, 3730, 842, 1337, 5383, 1659, 4831, 5238, 5376, 5667, 2103, 766, 4621, 2832, 1338, 3381, 932, 5635, 932, 4995, 4840, 2827, 2832, 4353, 32]]\n",
      "[np.int64(1)]\n",
      "[np.str_('17889449')]\n",
      "[[1062, 5403, 842, 3694, 4976, 3101, 4006, 4998, 4217, 4858, 0, 5508, 5508, 3803, 1457, 1297, 842, 0, 2430, 4734, 5240, 2279, 812, 4743, 5614, 2865, 4163, 3803, 3280, 1152, 3741, 3875, 5667, 22], [5240, 3391, 2403, 5489, 5287, 5614, 300, 3536, 2410, 772, 3966, 842, 434, 3536, 2410, 5125, 3966]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('21723792')]\n",
      "[[5240, 0, 5637, 427, 597, 1466, 2996, 1313, 374, 477, 842, 475, 597, 1313, 422, 511, 127, 5667, 3970, 842, 3970, 4597], [5240, 678, 3803, 2505, 5303, 3970, 5614, 3741, 957, 5667, 4995, 4840, 2826, 2832, 3873]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('9747868')]\n",
      "[[5240, 1688, 4662, 3791, 2832, 5675, 738, 434, 5693, 3856, 5698, 411, 437, 477, 5693, 442, 842, 483, 5693, 3856, 3816, 460], [5184, 4477, 5240, 4662, 3803, 3708, 1135, 1180, 1154, 437, 5413, 4836, 13], [5184, 4477, 5240, 4662, 3803, 3022, 1135, 1180, 1154, 434, 5413, 4836, 0, 5667, 1620, 2838, 5277, 515, 3536, 3803, 2403, 5489, 3803, 404, 5562, 257, 3995, 102, 5675, 2832, 5240, 4068, 842, 5184, 2573, 4597]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('22234519')]\n",
      "[[5330, 3803, 178, 2028, 3966, 5637, 4397, 952, 5303, 4441, 697, 5184, 2216, 3856, 839], [2279, 4745, 5637, 4843, 2832, 5240, 2216, 2572, 842, 839, 2572], [2631, 4523, 4360, 3803, 3217, 2711, 5153, 3803, 1741, 842, 715, 2183, 722, 5637, 1403, 1060, 3082, 4121, 3966, 5667, 2692, 4779, 1135, 1180, 0, 5652, 4442, 697, 5184, 2216, 3856, 839, 2832, 814, 3837, 0, 4395, 3575, 5383, 1755, 932, 5240, 3622, 5116, 697, 5042, 3803, 1135, 1180, 0, 0, 67, 5071, 3803, 5240, 5184, 2216, 697, 3584, 5204, 5383], [2533, 5240, 4623, 3803, 5240, 5204, 5383, 0, 5508, 3803, 5184, 2404, 1154, 814, 925, 2918, 0, 3370, 1028, 814, 2821, 3853, 2410, 697, 2054, 5250, 2832, 3082, 4121, 5675]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('21471765')]\n",
      "[[5261, 5042, 5614, 4059, 4395, 1534, 5383], [5240, 3310, 3887, 3388, 5637, 5240, 5475, 5105, 1205, 3640, 2631, 4797, 5518, 842, 4360, 3803, 3217, 963, 842, 3536, 4116], [2827, 1176, 1028, 3306, 2832, 5240, 4360, 3803, 1180, 1205, 2759, 5105, 1205, 3640, 917, 682, 2705, 5251, 917, 2335, 5384, 3803, 5105, 1205, 3000, 5303, 2577, 4098, 842, 4148], [5261, 1811, 5614, 1338, 4533]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16125515')]\n",
      "[[5240, 5413, 4338, 3803, 5240, 5042, 5637, 5303, 1749, 5240, 1991, 3803, 5240, 4255, 842, 5303, 3785, 2906, 618, 3965, 4171, 2410, 3574, 3856, 0, 1827, 4510, 4257], [3579, 4510, 4255, 2410, 1180, 5126, 5614, 1778, 5303, 0, 1180, 4523, 4235, 842, 5303, 2824, 4360, 3803, 3217], [2472, 2759, 3806, 5240, 1307, 5240, 3318, 3803, 1180, 5126, 0, 5303, 0, 3579, 4257, 5303, 4257, 5667, 3833, 3828, 1426]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('22689807')]\n",
      "[[2832, 5261, 5490, 2348, 830, 3803, 772, 3966, 4397, 952, 5667, 5021, 313, 3142, 4087, 5370, 1523, 5303, 4830, 5097, 5303, 3142, 3532, 2832, 4031, 2616, 4415, 2708, 539, 597, 1313, 475, 5303, 596, 35, 842, 3806, 4840, 3875, 1051, 2708, 539, 597, 1313, 471, 5303, 600, 53], [5254, 1659, 1725, 4840, 3534, 3391, 3875, 712, 5667, 5240, 3142, 842, 5370, 1378, 842, 5103, 1933, 2654, 1100, 2832, 3966, 5667, 2634, 4205, 2654, 4108, 3373]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('22945882')]\n",
      "[[3966, 4442, 4710, 3856, 4068, 2426, 5289, 1656, 2410, 5489, 5303, 437, 1664, 1940, 4733, 1289, 4386], [3921, 4480, 5614, 4840, 2446, 1663, 0, 2051, 3803, 5376, 5667, 4710, 842, 2446, 1664, 5303, 246, 2832, 4068, 3966]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('20339140')]\n",
      "[[3691, 4995, 4840, 5376, 1812, 2832, 1297, 3856, 4377, 1719, 3397, 4594, 5518, 3965, 4563, 1126, 2461, 3856, 4353, 5637, 3781], [1060, 842, 1664, 1036, 5240, 4986, 3803, 4377, 5250, 3966, 4442, 322, 3445, 1896, 3803, 3138, 118, 3966, 3856, 4068, 112, 3966, 5566, 0, 2927], [5330, 3803, 251, 3966, 5637, 2842, 2832, 5240, 2348, 830], [4754, 1896, 5614, 2533, 3822, 1663, 257, 1664, 3803, 4377, 5376], [4994, 5234, 5637, 3828, 3856, 5413, 4836, 932, 4938]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('17548243')]\n",
      "[[5240, 2358, 4071, 2980, 830, 2832, 2321, 233, 165, 4831, 5238, 5122, 5614, 3268, 2832, 3966, 952, 4068, 5237, 2832, 3966, 952, 4794, 5435, 2616, 4415, 483, 597, 1313, 67, 417, 3263, 4402, 67, 693, 2616, 4415, 490, 84, 391, 1587, 3503, 45], [5625, 754, 5303, 2754, 5240, 1992, 3803, 814, 2143, 871, 3822, 5254, 5153, 842, 5122, 2832, 3966, 5667, 711, 1180, 5652, 1802, 3741, 2614, 3317, 1741, 932, 946, 1154, 1341], [2705, 963, 5240, 2348, 830, 2832, 3093, 233, 3803, 772, 3966, 198, 842, 5667, 3268, 2403, 5489, 5122, 1802, 3741, 1809, 4841, 1060, 5240, 5376, 2573, 5435, 2616, 4415, 355, 597, 589, 3263, 4402, 97, 693, 2616, 4415, 300, 577, 567, 1587, 3503, 224], [1741, 888, 2304, 842, 2805, 5636, 917, 1389, 2821, 842, 1348, 4523, 2832, 711, 1180], [5261, 1337, 5383, 3054, 4501, 963, 1624, 1534, 5384, 0, 0, 0, 1534, 5384, 0, 0], [5240, 5383, 5614, 1347, 1031, 3069, 2591, 4690, 3886, 4840, 1051, 3803, 4794], [4794, 2591, 3691, 4840, 1988, 4723, 1051, 3899, 4068, 597, 1313, 3822, 1741, 1267, 5303, 888, 2592, 5303, 2304, 2279, 5303, 3900, 4360, 3803, 3217, 2279, 5303, 3856, 1341, 4414, 4960, 5303, 842, 5240, 597, 1313, 4690, 3886, 1338, 4840, 1051, 2410, 772, 3310, 3888], [1060, 3093, 227, 842, 2321, 233, 198, 3966, 5667, 711, 1180, 5637, 4397, 952, 4794, 437, 3445, 597, 3856, 4068, 596, 3823, 3995, 1663]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('19330830')]\n",
      "[[814, 2955, 4360, 5122, 0, 3435, 5614, 5509, 5303, 0, 889, 5376, 1988, 3822, 5122, 1154, 2461, 3803, 3385, 4353, 1166, 3899, 4616, 272, 3534, 4012, 4346, 272]]\n",
      "[np.int64(0)]\n",
      "[np.str_('15378098')]\n",
      "[[5675, 5667, 711, 1135, 1180, 4395, 5303, 5240, 4753, 2217, 2998, 2591, 4883, 1682, 2832, 5330, 842, 4048, 5635, 1045, 842, 3197, 2856, 2832, 2304, 4745, 4988, 5667, 5240, 5259, 1634, 3803, 1297], [772, 5054, 1416, 5240, 2462, 948, 3803, 1312, 2779, 5250, 2304, 5561, 3077, 2278, 963, 1018, 842, 963, 5240, 5287, 3803, 5240, 3674, 5270, 1635], [1539, 4711, 3803, 374, 5675, 5652, 5637, 1038, 3891, 1297], [5303, 2197, 5240, 1992, 3803, 4753, 2217, 4255, 3822, 2304, 842, 4360, 3803, 3217, 4353, 2832, 5675, 5667, 3433, 1135, 1180]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('17664469')]\n",
      "[[5287, 5303, 4605, 842, 3873, 2306, 5201, 3899, 5198, 2410, 2471, 2177, 3955, 2832, 5240, 3646, 4803], [3873, 5614, 282, 597, 1313, 144, 5303, 391, 2410, 1972, 191, 597, 1313, 5303, 348, 2410, 5198, 842, 363, 597, 1313, 265, 5303, 450, 2410, 5201, 129], [5261, 4395, 4039, 2768, 5383, 2175, 5413, 1883, 1017, 4496, 5303, 0, 5648, 5684, 1028, 3544, 4271, 635, 5303, 3900, 4605, 4409, 3873, 2410, 1406, 2832, 4039, 2771, 5383, 5667, 2098, 1321, 2387, 1972, 932, 2358, 3233, 711, 2486, 1180, 5250]]\n",
      "[np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('16260701')]\n",
      "[[4733, 2980, 830, 4641, 3691, 4840, 1812, 2832, 5122, 4411, 5667, 5240, 3061, 2572, 842, 5240, 4068, 2572, 4832, 5692, 1850, 2439, 5122, 4411, 3803, 460, 597, 1313, 419, 5303, 502, 842, 508, 597, 1313, 477, 5303, 543, 4597, 842, 3900, 5692, 5122, 4411, 3803, 546, 597, 1313, 508, 5303, 567, 842, 559, 597, 1313, 539, 5303, 582, 4597]]\n",
      "[np.int64(1)]\n",
      "[np.str_('23993401')]\n",
      "[[3539, 3966, 2832, 5240, 119, 1663, 2572, 2591, 4598, 5153, 963, 1018], [2344, 2426, 3966, 5637, 2074], [460, 3966, 5374, 3899, 1664, 842, 341, 3803, 3966, 5374, 3899, 119, 1664, 2241, 670, 2559, 5337, 96], [1952, 2304, 842, 1572, 1891, 5637, 5680, 2832, 5240, 119, 1663, 2572, 963, 1018]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('23866850')]\n",
      "[[3966, 5667, 1180, 189, 5652, 750, 5303, 3949, 2832, 5240, 5042, 5637, 4395, 5303, 5240, 2998, 923, 584, 3856, 5240, 1533, 923, 584], [3966, 4397, 952, 5303, 5240, 2998, 923, 4831, 4840, 2827, 3822, 5153, 3803, 1741, 77, 842, 888, 77, 2631, 4523, 4353, 77, 1059, 2541, 2631, 4998, 842, 4048, 842, 2045, 2465, 842, 3197, 2936, 5642, 1403, 5667, 1536], [5240, 1053, 3803, 5240, 2998, 5637, 2175, 5512, 5240, 0, 4769, 4413, 1741, 4723, 4751, 5303, 3384, 1741, 5240, 4769, 4413, 888, 4723, 0, 5303, 944, 888, 842, 5240, 2167, 3863, 2410, 4577, 842, 5376, 3803, 1180, 4360, 3803, 3217, 4369, 1555, 322, 2090, 4351, 1156, 5303, 5120, 2631, 4523, 4353], [5240, 3310, 3773, 3803, 5240, 5042, 5614, 5303, 1773, 5240, 1053, 3803, 4325, 3000, 2410, 1180, 3966, 5652, 4442, 4687]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('20026818')]\n",
      "[[3397, 4462, 5637, 4647, 5303, 1773, 5337, 842, 5122, 4411], [5255, 2591, 2336, 5336, 842, 1059, 2462, 842, 2631, 4523, 4360, 3803, 3217, 3888, 1403, 5667, 5264, 4443, 1540, 0, 0, 2343, 4377, 3856, 5240, 2921, 2831, 4295, 5512, 5240, 1057, 0, 0, 4072, 5160], [5303, 944, 5336, 2462, 3888, 842, 2631, 4523, 4360, 3803, 3217, 957, 5667, 1457, 1293, 5250, 1610, 2832, 3966, 5667, 2627, 842, 3635, 1180], [2829, 0, 0, 4510, 842, 2844, 2178, 5667, 4940, 842, 5135, 0, 1036, 842, 1940, 5376, 3370, 2066, 3965, 3888], [913, 3828, 0, 2591, 0, 842, 3856, 0, 3522, 1761, 4092, 3625, 842, 5600, 4575, 2696, 3856, 3014, 0, 0, 3856, 3329, 4575, 2696, 842, 3461, 3856, 3507, 2334], [3544, 3966, 2591, 3870, 3856, 3149, 5404, 577, 842, 711, 4976, 1850, 543]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('10944126')]\n",
      "[[3966, 777, 5303, 3908, 4087, 3290, 2591, 4841, 3268, 4261, 2439, 5122, 3391, 3536, 6, 842, 1059, 4605, 4409, 437, 257, 4, 5642, 1403, 5667, 5240, 1533, 923], [2832, 4213, 5042, 3803, 5376, 2410, 711, 1377, 1180, 5240, 3290, 4495, 1436, 3204, 3289, 4087, 1114, 842, 2911, 2387, 481, 2186, 5632, 5614, 5096, 5303, 5240, 4983, 3737, 1260, 1180, 5376, 2572, 3371, 1336, 1663, 1114, 481, 3289, 4495]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('17075117')]\n",
      "[[5413, 5692, 5122, 4409, 5614, 191, 5667, 1668, 842, 5667, 1269], [1423, 3667, 5614, 3539, 2443, 5667, 1668, 5237, 1269, 311, 130], [711, 2486, 1180, 3966, 5637, 4397, 952, 5303, 1883, 543, 3445, 3303, 842, 1321, 543, 3445, 3303, 1663, 4087, 2387, 544, 3445, 3303, 1664, 5303, 2186, 5632, 3856, 1321, 101, 3445, 3303, 1663, 4087, 2387, 5706, 3445, 3303, 1664, 5303, 2186, 5632], [2559, 5303, 5376, 4523, 715, 2183, 3791, 2832, 515, 1668, 477, 1269, 3803, 3966], [3900, 4605, 4409, 5614, 2663, 5667, 1668, 1300, 32]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('11790211')]\n",
      "[[3833, 3470, 4824, 5222, 4353, 1053, 5637, 2424, 5667, 3130, 2410, 1374, 1180, 1403, 5667, 4983, 3837, 1366], [3803, 416, 3966, 0, 4302, 4353, 1659], [5240, 3379, 3391, 2541, 4413, 4723, 4745, 2410, 5632, 4127, 5637, 546, 556, 2410, 3130, 5601, 539, 543, 2410, 3837, 1366, 31], [2832, 814, 2965, 5303, 5372, 830, 1405, 4751, 3921, 2962, 4751, 5091, 4353, 2866, 5091, 842, 2541, 4413, 4723, 4745, 963, 1960, 5287, 4095, 5240, 3833, 4995, 4840, 1811, 3781, 1060, 2573, 5614, 5240, 2541, 4413, 4723, 4743, 2410, 5632, 4127]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('20227168')]\n",
      "[[5240, 2749, 3324, 4815, 3379, 4743, 4831, 1059, 4623, 2832, 2572, 5237, 2832, 2572, 3966, 963, 1121, 3498, 156, 5601, 191, 842, 3498, 5601, 130, 733, 4680, 13], [1760, 2827, 2832, 5116, 5208, 5499, 2851, 5423, 3054, 3741, 0, 733, 4378, 4291, 4680, 842, 3069, 3370, 0, 0, 4360, 3803, 3217, 4353]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('23545101')]\n",
      "[[3691, 4840, 1277, 3899, 5287, 2832, 4360, 3803, 3217, 1891, 5637, 1767, 2410, 3966, 3822, 1006, 1172, 2580, 842, 2505, 2203, 2410, 4048, 2465, 5648, 1688, 4841, 2832, 1121, 2573, 13], [5376, 5614, 5635, 5316, 2832, 1121, 2573], [3885, 3575, 4287, 4395, 4039, 2768, 5042, 2842, 131, 3966, 5667, 2983, 4662, 3710, 3022, 1091, 1180], [5625, 1403, 5240, 4360, 3803, 3217, 3803, 3966, 5667, 3710, 3022, 1091, 1180, 5652, 4442, 697, 3016, 2505, 3856, 1896, 1006, 1172, 2580]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('18285602')]\n",
      "[[1135, 2831, 4841, 2825, 5240, 1896, 1869, 1403, 5667, 4983, 4377], [1152, 5240, 4192, 3803, 3522, 1761, 1802, 4841, 1559, 5667, 3921, 13, 842, 4477, 4360, 3803, 3217, 16], [3966, 5637, 946, 1960, 5629, 1940, 842, 5489, 5303, 5632, 733, 4386], [1135, 2831, 4841, 4477, 5240, 3792, 3803, 3522, 1761, 1403, 5667, 4983, 0, 5207], [5330, 3803, 362, 3966, 5637, 4397, 952, 1060, 3093, 230, 842, 3345, 232, 2832, 5413, 1178, 1259, 842, 0, 5637, 2842, 2832, 5240, 830]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16942833')]\n",
      "[[733, 173, 5674, 1121, 2573, 5128, 1936, 4068, 842, 1523, 4510], [5240, 1687, 2832, 2758, 842, 2827, 2832, 4353, 4745, 5637, 4841, 2567, 2832, 2572, 2410, 5240, 2358, 173, 5674], [715, 2183, 2410, 1936, 5614, 164], [5645, 733, 272, 5674, 327, 2832, 2572, 5562, 391, 2832, 2572, 5637, 1929, 94], [106, 3414, 1416, 5240, 5042], [733, 173, 5674, 376, 3966, 5562, 300, 5637, 1929, 28]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('12610183')]\n",
      "[[5240, 3391, 4031, 5614, 348, 842, 341, 3536, 2410, 1251, 842, 1971, 4597, 511, 842, 5240, 5692, 5122, 4409, 5614, 450, 842, 442, 2410, 1251, 842, 1971, 4597, 596], [5240, 5042, 5614, 1756, 5303, 1765, 164, 2826, 5238, 3054, 2446, 437, 5303, 502, 2832, 3391, 4261, 2439, 5122, 4031, 2832, 2306, 3803, 5240, 1896, 2960, 4495], [3258, 711, 1135, 1180, 3966, 5637, 4397, 952, 3835, 5042, 1405, 1636, 543, 3445, 3858, 1664, 5303, 156, 2098, 483, 3445, 3015, 3077, 1664, 842, 2387, 438, 3445, 3077, 1664, 4862, 1635, 2186, 304, 1664, 5562, 131, 3445, 3077, 1663, 0, 3445, 3077, 1663, 842, 0, 0, 5011, 2280, 0, 0, 3109, 5047, 1664, 5303, 144, 4862, 1635, 2186, 156, 1664]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('20151824')]\n",
      "[[2403, 5489, 2196, 5637, 4007, 130, 3536, 733, 616, 5303, 944, 616, 3887, 2832, 1960, 2572, 1154, 146, 5653, 1111, 4728, 0, 4795, 0, 3386, 733, 5394, 5012, 842, 3635, 5428], [3743, 616, 4182, 5512, 5668, 3803, 3283, 2410, 5632, 1802, 3741, 4208, 1779, 3803, 4250, 2741, 932, 789, 3791, 5642, 3284, 783, 5614, 5669, 2410, 5632]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('23504821')]\n",
      "[[5376, 924, 5637, 1403, 5512, 5240, 5025, 3263, 4402, 5230, 842, 1587, 4281, 2617, 3503, 693, 2410, 5383, 5027, 5583, 3430, 4213, 2692, 4780, 737, 4811, 0, 1018, 2541, 2631, 4998, 4743, 842, 1968, 1548, 3827, 2572, 4006, 4998, 4253, 4662, 2282, 842, 5376, 2678], [5240, 4395, 1534, 1112, 1135, 1180, 5384, 3803, 3857, 2185, 5383, 1725, 4841, 2825, 4261, 2439, 5122, 5667, 5240, 5508, 3803, 2185, 4087, 2216, 2179, 2214, 5562, 4068, 4087, 2216, 3972, 2214, 2832, 3966, 5667, 711, 1135, 1180, 5652, 1778, 1850, 4261, 733, 5376, 5667, 3726, 925, 2919], [5240, 3391, 5203, 2832, 2711, 5614, 3536, 5667, 2179, 2214, 5562, 3536, 5667, 3972, 2214, 2616, 4415, 539, 0], [5240, 322, 3072, 2832, 164, 5061, 3803, 5240, 4351, 1156, 2841, 2541, 2631, 4998, 0, 2663, 4745, 4398, 101, 2869, 1059, 2711]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('20362386')]\n",
      "[[4117, 4459, 3803, 5240, 0, 1421, 1888, 3741, 893, 5303, 2824, 1963, 5499, 2851, 733, 0], [5303, 3025, 5646, 3885, 5561, 3803, 4117, 0, 4459, 5663, 4476, 1963, 2851, 842, 2614, 1049, 1988, 3822, 3966, 4360, 3803, 3217, 4353], [4840, 4280, 3803, 3966, 1777, 5499, 2851, 1963, 733, 4378, 4291], [0, 5667, 3391, 0, 0, 4459, 3444, 2404, 1154, 2414, 3803, 5240, 0, 837, 2998, 2572, 5562, 4983, 837, 5673, 4117, 4459, 1533, 2572], [2410, 2998, 842, 1533, 2573, 4597, 3379, 1229, 1939, 5614, 119, 539, 842, 130, 539, 0, 3176, 3822, 1638, 5614, 4193, 2832, 4862, 842, 2006, 1219, 304, 842, 2851, 889, 0, 5502, 3272, 963, 3498, 5614, 543, 842, 515, 377, 842, 963, 3498, 5614, 442, 842, 404, 0], [3828, 2717, 4805, 1485, 3966, 5447, 4217, 0, 956, 4378, 3140, 4291, 0, 4007, 1154, 4851, 5112, 963, 3828, 5227, 4485, 3827, 2942, 5637, 0, 952, 3741, 4393, 5303, 2998, 450, 3856, 1533, 2573, 454]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('24045440')]\n",
      "[[4863, 3053, 2979, 4652, 5042, 2771, 842, 3077, 3966, 2591, 3261, 3291, 3695, 3036, 963, 1793], [2006, 3966, 2591, 2037, 2675, 842, 272, 3966, 2591, 0, 2675], [797, 3585, 2986, 5042, 5384, 2224, 2410, 5240, 5376, 3803, 4652, 3833, 5563, 3230, 2906, 3054, 2533, 3822, 5376, 3887, 842, 4353, 2832, 4227], [2462, 3267, 5222, 2403, 5489, 5614, 2175, 1154, 2492, 4353, 5120], [5240, 3391, 0, 2322, 2851, 4743, 5614, 4111, 4398, 224, 842, 5240, 3391, 4353, 4743, 5614, 586, 0, 4398, 0], [3966, 5667, 2037, 2675, 2591, 4841, 1059, 5692, 2001, 577, 5237, 3966, 5667, 0, 2675, 376, 37]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('23525453')]\n",
      "[[4084, 2385, 5614, 1370, 2446, 1121, 2573, 4224, 5303, 842, 2405, 5376], [1061, 0, 3530, 867, 736, 5546, 2059, 2574, 2280, 5553, 2613, 1035, 4833, 5303, 1028, 1996, 2832, 0, 5240, 0, 3803, 4084, 2385], [1380, 3013, 5250, 5667, 1061, 842, 1321, 5614, 1989, 842, 4698, 2832, 3336, 3755, 3395, 3555], [2705, 5646, 3013, 1719, 3803, 1061, 1176, 1028, 5509, 5303, 5372, 3555, 4544, 5472], [2832, 678, 1378, 5250, 4831, 2567, 1994, 2832, 5240, 3966, 5667, 2662, 3209, 3803, 5553, 2257, 32], [5240, 2358, 2572, 4442, 3013, 1061, 323, 3445, 5667, 1321, 322, 3445, 5250, 842, 5240, 4754, 2572, 4442, 3013, 1321, 322, 3445, 5250, 783], [2832, 5240, 1061, 2572, 5240, 3209, 3803, 5553, 2832, 5240, 4084, 2385, 5637, 4841, 3275, 1403, 5303, 5264, 3803, 5240, 1321, 2572, 733, 5376, 5648, 4831, 2567, 1994, 32]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('15738537')]\n",
      "[[2616, 4415, 5614, 437, 5], [4701, 5614, 789, 946], [3391, 5122, 5614, 173, 3536, 2410, 5240, 2453, 3289, 1061, 2572, 842, 130, 3536, 2410, 5240, 2453, 3289, 4068, 2572, 2616, 4415, 553, 173], [1061, 3530, 867, 736, 5546, 2059, 2574, 2280, 2858, 5122, 5642, 1380, 5667, 3047, 1017, 1297, 2832, 2358, 3233, 5376, 3803, 3433, 1377, 1180, 1594], [3966, 5637, 4397, 952, 5303, 2453, 3289, 4068, 0, 3856, 2453, 3289, 1061, 110], [1152, 5614, 1534, 5667, 3857, 3399, 842, 1802, 3741, 1233, 5042, 1927, 1843], [2616, 4415, 5614, 396, 0], [5240, 4217, 2051, 4095, 5614, 3900, 5122], [4755, 2051, 4096, 5637, 4261, 2439, 5122, 4605, 4409, 4605, 1939, 842, 4360, 3803, 3217]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('17664468')]\n",
      "[[4353, 5614, 4199, 3268, 2410, 3966, 3822, 1668, 5237, 5264, 3822, 1269, 2410, 772, 5287, 5303, 1770, 828, 1727, 5240, 4994, 5097, 3803, 1668, 1403, 5667, 1269], [2832, 5524, 711, 2486, 3856, 2491, 3094, 1180, 3966, 4443, 1668, 3741, 3833, 2591, 4995, 2825, 3900, 5122, 842, 5287, 5303, 5403, 4261, 1152, 5255, 789, 2591, 1059, 4198, 3803, 4353, 1403, 5667, 3966, 4443, 1269], [5287, 5303, 1770, 3803, 2541, 2631, 4998, 4217, 2051, 4095, 4841, 2309, 1668, 3899, 1269, 3263, 4402, 5230, 32]]\n",
      "[np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('23945243')]\n",
      "[[2832, 5240, 2501, 2572, 3873, 5614, 183, 265, 1669, 5614, 376, 265, 842, 3556, 5614, 3536], [5240, 1992, 4701, 842, 4353, 5637, 1774, 842, 834], [5240, 753, 3803, 5261, 5042, 3054, 5303, 2252, 5240, 1992, 4701, 842, 4360, 3803, 3217, 4353, 3803, 2501, 842, 3992, 3822, 3966, 5667, 711, 3698, 4962, 3755], [2472, 5240, 3544, 1389, 5336, 2832, 5240, 3992, 2572, 5637, 3667, 376, 144, 842, 2304, 348, 549, 5645, 5264, 2832, 5240, 2832, 2501, 2572, 5637, 4870, 4408, 348, 549, 842, 1798, 183, 376]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('10880548')]\n",
      "[[1337, 948, 842, 2998, 4255, 2410, 3415, 5150, 3334, 2832, 1135, 1180, 5126, 3054, 2318, 842, 629, 5303, 3966, 3175, 5303, 4480, 2832, 5153, 842, 2826, 2832, 4812, 2465], [772, 4994, 5234, 5637, 5413, 4836, 842, 5637, 4007, 963, 5240, 786], [5240, 4337, 3803, 5261, 5042, 5614, 5303, 5230, 5240, 1994, 3803, 1433, 3415, 948, 1353, 2998, 4255, 2832, 648, 4536, 3803, 5153, 5240, 2826, 2832, 4360, 3803, 3217, 4353, 842, 4812, 2465, 2832, 1135, 1180, 5126], [5240, 2998, 5322, 4067, 3899, 3534, 4012]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('18707619')]\n",
      "[[2594, 3209, 5356, 3637, 842, 4353, 2462, 948, 3803, 1180, 5250, 2304, 2279, 5060, 4743, 5637, 946, 963, 4506, 2997], [3379, 4749, 2856, 2832, 2618, 2446, 1018, 5303, 5629, 130, 5614, 543, 164, 1877, 2832, 5240, 322, 5706, 3075, 2572, 29, 5601, 1018, 842, 67, 543, 1877, 2832, 5240, 224, 5706, 3075, 2572, 3698, 4840], [2103, 1058, 4405, 2858, 2594, 2618, 3209, 842, 2828, 4353, 2832, 816, 3966, 5667, 0, 3803, 5406], [5261, 5614, 4395, 1901, 1096, 3939, 2572, 1896, 2352, 5042, 947, 5240, 1994, 842, 4701, 3803, 3823, 5631, 2103, 1058, 2832, 3966, 5667, 4909, 5406, 4443, 1297], [815, 3054, 1389, 2832, 3966, 4443, 1297, 1236, 5153, 5238, 2614, 3317, 2802, 3822, 4360, 3803, 3217, 4353], [709, 3966, 5667, 815, 2618, 119, 1877, 5637, 4395, 5303, 4441, 2103, 1058, 322, 5706, 3075, 3856, 224, 5706, 3075, 3823, 5631, 2410, 130, 5632], [5356, 5508, 5614, 3274, 1940, 5240, 5042, 2832, 1121, 2573]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('12202326')]\n",
      "[[5303, 1773, 5646, 3966, 5667, 3258, 711, 3698, 4888, 1253, 3286, 1180, 5485, 2410, 4581, 3856, 4378, 4386, 842, 5667, 3470, 5262, 5153, 4828, 1028, 2533, 3929, 5262, 4386, 2791, 3856, 932, 3638, 5303, 5372, 5153], [2832, 5240, 1713, 5376, 2572, 427, 124, 396, 3966, 4442, 5262, 4386, 311, 4442, 3828, 3803, 5240, 4457, 4496, 497, 467, 1803, 5673, 4443, 5262, 4386, 5240, 4543, 5270, 5637, 770, 963, 5240, 2051, 3803, 5240, 5042, 5673, 2615, 4442, 5240, 5376], [266, 3966, 5667, 4214, 5487, 3698, 4888, 1253, 3286, 1180, 5238, 3054, 3258, 5321, 711, 2410, 4581, 3856, 4378, 4386, 5667, 1623, 2964, 5667, 3470, 5262, 5153, 842, 5667, 3691, 2873, 2410, 2790, 5262, 4386], [4755, 4360, 3803, 3217, 715, 2183, 5122], [3691, 2188, 3803, 1811, 5614, 3781, 1060, 5240, 5413, 5376, 2573, 2832, 5226, 3803, 664, 3208, 888, 1741, 842, 4322, 1865, 932, 4461, 1154, 5240, 3966], [110, 124, 3803, 5240, 3966, 2832, 5240, 2790, 5376, 2572, 4442, 5262, 4386, 586, 4442, 3828, 3803, 5240, 4457, 4496]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('14996859')]\n",
      "[[808, 772, 5376, 2573, 3966, 5652, 2591, 3362, 2591, 5240, 0, 4048, 2465, 963, 4502, 9, 842, 963, 2076, 77], [772, 4994, 5234, 5637, 5413, 4836], [1337, 3000, 5303, 681, 1389, 5153, 957, 5667, 5376, 4828, 1028, 1499, 5303, 2824, 4048, 842, 2045, 2465, 963, 5240, 2051, 3803, 4217, 5376, 2410, 1135, 1180]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('22551128')]\n",
      "[[5240, 4283, 3803, 3966, 5667, 715, 2183, 5637, 246, 2832, 5240, 3274, 1896, 2572, 5562, 341, 2832, 5240, 2662, 1896, 2572, 28, 842, 265, 2832, 5240, 5283, 766, 2572, 5562, 322, 2832, 5240, 2572, 5447, 5281, 2692, 5668, 119], [4843, 4623, 5637, 2424, 2410, 3274, 1896, 4382, 4087, 5283, 766, 567, 5562, 2662, 1896, 4382, 4087, 5281, 2692, 5668, 577, 3856, 2662, 1896, 4382, 4087, 5283, 766, 586], [3539, 3966, 2832, 5240, 2662, 1896, 2572, 5237, 2832, 5240, 3274, 1896, 2572, 5637, 2697, 2410, 963, 3180, 1664, 363, 5601, 144, 9], [616, 5074, 4411, 5637, 570, 2832, 5240, 2572, 4443, 3274, 1896, 4382, 5562, 582, 2832, 5240, 2572, 4443, 5240, 2662, 1896, 842, 577, 2832, 5240, 5283, 766, 2572, 5562, 574, 2832, 5240, 2572, 5447, 5281, 2692, 5668]]\n",
      "[np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('20842129')]\n",
      "[[1659, 5637, 827, 5489, 5303, 3536], [963, 3536, 733, 5240, 2051, 3803, 4687, 2541, 2711, 5614, 2663, 2832, 5240, 5338, 923, 5237, 2832, 5240, 4028, 923], [1152, 5303, 3230, 2264], [2711, 5614, 946, 963, 1018, 963, 2051, 3803, 1634, 842, 842, 3536, 733, 1420, 3803, 4687, 5512, 5240, 2167, 3862, 2410, 4577, 842, 5376, 3803, 1180, 2090, 4360, 3803, 3217, 4369, 1156, 4351, 1156, 842, 5240, 2090, 4351, 2627, 842, 3635, 1180, 4936, 3520, 2090, 4351, 812, 0], [1152, 5240, 3274, 1422, 1888, 3741, 779, 5303, 0, 1706, 1453], [5240, 4217, 2711, 4723, 5614, 2541, 2711, 3995, 4295]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('17307761')]\n",
      "[[5098, 130, 5629, 2572, 2217, 4256, 2832, 678, 5303, 5513, 1205, 1403, 5667, 5513, 1205], [3691, 715, 1992, 5637, 3744], [1341, 4828, 0, 664, 2410, 5241, 3966], [5303, 1773, 2462, 842, 4322, 1053, 3803, 130, 5629, 5098, 2572, 2217, 4256, 1940, 5376, 2410, 1963, 4976, 1135, 1180, 5667, 4862, 3534, 2403, 5489], [3488, 1992, 3506, 5667, 695, 2410, 1018, 5536, 5042, 4858, 5376, 963, 1018, 842, 737, 2494, 2998, 1988, 2149, 2998, 3478, 1533, 963, 130, 5632, 3803, 142, 597, 1466, 2996, 566, 5303, 187, 2410, 0, 0, 2832, 130, 3480, 194, 543, 5303, 310, 2410, 3480, 3803, 3507, 2962, 664, 4563, 2832, 5629, 5303, 2410, 4829, 3499, 5303, 2410, 1135, 1180, 4936, 5060, 3803, 4360, 3803, 3217, 842, 5303, 2410, 4108, 3537], [3691, 4840, 1988, 5614, 4761, 2410, 2508, 4360, 3803, 3217, 2279, 5648, 5614, 5240, 4217, 3887], [0, 5675, 2082, 5240, 5042, 188, 1416, 5240, 4862, 3534, 2403, 5489], [5270, 3622, 2631, 4797, 3827, 1343, 2832, 0, 842, 1394, 2217, 0], [4151, 4393, 1534, 4287, 3837, 5383]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('17999405')]\n",
      "[[2998, 4957, 4563, 2663, 4360, 3803, 3217, 3539, 4769, 1994, 1059, 1393, 842, 3197, 3641, 903, 3803, 1209, 5441, 2690, 842, 5150, 1865, 963, 3536, 1403, 5667, 1536, 842, 4916, 1992, 5637, 5133, 5303, 3536, 842, 130, 3536], [5240, 2998, 5190, 1580, 1393, 0, 1553, 5441, 842, 5150, 3334], [1152, 5255, 4563, 3691, 3881, 1992], [963, 3534, 2403, 5489, 2998, 3966, 4563, 3197, 5441, 842, 1059, 1393, 5667, 4957, 5237, 1533, 3966]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('19050973')]\n",
      "[[5625, 4920, 5240, 2264, 5303, 5648, 923, 3538, 1573, 1028, 4477, 1154, 5512, 4783, 3291, 3695, 1017, 3334, 2832, 5675, 5667, 1338, 3695, 3641, 1963, 1135, 1180], [3966, 2832, 5240, 4899, 2572, 2494, 3275, 4414, 2410, 923, 5139, 9, 5153, 9, 842, 1949, 45, 1152, 3741, 1834], [4783, 3696, 5637, 0, 5512, 0, 0, 783, 3856, 5667, 0, 0, 0, 0, 0], [3828, 5268, 2008, 2006, 5675, 5637, 4397, 777, 5303, 4783, 3291, 3695, 1085, 2404, 1154, 1004, 1331, 2759, 5240, 4783, 3695, 5614, 4108, 3856, 3741, 1767, 4899, 3856, 4677, 1004, 1331, 4376, 4783, 3291, 3695, 1085, 2404, 2791, 1154, 1004, 1331]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('16135469')]\n",
      "[[1135, 1180, 3966, 5637, 4501, 5672, 5632, 733, 5115], [4755, 2051, 4096, 5637, 1742, 5153, 842, 4113, 0, 2574], [1152, 1981, 0, 2567, 4480, 2832, 5261, 3887, 4527, 5303, 1617, 963, 3536, 2410, 3966, 5652, 0, 3539, 4184, 2410, 4428, 2084], [2832, 828, 1535, 2410, 5042, 4858, 842, 1018, 1742, 5153, 5568, 4243, 4840, 2826, 2832, 2065, 2304, 963, 3536, 4527, 5303, 1617, 3955, 808, 5675, 5652, 0, 3197, 4184, 2410, 4428, 2084, 963, 1018]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('16135471')]\n",
      "[[3691, 4840, 1812, 5637, 2753, 2410, 889, 4154, 3856, 4113, 5230, 1275, 4745, 2410, 1466, 5667, 3336, 2304, 1180, 4769, 1994, 888, 1741, 3856, 4360, 3803, 3217], [2949, 2842, 3762, 4413, 4723, 947, 1466, 5667, 3336, 2304, 119, 4095, 3762, 4413, 4724, 3389, 2304, 963, 5683, 994, 842, 1057, 5240, 2462, 948, 3803, 1180, 5250, 2304, 842, 4061, 2304, 4724, 5240, 1180, 4769, 1994, 4723, 5240, 2167, 3862, 2410, 4577, 842, 5376, 3803, 1180, 4360, 3803, 3217, 4369, 1156, 842, 5240, 2695, 888, 842, 1741, 4723], [2705, 2471, 4577, 3054, 3638, 5303, 2754, 5621, 5303, 2824, 5240, 0, 842, 0, 3803, 4320, 3000, 2410, 3336, 1180, 4523, 2304], [5261, 4395, 1534, 5383, 3035, 118, 5675, 1386, 697, 1297, 2410, 4976, 3856, 2768, 1135, 1180, 2832, 2364, 1297, 5376, 1259], [0, 1984, 842, 5103, 2613, 5240, 4133, 5303, 955, 5675, 5303, 1551, 5667, 1180, 4523, 2304, 2832, 5240, 4824, 5222]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('19858379')]\n",
      "[[5261, 5378, 2410, 3875, 1051, 2410, 2503, 1188, 5614, 1502, 654, 1813, 4253, 5050, 635, 5303, 1018, 5024, 2282, 4976, 842, 4006, 4998, 842, 4542, 733, 694, 2410, 5254, 5024, 2282, 92], [3540, 5240, 3424, 830, 3803, 5413, 679, 5041, 3037, 0, 3966, 4831, 4840, 5122, 1051, 2832, 2306, 3803, 2503, 1188, 2708, 574, 597, 1313, 543, 5303, 602, 45, 5667, 3691, 0, 2658], [3966, 5667, 4214, 5487, 2673, 3856, 0, 4300, 3258, 711, 3856, 3433, 1198, 3803, 5240, 3932, 5667, 4006, 4998, 3856, 5637, 4467]]\n",
      "[np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('17143593')]\n",
      "[[5240, 3988, 5033, 4255, 1802, 3741, 2902, 5240, 3888, 3385, 1031, 5240, 5153, 4563, 1154, 3966, 5637, 3741, 1490, 3803, 0], [4829, 4398, 3803, 3546, 5028, 923, 1317, 842, 4360, 3803, 3217, 3387, 5637, 5179, 4224, 5303, 842, 963, 1420, 3803, 4386, 842, 963, 3536, 733, 4386], [5240, 2838, 3803, 3293, 1940, 5240, 5042, 5614, 3274, 2410, 1121, 2573, 842, 1802, 3741, 1809, 1060, 2573]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15547181')]\n",
      "[[5625, 1403, 5240, 1378, 3803, 1883, 1195, 5667, 5240, 1378, 3803, 3917, 1195, 932, 2358, 3233, 1297, 2410, 4976, 2747, 3077, 2101, 3897, 3856, 4217, 0, 1180], [733, 3391, 2403, 5489, 3803, 265, 3536, 1121, 2573, 2591, 4843, 4261, 2439, 5122, 3392, 3803, 164, 3536, 2410, 1883, 1195, 842, 156, 3536, 2410, 3917, 1195, 2616, 4415, 2708, 1883, 3917, 600, 597, 1466, 2996, 1313, 566, 5303, 144, 0, 3900, 5122, 4411, 963, 5693, 497, 842, 511, 4597, 2708, 144, 597, 1313, 590, 5303, 376, 0, 842, 3773, 5403, 475, 842, 477, 4597, 1811, 1060, 1883, 842, 3917, 597, 1313, 5303, 0, 842, 0, 138, 543, 842, 546, 4597, 1811, 1883, 3917, 597, 1313, 5303, 0, 4605, 4411], [1297, 5667, 4079, 740, 842, 5196, 3917, 3054, 1499, 5240, 4983, 3803, 1205, 2410, 5376, 3803, 3897, 1198], [5376, 5667, 1883, 1195, 5614, 957, 5667, 4995, 4841, 3539, 2559, 3667, 596, 5562, 567, 1811, 119, 597, 1313, 5303, 156, 9, 842, 3668, 1425, 5237, 5376, 5667, 3917, 1195], [797, 3606, 1802, 3741, 2902, 1896, 1719, 3856, 3965, 4701]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('16321761')]\n",
      "[[678, 3803, 5214, 1940, 842, 733, 4386, 2410, 3966, 5667, 3673, 1791, 2538, 4841, 2825, 5122, 5673, 3641, 1988, 3822, 2711], [1018, 2711, 4745, 1802, 3741, 1809, 1060, 2573], [1018, 4370, 5637, 993, 2410, 0, 574, 3966], [5240, 4839, 3803, 842, 4283, 3803, 3966, 5667, 2825, 2711, 4745, 1703, 932, 1275, 3803, 100, 4096, 3856, 3539, 5637, 4461], [5625, 1166, 1277, 2446, 1018, 4743, 2410, 4805, 4158, 2711, 3388, 2304, 3900, 2631, 4902, 2461, 2045, 2461, 2474, 5441, 2936, 842, 1393, 1700, 842, 1812, 1060, 2573, 2410, 5254, 3388, 963, 2186, 5287, 4095], [5261, 0, 4565, 5240, 2631, 4523, 4360, 3803, 3217, 2711, 3803, 5240, 3966, 2832, 5261, 5383]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15472910')]\n",
      "[[2446, 215, 220, 0, 2010, 3966, 5637, 639, 842, 0, 5637, 2173], [5240, 5042, 0, 5240, 5535, 3803, 5454, 5041, 2832, 919, 3803, 1850, 4207, 842, 5240, 0, 3803, 4765, 910, 3887, 3388], [5240, 4337, 3803, 3885, 5042, 3054, 5303, 1402, 5240, 1991, 3803, 5105, 1205, 5601, 2998, 908, 3618, 3857, 2160, 1927, 1378, 842, 4386, 5512, 4360, 3803, 3217, 4353, 932, 5240, 4217, 3384, 3803, 5074], [3100, 4715, 3054, 1625, 5240, 3544, 1389, 5403, 2832, 5701], [3857, 2160, 5250, 4621, 2832, 1059, 5330, 2377, 4353, 4743, 5237, 4386]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23477998')]\n",
      "[[3965, 2711, 5614, 946, 5512, 5240, 2462, 948, 3803, 1180, 5250, 3286, 4369, 2832, 5226, 3803, 5287, 5303, 5150, 4261, 5396, 5287, 5303, 1770, 5398, 2832, 5383, 3887, 2866, 5311, 842, 5398], [932, 5376, 2410, 3496, 3054, 3698, 1623, 3074, 2802, 3822, 3965, 2631, 4523, 4360, 3803, 3217, 2711, 3054, 814, 2821, 1498], [3698, 4840, 5378, 5334, 1713, 5287, 5303, 1571, 842, 5287, 5303, 1953, 2708, 547, 597, 1313, 434, 246, 842, 2708, 543, 597, 1313, 427, 183, 4597, 5614, 789, 3781]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('11142481')]\n",
      "[[2705, 5240, 3900, 5122, 5614, 4843, 5084, 5238, 794, 1114, 842, 2911, 2453, 0, 3370, 1028, 932, 1989, 932, 2534, 5242, 2832, 4787, 932, 2358, 842, 4754, 3233, 5376], [5413, 2717, 2427, 3966, 2446, 3682, 3070, 1259, 5637, 4395, 5303, 5240, 1533, 923, 1504, 3803, 1088, 1635, 3803, 3567, 225, 3445, 3303, 3822, 1663, 2404, 1154, 1114, 2453, 484, 3445, 3303, 3822, 1663, 842, 3204, 4576, 3856, 5303, 5240, 2244, 923, 1504, 3803, 5413, 1088, 1635, 3803, 5240, 4709, 4495, 932, 2832, 5240, 1533, 923, 0, 5303, 5270, 5632, 3803, 1526, 2910, 2453, 225, 3445, 3303, 1663, 5631, 1114, 3204, 224, 3445, 3303], [5625, 2614, 4445, 5083, 5238, 1114, 2387, 2453, 3370, 5676, 5566, 0, 1831, 0, 5649, 1526, 2910, 2453, 3370, 0, 1254, 5566, 5280, 5157, 4523, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16203762')]\n",
      "[[3391, 5122, 5614, 156, 3536, 5214, 4386, 5562, 130, 3536, 4386, 783], [3822, 3345, 164, 232, 5240, 5505, 2408, 842, 1927, 700, 0, 5214, 0, 1192, 0, 0, 4577, 2940, 2410, 5240, 5376, 3803, 709, 3966, 5667, 3673, 1791, 2538, 3582, 0, 5667, 4386, 842, 5245, 932, 3316, 5376], [3966, 4443, 1454, 842, 3316, 5214, 4386, 2591, 4841, 2825, 3900, 5122], [2364, 2717, 4807, 5270, 2538, 3582, 3966, 5637, 4395, 5303, 4441, 2009, 5214, 4386, 309, 3856, 4386, 783, 308]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('19546404')]\n",
      "[[3966, 5652, 4442, 1970, 4563, 4841, 2663, 3209, 3803, 1798, 842, 1126, 5153]]\n",
      "[np.int64(1)]\n",
      "[np.str_('18308419')]\n",
      "[[3900, 0, 3966, 5637, 2074, 3005, 5240, 5383], [4039, 2771, 4395, 5383, 5614, 1215, 3886, 5303, 1402, 5413, 4734, 3803, 5240, 5581, 5595, 1321, 1247, 4495, 2832, 3966, 5667, 3258, 711, 5480, 4103, 4252, 4976, 2773, 3856, 3433, 4976, 3077, 3698, 4888, 1253, 3286, 1180], [5240, 5413, 4734, 917, 2111, 2832, 5226, 3803, 3900, 4605, 4409, 3391, 5287, 5303, 4261, 842, 3900, 5122]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23551928')]\n",
      "[[4812, 1948, 4570, 1421, 842, 3581, 1510, 5238, 1176, 725, 1121, 3414, 842, 5675, 842, 2613, 1035, 3744, 5303, 3813, 0, 4841, 733, 5376, 2410, 4469, 842, 818, 1180], [5675, 2074, 2832, 5240, 5042, 2998, 5637, 3822, 994, 460, 5693, 3815, 4168, 1231, 553, 3357, 471, 842, 3391, 3803, 5693, 0, 5376], [842, 5238, 972, 5303, 4812, 4525, 4716, 3370, 1028, 1600, 2832, 5240, 1779, 842, 2816, 3803, 3000, 2410, 5261, 1364, 3803, 3966], [5240, 3310, 3887, 3388, 917, 4360, 3803, 3217, 4353, 2167, 3863, 2410, 4577, 842, 5376, 3803, 1180, 1555, 4360, 3803, 3217, 4369, 2090, 4351, 1156, 842, 1377, 1180, 4936, 3520, 4351, 0, 4812, 2465, 2331, 4812, 2465, 2866, 842, 4322, 5635, 1045, 1140, 5150, 3023, 1741, 888, 2802, 3803, 2183, 4723, 4649, 1590, 374, 1111, 2782], [1760, 5261, 3069, 4544, 814, 0, 0, 842, 5455, 3067, 2832, 5240, 2343, 3803, 1180, 5127], [2410, 5264, 4564, 4812, 664, 963, 1018, 391, 4812, 1948, 5614, 957, 5667, 4398, 3803, 4936, 3388, 3803, 4322, 5635, 1045, 772, 2832, 5240, 2738, 1832]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('20006921')]\n",
      "[[364, 4068, 5614, 2533, 3822, 1663, 842, 2505, 102, 3445, 1384, 3822, 1663, 2404, 1154, 364, 3822, 1663], [963, 5240, 2051, 3803, 5240, 5042, 490, 282, 842, 483, 2591, 4974, 1850, 2832, 5240, 131, 3445, 273, 3445, 842, 4068, 2572, 4597], [4214, 5487, 3966, 5667, 3973, 5637, 4397, 777, 5303, 364, 131, 3445, 273, 3445, 3856, 4068], [364, 3054, 4698, 842, 5635, 5316, 5642, 1380, 5667, 2505]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('22857983')]\n",
      "[[5330, 3803, 462, 3966, 5656, 4290, 4936, 877, 4313, 1688, 5303, 100, 3675, 3490, 3856, 1154, 437, 224, 3675, 3490, 963, 1018, 5637, 4393, 5303, 2744, 3856, 1163]]\n",
      "[np.int64(0)]\n",
      "[np.str_('19273701')]\n",
      "[[4028, 1275, 4745, 5637, 2410, 1268, 842, 2410, 1147, 73, 963, 5632, 842, 5637, 842, 130, 2410, 1268, 842, 1147, 4597, 54, 963, 173, 5632], [1268, 3808, 2821, 2710, 842, 5122, 1053, 2410, 4205, 3966, 5667, 711, 5662, 5414, 3124, 1594], [3966, 5667, 5662, 5414, 4998, 5652, 4442, 1268, 2241, 2825, 2527, 963, 5632, 5645, 3966, 5652, 4442, 1147, 783, 1769, 13], [5261, 930, 4565, 5240, 2631, 4523, 4360, 3803, 3217, 2710, 3888, 2446, 1357, 183], [4113, 2683, 830, 1154, 3124, 3598, 4998, 5614, 4007]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('15454357')]\n",
      "[[841, 3822, 5376, 0, 4831, 5238, 3948, 5652, 0, 814, 2217, 4455, 4563, 4841, 3539, 5330, 2217, 5237, 3948, 5652, 1802, 3741, 4440, 814, 2217, 4455, 3379, 1811, 3423, 2708, 3995, 5629, 597, 1313, 3423, 2708, 3995, 5629, 9], [5512, 4851, 1097, 0, 4395, 1534, 5383, 418, 1135, 1180, 5126, 5637, 4397, 952, 5303, 4441, 814, 3825, 2217, 4455, 3833, 814, 3825, 2217, 4455, 4087, 4485, 5303, 814, 2217, 4932, 3856, 5513, 1205], [5240, 4217, 3887, 5614, 4769, 4563, 5330, 2217, 2832, 3425, 2111, 3423, 2703, 3995, 5629, 963, 5632, 0], [3885, 2353, 5082, 5238, 814, 3825, 4455, 3370, 2856, 2217, 1039, 2832, 3673, 1791, 1135, 1180, 5126, 3955, 2759, 3069, 3054, 0, 5629, 733, 5240, 4455]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('22104618')]\n",
      "[[1031, 3644, 1337, 3729, 2711, 3888, 1810, 1017, 3822, 5240, 5042, 923, 828, 5637, 0], [2505, 2410, 711, 3933, 1180, 891, 3054, 3929, 842, 5240, 4252, 3054, 4103, 3321, 2631, 4523, 4360, 3803, 3217, 2711, 3955, 2821], [3379, 2866, 4745, 4542, 4974, 549, 963, 1018, 299, 553, 963, 2006, 5632, 196, 348, 5661, 4838, 4402, 5230, 976, 5303, 3513, 1770, 3803, 4048, 2461, 1890, 4745, 0, 5667, 4888, 2827, 2832, 3921, 842, 888, 1741, 4745]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('18803678')]\n",
      "[[5240, 755, 3803, 5240, 4193, 4287, 4395, 5042, 5637, 5303, 2174, 5240, 2802, 3803, 5394, 5012, 4236, 2741, 5601, 4655, 3822, 4360, 3803, 3217, 4353, 3803, 5281, 1180, 3966, 5447, 4682, 842, 5303, 2174, 1994, 3803, 1121, 4237], [4453, 2716, 5394, 4655, 2613, 1034, 5240, 3502, 3803, 1307, 2410, 4382, 4549, 616, 4682, 2832, 3274, 4662, 5281, 1180, 3966], [1017, 3822, 4795, 4655, 5010, 0, 783, 0, 3453, 0, 0, 0, 3691, 1811, 2832, 616, 5074, 5614, 3781, 1060, 4655, 842, 2741, 2573, 589, 842, 600, 4597], [1154, 1530, 4353, 5614, 4199, 2832, 5240, 4655, 2572], [963, 3536, 3966, 5456, 814, 4655, 5012, 5230, 1794, 146, 5653, 1111, 4725, 0, 842, 3635, 5428], [2446, 5303, 3536, 1277, 3803, 4353, 4724, 842, 5061, 5637, 3691, 3268, 4995, 1813, 2832, 1121, 2573, 3803, 3966]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('23185753')]\n",
      "[[5264, 2832, 5240, 5376, 2572, 680, 5322, 2365, 437, 101, 3490, 1960, 5287, 5412, 1656], [772, 2591, 1297], [5240, 5692, 4914, 2466, 842, 4322, 2466, 1688, 2832, 5240, 5376, 2572], [5240, 5622, 2618, 842, 4086, 5637, 1767, 2832, 772, 3966, 1036, 5376, 3536, 733, 5376, 842, 5692, 733, 5376]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('15920748')]\n",
      "[[963, 3536, 5240, 4480, 3803, 2838, 4810, 3803, 2559, 854, 3625, 5600, 5691, 842, 0, 5614, 1813, 586, 3803, 5240, 3966, 2825, 2832, 2572, 5562, 508, 2832, 2572, 5562, 442, 2832, 2572, 4], [3769, 2953, 1774, 1154, 1804, 2678, 842, 4998, 1774, 1154, 3884, 5052, 2541, 948, 842, 4353, 1774, 1154, 5240, 2167, 3863, 2410, 5240, 4577, 842, 5376, 3803, 1180, 4360, 3803, 3217, 4369, 5561, 2090, 4351, 1156, 5637, 2175, 963, 1018, 963, 5240, 2051, 3803, 4687, 842, 963, 3536], [2832, 5240, 0, 5222, 3833, 1574, 0, 4840, 2802, 3822, 3965, 3888], [733, 4687, 586, 3803, 3966, 2241, 4687, 5337]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21296449')]\n",
      "[[5251, 5614, 3691, 4995, 4840, 1811, 2832, 2559, 3667, 842, 5273, 1060, 5240, 2426, 924], [5240, 4669, 3803, 1297, 2410, 711, 3755, 3966, 842, 1973, 0, 4544, 1537], [5625, 2175, 1898, 3803, 5631, 1883, 5303, 2754, 3197, 5335, 1338, 1989, 1896], [4755, 2060, 3900, 5122, 3875], [5251, 5614, 4840, 1687, 2832, 3921, 4745, 2446, 1018, 5303, 4113, 1635, 842, 52, 842, 13, 4597], [4353, 3691, 1811, 2832, 5330, 4745, 1060, 1018, 842, 1635], [4805, 3966, 1802, 3741, 4441, 889, 5376, 1031, 3803, 1770, 2832, 4312, 3856, 1672], [2663, 1896, 3209, 5637, 3741, 957, 5667, 2857, 5336, 5508, 3803, 3077, 866, 3856, 2696, 4411]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23690410')]\n",
      "[[5240, 1182, 2998, 5614, 1989, 2410, 2829, 4048, 664, 1805, 0, 842, 1111, 3360, 2866, 2832, 1377, 1180, 5126], [2832, 5261, 5413, 2572, 4395, 1534, 5383, 0, 1377, 1180, 5126, 5637, 4397, 952, 5303, 5240, 2631, 1359, 2998, 119, 0, 1017, 5213, 1716, 2631, 1359, 4801, 1716, 3899, 3536, 2397, 3822, 4048, 664, 5634, 3334, 1805, 0, 765, 842, 4895, 3856, 5513, 1205], [963, 130, 3536, 4840, 2998, 1992, 5637, 3781, 2410, 3507, 4048, 664, 304, 3480, 16, 1111, 3360, 2866, 3109, 9, 2065, 2446, 5330, 2301, 27, 842, 2065, 2446, 0, 2301, 39], [5240, 2998, 3054, 5362, 5277, 2225, 5213, 1180, 5103, 842, 2906, 4798, 2832, 986, 842, 3881, 1578]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('19289630')]\n",
      "[[4217, 2051, 4095, 5614, 3900, 5122, 4755, 2051, 4096, 5637, 3773, 4605, 4409, 3873, 4701, 5150, 2826, 842, 4360, 3803, 3217, 4353], [2832, 3966, 5667, 4474, 3856, 3433, 4731, 5649, 4606, 5667, 2501, 5637, 4761, 3644, 2501, 283, 3729, 438, 3445, 1663, 2825, 3900, 5122, 1403, 5667, 3438], [2832, 5240, 2501, 283, 3445, 1663, 438, 3445, 1663, 842, 3438, 2573, 4597, 3391, 3900, 5122, 5614, 842, 3536, 0, 4605, 2177, 1598, 2832, 4909, 5404, 5637, 842, 5667, 3691, 4995, 4840, 1811, 1060, 2009, 2501, 923, 842, 3438], [5667, 5240, 2204, 3803, 5403, 2650, 5414, 2183, 5667, 2501, 5240, 715, 2182, 4249, 5637, 2511, 1502, 5667, 5264, 4214, 3781]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('16849753')]\n",
      "[[1248, 963, 5240, 3857, 1896, 698, 5614, 5635, 5316, 1154, 5254, 3966, 5667, 1162], [5303, 1402, 5240, 1992, 3803, 0, 0, 1248, 0, 0, 5239, 842, 4068, 4066, 3822, 897, 842, 4360, 3803, 3217, 4353, 2832, 3966, 5667, 1180, 4523, 854, 1161, 5155, 1162]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('17626056')]\n",
      "[[5218, 5693, 3803, 5376, 4071], [1406, 3803, 1380, 2692, 5250, 0, 5562, 3801, 5250, 0, 3888, 4641, 3691, 4840, 1812], [5240, 3379, 737, 3803, 4393, 5675, 5614, 490, 4749, 5693], [2508, 4149, 2832, 5424, 0, 986, 589, 842, 3672, 0, 272], [3881, 1181, 1672, 2446, 772, 1235, 5556, 0, 1264, 1850, 1720, 842, 4360, 3803, 3217], [2692, 4560, 5250, 2858, 1204, 842, 0, 4662, 5642, 4987, 3344, 5693, 733, 5240, 3416]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21741309')]\n",
      "[[177, 3803, 412, 3966, 5637, 952, 5303, 1825, 5250, 157, 4983, 3929, 1205, 842, 154, 1334, 1262, 1205], [1825, 5250, 3054, 0, 0, 4824, 5222, 4326, 5238, 5614, 1778, 2410, 3966, 842, 5241, 0, 3249, 5667, 3217, 0, 3856, 3217, 3231, 2779], [116, 121, 842, 114, 3966, 4597, 5637, 827], [4841, 3539, 3966, 5652, 2591, 4442, 1825, 5250, 4563, 5238, 5240, 5042, 2572, 2591, 1035, 0, 1403, 5667, 5264, 5652, 4442, 4983, 3929, 1205, 311, 475, 4], [5240, 4217, 3888, 4481, 2832, 5542, 1828, 3803, 1865, 1036, 842, 733, 1420, 3803, 5240, 5042, 5637, 3385, 5667, 5240, 2462, 948, 3803, 1312, 2779, 5250, 4948, 5635, 1045, 4723, 3965, 1825, 3023, 2695, 888, 842, 1741, 4723, 3072, 2446, 5240, 5037, 3001, 2410, 5153, 842, 1449, 4360, 3803, 3217, 4723, 842, 3515, 1979, 5150, 948, 4723], [778, 0, 5614, 1154, 5508, 3803, 0, 0, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('8628043')]\n",
      "[[5303, 1467, 3074, 1457, 5532, 842, 4608, 5303, 5376, 842, 4472, 2832, 3966, 5667, 1135, 1180, 2045, 5635, 1045, 5614, 946, 2186, 3536, 2410, 5693, 842, 963, 842, 3536, 733, 4472, 2832, 182, 3966, 5652, 5637, 4179, 842, 0, 3966, 5652, 5637, 4121], [1121, 3388, 5637, 4607]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('14990635')]\n",
      "[[2426, 2717, 4805, 3966, 5637, 2028, 2410, 5240, 5042, 0, 3966, 600, 3950, 2832, 5240, 2631, 4523, 4360, 3803, 3217, 2711, 5042], [5330, 3803, 398, 3966, 5667, 3258, 711, 4976, 2771, 3856, 4976, 3077, 3755, 5404, 5637, 2842], [5240, 2167, 3863, 2410, 4577, 842, 5376, 3803, 1180, 2090, 4360, 3803, 3217, 4369, 4351, 1156, 842, 2090, 4351, 3286, 1180, 4936, 3520, 3165, 5637, 5509, 5303, 3025, 0, 5150, 4536, 842, 1277, 2832, 2711], [2711, 842, 5150, 4536, 5637, 2111, 2832, 5240, 5376, 924], [5240, 3966, 5637, 3781, 2410, 3474, 3803, 5693], [4623, 1018, 4253, 1659, 5637, 2106, 1868, 2832, 5240, 5376, 2573], [2845, 1598, 5637, 2930, 1850, 5321, 711, 2410, 1623, 4386, 842, 1298, 5153, 3856, 1260, 5403, 0, 5240, 0], [3965, 1422, 5667, 4596, 5303, 5240, 2711, 3028, 5614, 3474, 539]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21538678')]\n",
      "[[4154, 842, 4113, 948, 1659, 5637, 1370, 5566, 3832, 4370, 842, 1656, 4873, 1797]]\n",
      "[np.int64(0)]\n",
      "[np.str_('20855825')]\n",
      "[[2460, 438, 3445, 5614, 957, 5667, 4995, 4840, 2856, 2832, 4031, 842, 3741, 957, 5667, 2857, 5337], [5625, 1403, 2460, 438, 3445, 4495, 5667, 5240, 0, 1896, 3803, 2460, 283, 3445, 3995, 3534, 2410, 5376, 3803, 4121, 5675, 5667, 2154, 4446, 4108, 711, 1135, 1180, 5652, 2241, 4261, 733, 4224, 2054, 5250], [2460, 438, 3445, 5614, 5635, 5316, 5667, 3691, 1896, 1737, 715, 2183], [4217, 2051, 4095, 5614, 4261, 2439, 5122, 4031]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('14722035')]\n",
      "[[2051, 4096, 2842, 4261, 2439, 5122, 4031, 4605, 4409, 4681, 842, 4605, 1939, 4426, 4360, 3803, 3217, 4353, 5337, 842, 3900, 5122, 3875], [4039, 2768, 5383, 1382, 1911, 842, 1908, 1906, 2832, 3433, 1135, 1198, 4831, 2857, 4605, 3899, 5238, 2237, 5667, 1906], [5251, 5614, 3691, 4840, 1811, 2832, 4681, 4426, 3856, 4031, 1060, 924]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('21561791')]\n",
      "[[5673, 708, 5233, 4542, 963, 1221, 3208, 2410, 3536], [3966, 5667, 4313, 3616, 3675, 3490, 5265, 2591, 4841, 2663, 5692, 4662, 3803, 4261, 1403, 5303, 1163, 450, 5601, 327, 58, 4597]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('21366974')]\n",
      "[[1135, 4687, 3054, 5316, 5635, 1154, 3544, 3816, 1135, 1180, 3966, 5673, 2806, 5241, 3900, 2631, 4523, 4360, 3803, 3217, 2711], [5303, 944, 5646, 3818, 3803, 4113, 3843, 4386, 4687, 2832, 5675, 5667, 3274, 4662, 1004, 3695, 3641, 1135, 1180, 5405, 4866, 3803, 3197, 5237, 1352, 5164, 797, 5240, 0, 1598, 2471, 4476, 5240, 2028, 4866, 5303, 3369, 3803, 1352, 5374, 1154, 1135, 0, 5115, 842, 2054, 5250, 2828, 4360, 3803, 3217, 842, 3054, 3539, 1569, 1989], [5240, 4983, 5376, 3803, 4113, 3843, 5653, 1135, 3050, 3856, 5240, 3818, 3803, 4687], [797, 2711, 4828, 0, 1028, 5179, 3005, 636, 5642, 0, 5376]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('9807987')]\n",
      "[[2832, 4039, 2768, 5041, 3047, 3054, 662, 2832, 3433, 1377, 1180, 1152, 5240, 3900, 1051, 2613, 3741, 1035, 946, 2832, 4393, 1337, 5383], [5667, 3391, 2403, 5489, 3803, 144, 3536, 5240, 3900, 5122, 5614, 4841, 1059, 2832, 5240, 3047, 2572, 4, 5667, 363, 5692, 5122, 2832, 5240, 3047, 2572, 5562, 144, 2832, 5240, 5105, 1205, 2572], [3966, 5667, 4300, 3433, 1377, 1180, 5648, 2591, 4259, 5672, 3536, 3803, 5376, 5667, 2387, 5637, 4397, 952, 2009, 323, 356, 3445, 3303, 3047, 2186, 5632, 5667, 5105, 1205, 3856, 5105, 1205, 783, 2832, 4415], [5122, 5673, 4006, 4998, 1770, 4, 5673, 5634, 3272, 3803, 3539, 5237, 43, 842, 3921, 2439, 5122, 16, 5637, 4841, 1059, 2832, 5240, 3966, 2533, 3047], [5652, 4006, 4998, 5614, 2832, 553, 396, 3966, 2832, 547, 391, 3966, 842, 2832, 333, 183, 3966]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('12953086')]\n",
      "[[5240, 4217, 2060, 5614, 5240, 4280, 3803, 3966, 5673, 4261, 963, 5693], [3391, 4261, 2439, 5122, 5287, 2832, 5240, 5198, 923, 183, 3536, 597, 1313, 164, 5303, 199, 3536, 842, 5240, 4327, 923, 199, 3536, 597, 1313, 173, 5303, 246, 3536, 5637, 789, 3741, 4995, 4841, 1813], [5240, 5198, 4495, 5614, 957, 5667, 2663, 2442, 3803, 2645, 5337], [1152, 3275, 2442, 3803, 2492, 842, 0, 5337, 5237, 5240, 4327, 4495]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('17047644')]\n",
      "[[5625, 1403, 4098, 3803, 5270, 5601, 4862, 1582, 3803, 3672, 2515, 4079, 1017, 1378, 1297, 5667, 4491, 5303, 1992, 3822, 4360, 3803, 3217, 4353, 842, 5122], [5261, 4393, 3576, 5383, 5614, 1463, 5303, 2142, 5240, 3849, 1939, 3803, 3929, 1297, 2832, 711, 3698, 4888, 1253, 3286, 1180, 3755], [3391, 5122, 2832, 5240, 1155, 2572, 5614, 304, 5601, 333, 5632, 2832, 5240, 1158, 2572, 543, 2708, 67, 597, 1313, 563, 327]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('12195763')]\n",
      "[[4863, 711, 1180, 3966, 5667, 3660, 3921, 3822, 5163, 3541, 5250, 3691, 3268, 4443, 3824, 5376, 4196, 3507, 3921, 618, 3856, 3539, 1152, 3197, 5237, 3822, 0, 4723, 3803, 100, 2832, 5240, 3153, 5629, 842, 2533, 4974, 3541, 1896, 2832, 5240, 3153, 1664, 5637, 703, 5303, 5240, 5042], [2832, 0, 3803, 5240, 4623, 3786, 2832, 5240, 5042], [5240, 2262, 5508, 3803, 5240, 1927, 2410, 1180, 3921, 4828, 1028, 4368]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15839957')]\n",
      "[[3344, 3966, 5667, 711, 3897, 1180, 5663, 1777, 4474, 1850], [2505, 1195, 5376, 4841, 2828, 5240, 4031, 3803, 3966, 5667, 4079, 4779, 4474, 3897, 1180], [2410, 5264, 3966, 5652, 2614, 4472, 3803, 1850, 963, 3180, 3536, 733, 2921, 5250, 5240, 3917, 4079, 1378, 2613, 1035, 4833, 5303, 1028, 5096, 5376, 5303, 4079, 3532], [3966, 5374, 5667, 2505, 1195, 4563, 4841, 2300, 3928, 3803, 610, 5153, 842, 4841, 2825, 2541, 4360, 3803, 3217]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('12431965')]\n",
      "[[3391, 0, 2410, 3349, 842, 4068, 3966, 5637, 3536, 842, 3536, 4597, 586], [3966, 3822, 3349, 2591, 4841, 4104, 4360, 3803, 3217, 963, 842, 3536], [5255, 5637, 4395, 5303, 4441, 3349, 100, 3445, 3856, 4068, 3858, 1068, 2410, 5489, 5303, 5693], [5337, 5614, 2511, 3230, 5303, 3593, 5153, 191, 2559, 2410, 3349], [5251, 5637, 0, 2028, 3966, 298, 3349, 842, 298, 4068]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('21839414')]\n",
      "[[797, 5240, 4496, 2832, 5261, 5042, 5637, 4843, 2832, 5238, 5255, 2842, 5196, 5240, 3379, 5517, 4745, 842, 3270, 0, 3803, 5517, 4745, 5637, 1813, 808, 4496], [3833, 5240, 2279, 4902, 2293, 5635, 1045, 5060, 2591, 3691, 4525, 5667, 2104, 480, 4606, 842, 5517, 4745]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('19714296')]\n",
      "[[1659, 5082, 5238, 5240, 2500, 1378, 3370, 2824, 1337, 3888], [3966, 5637, 4397, 952, 5303, 2500, 3856, 5376, 2500, 923, 438, 3445], [2500, 4606, 1415, 4605, 1590, 3946, 4605, 4146, 842, 341, 4974, 1850, 4749, 385, 1850, 1533, 4409, 1669, 1590, 4146, 4749, 434], [1082, 2257, 5614, 946, 1154, 2798], [1664, 842, 164, 2832, 304, 1663, 1635, 923, 932, 2832, 2500], [4606, 4146, 842, 173, 4749, 396, 1669, 422]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('20530648')]\n",
      "[[5260, 5413, 5675, 5667, 1135, 1180, 5637, 4397, 952, 5303, 4729, 3867, 2217, 2572, 173, 842, 2508, 2217, 2572, 173], [4287, 4395, 1534, 4059, 5383, 5667, 2676, 1533], [5303, 2197, 5240, 1992, 3803, 4729, 3867, 2217, 3822, 5492, 3226, 1948, 2832, 1135, 1180, 5126], [4729, 3867, 2217, 2591, 1049, 1992, 3822, 3921, 4360, 3803, 3217, 842, 942, 3803, 5028], [5240, 1275, 2832, 2541, 4360, 3803, 3217, 0, 1988, 4866, 341, 842, 5028, 3803, 2265, 0, 9, 1988, 4866, 460, 5637, 4841, 2567, 2832, 5240, 4729, 3867, 2217, 2572, 5237, 2832, 5240, 2508, 2217, 842, 1533, 2572]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('16670385')]\n",
      "[[4783, 3291, 3695, 1085, 2832, 5675, 5667, 3839, 1135, 1180, 3054, 4678, 5509, 2832, 4916, 1578, 2410, 4979, 5240, 1003, 1760, 3230, 1659, 2446, 4395, 5384, 3822, 3538, 842, 3542, 3888], [5240, 4527, 4663, 3803, 889, 3293, 842, 4781, 3272, 2410, 5240, 4783, 3291, 3695, 1085, 2572, 1403, 5667, 5240, 4983, 1004, 5376, 2572, 963, 130, 3536, 5637, 369, 597, 1466, 2996, 1313, 265, 5303, 483, 622, 4411, 5562, 144, 842, 369, 597, 1313, 300, 5303, 437, 622, 4411, 119, 5562, 327, 4597], [5240, 4217, 3887, 3388, 5637, 923, 842, 4829, 3538, 842, 4360, 3803, 3217], [5625, 1463, 3575, 4395, 5383, 5303, 1402, 4360, 3803, 3217, 3888, 1060, 3966, 5667, 1338, 3695, 3641, 3022, 1135, 1180, 5652, 4442, 4783, 3291, 3695, 1085, 842, 3966, 5652, 4442, 4983, 1004, 5376], [0, 5507, 3189, 3803, 2695, 5000, 842, 5287, 5303, 0, 3803, 3731, 1663, 5303, 1663, 663, 733, 5115, 5637, 4995, 4841, 3275, 2832, 5240, 4783, 3291, 3695, 1085, 2572, 772, 9, 842, 1004, 3843, 5287, 5614, 4477, 81], [3900, 3965, 4461, 4360, 3803, 3217, 842, 923, 2465, 4745, 5637, 4995, 4841, 1059, 2832, 5240, 4783, 3291, 3695, 1085, 2572, 5278, 772, 3856, 16], [772, 4994, 5234, 5637, 5413, 4836]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('22494567')]\n",
      "[[5240, 2541, 4039, 2771, 4788, 5188, 2832, 5480, 3755, 4718, 5042, 2175, 5240, 1994, 842, 4701, 3803, 5240, 2095, 2574, 2280, 4446, 2004, 0, 0, 2918, 2117, 932, 3316, 5376, 2832, 3755, 3966, 5673, 4261, 733, 2358, 3233, 1297], [4031, 5614, 4841, 4269, 2832, 5240, 2117, 5376, 923, 1121, 3900, 2616, 4415, 2708, 471, 0, 842, 2832, 3966, 5667, 2004, 2767, 4108, 1850, 2708, 437, 0]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('16921034')]\n",
      "[[3966, 5667, 1415, 3856, 3946, 4605, 5637, 3539, 3223, 5303, 2614, 2826, 2832, 5240, 4353, 4605, 5237, 3966, 5667, 4974, 3856, 4262, 1850, 32], [2117, 3741, 3833, 2828, 5122, 2832, 4214, 5374, 3966, 5667, 3755, 1152, 789, 2828, 5403, 4523, 5153, 842, 2821, 942, 3803, 4353], [1422, 5667, 4353, 5614, 577, 963, 1018, 842, 3539, 5237, 526, 1940, 5376]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('21709202')]\n",
      "[[5261, 4039, 2771, 5383, 5614, 1463, 5303, 5230, 5646, 5240, 3746, 5546, 0, 740, 933, 0, 5642, 1380, 5667, 2358, 3233, 4079, 1017, 1297, 2828, 5122, 2832, 3966, 5667, 711, 3698, 4888, 1253, 3286, 1180, 3755, 5562, 1297, 783], [5240, 678, 3803, 933, 5303, 1195, 842, 3917, 797, 2511, 5635, 5316, 2284, 5303, 2824, 2448, 1994, 2832, 711, 3755], [3391, 4031, 5614, 3536, 2832, 1121, 924, 2708, 67, 0, 5649, 3873, 5614, 282, 2832, 1121, 924]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('20351334')]\n",
      "[[5240, 3900, 5337, 4248, 3803, 5580, 5614, 3332], [3391, 4031, 5614, 3536, 2410, 1960, 923, 2708, 23, 597, 1313, 0, 5303, 210], [2559, 2663, 5237, 5580, 1883, 845, 563, 553, 3667, 434, 376, 45, 5273, 322, 156, 2320, 3667, 1508, 376, 119, 2304, 363, 341, 2927, 4858, 4431, 327, 3625, 292, 265, 5600, 265, 156, 785, 199, 355, 0, 199, 130, 610, 3921, 224, 0, 156, 4015, 3661, 100, 164, 928, 1798, 130, 0, 842, 0, 1854, 5637, 3781], [3873, 4974, 1850, 3391, 3875, 5637, 5562, 363, 5562, 376, 5562, 3536, 2708, 601, 597, 1313, 0, 5303, 187, 4597]]\n",
      "[np.int64(1), np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('17251532')]\n",
      "[[3966, 5637, 4397, 952, 2832, 4415, 5303, 5446, 2009, 5515, 927, 2035, 3856, 5115, 5667, 112, 3966, 5447, 2035, 842, 442, 5447, 5115, 404, 0, 842, 0], [963, 5692, 5150, 4745, 5637, 1059, 2832, 5240, 5116, 2572, 58], [1940, 5240, 2358, 5692, 3803, 2403, 5489, 5251, 5637, 144, 3317, 715, 2183, 2832, 5240, 2035, 2572, 130, 842, 100, 2832, 5240, 5116, 2572, 224, 257, 3545, 4523, 5303, 5240, 2998], [5240, 1994, 842, 4701, 3803, 5515, 927, 2035, 932, 1403, 5667, 4983, 5116, 3437, 2410, 5240, 5376, 3803, 5151, 5515, 2341, 4541, 5440], [5625, 1463, 4395, 5383, 1405, 5515, 927, 2035, 842, 5115, 2832, 5675, 5667, 5151, 5515, 2341]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('17467230')]\n",
      "[[0, 3803, 0, 1992, 842, 2631, 2827, 5637, 3781], [5254, 1992, 4542, 5642, 4995, 1535, 2410, 1018, 3209, 3803, 1865, 2793, 842, 2631, 932, 5635, 932, 0, 1850, 842, 1180, 5376, 5539], [5240, 3159, 2858, 5637, 5479, 5303, 2825, 2631], [4113, 5115, 263, 1135, 1180, 3966, 5637, 4395, 5303, 2998, 3856, 948, 3833, 5042, 924], [1865, 4480, 3054, 0, 932, 814, 2821, 0, 1154, 5648, 2631, 1176, 1028, 2825]]\n",
      "[np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('9531327')]\n",
      "[[1590, 733, 4547, 2887, 5376, 5614, 646, 2832, 460, 3803, 5240, 3966, 2832, 5240, 1533, 2572, 842, 2832, 467, 3803, 5240, 3966, 2832, 5240, 2545, 1612, 2572, 3753], [5240, 1939, 3803, 5122, 842, 1850, 2439, 5122, 963, 5693, 733, 4394, 5637, 2148, 963, 257, 842, 199, 2410, 5240, 1533, 2572, 842, 257, 842, 156, 2410, 5240, 2545, 1612, 2572, 3753]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('18959455')]\n",
      "[[5054, 5637, 4397, 952, 5303, 2009, 5240, 1533, 2572, 4443, 3691, 3594, 2998, 144, 3856, 5240, 2244, 2572, 4443, 4154, 842, 4122, 3246, 3594, 5250, 4801, 156], [3691, 4995, 4840, 1812, 5637, 2424, 2410, 3537, 77, 3856, 3921, 77, 3209], [1533, 5054, 4442, 4677, 2695, 1205, 5673, 889, 3594, 5250, 2998]]\n",
      "[np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('16476840')]\n",
      "[[3966, 2832, 5261, 5042, 2591, 2805, 4353, 1403, 5667, 4105, 3736], [0, 2567, 4353, 1051, 5614, 3781, 2410, 5240, 2279, 814, 2304, 842, 2279, 814, 845, 5061, 2832, 5240, 5064, 3803, 3966, 5667, 1018, 2618, 3209, 100, 1877], [5261, 5614, 2111, 5303, 0, 2832, 4353, 1701, 976, 5303, 2103, 766, 3803, 600, 385, 842, 374, 2410, 5240, 2279, 2508, 5330, 2279, 814, 2304, 842, 2279, 814, 845, 5060, 4745, 4597, 5562, 1147]]\n",
      "[np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('9093725')]\n",
      "[[733, 0, 2410, 0, 2832, 4206, 1279, 1297, 5376, 5614, 2705, 957, 5667, 5122, 1051, 16], [5240, 4623, 4830, 5238, 1297, 1176, 0, 5303, 1121, 4364, 842, 4360, 3803, 3217, 2832, 711, 2486, 1180], [4843, 1811, 5614, 4761, 2832, 5240, 5375, 4049, 2177, 3803, 5646, 5240, 3965, 5614, 5053, 2825, 3856, 1523, 5303, 1881, 5635, 2410, 963, 3180, 3536, 183, 327, 460, 5562, 322, 224, 32], [1060, 3081, 212, 842, 2321, 216, 486, 3966, 5667, 2486, 1180, 5637, 4395, 5303, 2009, 1297, 2832, 678, 5303, 1057, 5105, 1205, 3856, 5303, 1057, 5105, 1205], [4360, 3803, 3217, 5614, 2175, 5667, 5240, 2090, 4351, 1156, 2948]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('7680374')]\n",
      "[[2008, 3828, 3931, 3966, 842, 556, 1533, 3966, 5637, 3527, 2410, 3391, 3803, 191, 842, 246, 3536, 4597, 2410, 2183, 3803, 4868, 3538, 842, 5240, 4383, 1581, 3803, 3433, 1115, 1850], [3931, 5376, 3803, 1135, 1180, 3966, 0, 4477, 4868, 3538], [2471, 4577, 3822, 1896, 842, 0, 3803, 5376, 3054, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('16966687')]\n",
      "[[2410, 5264, 3966, 5652, 0, 963, 3180, 3536, 733, 2921, 5250, 3917, 4079, 2613, 4833, 3513, 5122, 712, 3899, 4079, 5673, 3917], [5270, 2717, 2344, 4862, 3966, 189, 2505, 4087, 1195, 189, 1195, 5637, 4397, 952], [5649, 3606, 5614, 4841, 3539, 1389, 2832, 5240, 1378, 0, 5077, 932, 2320, 3667, 3856, 2894, 5637, 0], [3691, 4995, 4840, 1812, 2832, 4360, 3803, 3217, 4745, 1060, 924, 5637, 3744]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('22228392')]\n",
      "[[3921, 3054, 3803, 5240, 3544, 1389, 5153, 5238, 1180, 3965, 5684, 2240], [3069, 3054, 4457, 5303, 4301, 3921, 3334, 1984, 5303, 772, 1180, 3966]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('15750359')]\n",
      "[[5303, 4059, 814, 3028, 3803, 2880, 2687, 2410, 5153, 3803, 2154, 5668, 2832, 1135, 1180, 5126], [2344, 4805, 471, 5675, 3423, 2845, 1598, 842, 450, 5637, 4395, 5303, 5240, 5042], [2008, 2364, 3997, 570, 417, 450, 3803, 5675, 1416, 5240, 5042], [5251, 5614, 3691, 2188, 3803, 1811, 4761, 1060, 2573, 2410, 2009, 664, 693, 1811, 597, 1466, 2996, 1313, 5303, 183, 3856, 4248, 4745, 693, 1811, 597, 1313, 5303, 144, 5512, 5261, 5383, 1754], [797, 4113, 2683, 4140, 1167, 5086, 5238, 502, 186, 5684, 1028, 3638, 3995, 2572, 5303, 1765, 1812, 3803, 5261, 3308, 5667, 5081, 0], [5240, 4217, 3887, 3388, 5637, 5240, 664, 4743, 842, 4248, 4743, 3803, 5240, 3384, 0, 3397, 3887, 4248, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('24075621')]\n",
      "[[5254, 4623, 4301, 2471, 5103, 2410, 5240, 1994, 3803, 614, 2832, 5261, 4105], [3391, 5287, 5303, 4261, 3803, 3379, 3921, 2962, 5614, 3268, 2832, 3966, 952, 5303, 614, 4087, 4166, 292, 3536, 597, 1313, 199, 3741, 2146, 5237, 2832, 5264, 952, 5303, 4068, 4087, 4166, 191, 3536, 156, 3741, 2146, 2616, 4415, 2708, 563, 597, 1313, 508, 5705, 0, 932, 5614, 3391, 5287, 5303, 4261, 3803, 3921, 2977, 5667, 1656, 663, 100, 3536, 597, 1313, 144, 5601, 3536, 2708, 553, 597, 1313, 508, 593, 24], [614, 643, 4087, 4166, 4841, 2828, 0, 4261, 2439, 5122, 2832, 962, 3856, 3462, 5151, 1297, 3617, 3966, 5667, 3433, 1223, 4589, 4290, 1180, 1403, 5667, 4166, 783], [3921, 5614, 946, 5667, 5240, 1127, 4815, 4369, 842, 2631, 4523, 4360, 3803, 3217, 2711, 5667, 5240, 2462, 948, 3803, 1180, 5250, 4290, 2279, 4369], [1060, 914, 304, 236, 842, 3095, 265, 238, 3966, 5667, 4262, 3433, 1223, 4589, 4290, 1180, 5637, 2074, 3005, 3584, 1901, 1096, 4068, 1534, 5383], [3391, 5287, 5303, 2711, 1770, 5614, 3268, 2832, 3966, 952, 5303, 614, 4087, 4166, 5237, 2832, 5264, 952, 5303, 4068, 4087, 4166, 932, 946, 1154, 5240, 2279, 5330, 4743, 130, 3536, 597, 1313, 119, 156, 5601, 3536, 100, 2708, 549, 597, 1313, 505, 590, 16, 842, 1154, 5240, 4743, 3822, 3074, 4290, 1180, 4936, 5060, 119, 3536, 144, 5601, 3536, 2708, 526, 597, 1313, 483, 566, 4]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('20973982')]\n",
      "[[5261, 4039, 282, 5629, 3837, 3129, 5042, 2175, 5240, 3706, 3803, 1657, 766, 698, 5631, 5601, 932, 814, 2259, 1900, 4732, 2186, 3856, 5632, 2832, 3966, 5667, 1314], [4807, 3828, 3997, 3803, 3966, 2832, 5240, 1980, 2572, 842, 546, 2832, 5240, 4373, 2572, 646, 5240, 5189, 2649, 3803, 119, 1877], [1723, 842, 1850, 4989, 5637, 4843, 1060, 2573], [1657, 766, 5642, 698, 0, 5667, 1297, 3822, 814, 1980, 896, 5303, 1028, 4844, 1993, 5303, 1657, 766, 5631, 1900, 5667, 3691, 5461, 715, 2183], [5251, 5614, 3470, 1811, 2832, 5240, 4217, 2060, 3803, 3379, 1275, 2832, 2649, 1018, 5303, 5629, 144, 1060, 5240, 4373, 842, 5240, 1980, 2573, 67, 1877, 597, 1466, 2996, 292, 183, 1877], [4805, 2717, 2344, 5413, 3966, 372, 4373, 3966, 0, 1980, 3966, 4442, 1896, 3803, 1657, 766, 842, 5637, 2842, 2832, 5240, 830], [700, 3803, 2128, 3822, 0, 4732, 5667, 1297, 700, 1573, 1051, 3966, 1154, 4479, 1336, 5585, 842, 4135, 2070, 3822, 5287, 1297, 1719]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15892043')]\n",
      "[[2410, 3966, 5667, 861, 4205, 3433, 1135, 1198, 1190, 4087, 1883, 4841, 2857, 3900, 5122, 1403, 5667, 1883, 783], [5240, 3379, 4360, 693, 5122, 5614, 2857, 1154, 3536, 2832, 5240, 1190, 1883, 2572], [2631, 842, 1569, 3888, 2832, 5240, 5413, 924, 5637, 1403, 842, 1569, 1991, 5614, 2148], [5240, 3379, 1569, 3995, 4347, 2481, 5667, 1378, 5250, 5614, 144, 464, 1889, 4983, 1780, 0, 1889], [1659, 3822, 5122, 5287, 842, 3397, 1205, 4594, 5508, 5637, 4288, 1370, 2832, 5240, 5383]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('24084921')]\n",
      "[[5675, 5667, 814, 0, 0, 4442, 1480, 2109, 0, 1250, 492, 3445, 4087, 3403, 643, 3554, 3445, 0, 3856, 4068, 0]]\n",
      "[np.int64(0)]\n",
      "[np.str_('14550448')]\n",
      "[[733, 842, 5693, 5261, 2591, 1688, 5303, 434, 842, 391, 4597, 45], [1036, 4386, 567, 3803, 3966, 4563, 0, 687, 2410, 0, 963, 3180, 2335, 5289, 1940, 5240, 4213, 5692], [3966, 4563, 2857, 3176, 5667, 2457, 1091, 5498, 2851, 1060, 5240, 842, 5692, 4370, 396, 5562, 437, 58]]\n",
      "[np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('16280763')]\n",
      "[[3691, 1812, 2832, 4353, 1060, 2573, 842, 5637, 2424], [5377, 5637, 5635, 5316, 2832, 5240, 2573], [5184, 2613, 1035, 4833, 5303, 1028, 3539, 1989, 842, 4698, 5237, 4687, 2832, 5261, 4803], [3803, 2572, 3966, 508, 2591, 2587, 1403, 5667, 2832, 2572, 842, 348, 2832, 2572], [789, 5625, 2175, 5241, 1992, 3822, 3965, 2691, 4998, 4360, 3803, 3217, 4353, 4812, 2461, 842, 4290, 4936, 877, 4520, 2439, 5122], [5625, 3026, 5240, 4669, 3803, 5184, 842, 4386, 4687, 2410, 5240, 4211, 842, 5376, 3803, 2587, 842, 1135, 3921, 1940, 697, 1066, 3532, 733, 4378, 4291, 4680, 2832, 3966, 5667, 4290, 1180]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('9414057')]\n",
      "[[543, 3803, 5240, 3966, 2591, 0, 5240, 2083, 3921, 0, 460, 2591, 0, 5303, 5240, 981, 1220, 842, 570, 3803, 3921, 4745, 5637, 1416, 2832, 5240, 3921, 1800], [5240, 5508, 3803, 5240, 3921, 1984, 4255, 1154, 3765, 4828, 1028, 0, 1499, 3822, 3827, 5468], [2405, 0, 2832, 329, 3966, 3966, 5652, 3638, 1871, 3766, 842, 5652, 1802, 3741, 3637, 1871, 3766, 963, 2685, 5637, 4397, 952, 5303, 1533, 3856, 2998, 2572]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('19194996')]\n",
      "[[3003, 5637, 1463, 5303, 2252, 5675, 2240, 3803, 5687], [3069, 4760, 2318, 5303, 5082, 5238, 2192, 3370, 1028, 1569, 1989, 633, 5376, 5238, 1573, 1028, 2853, 3005, 5240, 3829, 1205, 3803, 5675], [2533, 5240, 2225, 2188, 5104, 5240, 2820, 3803, 4902, 5103, 2832, 695, 5303, 1135, 1180], [5251, 5637, 3691, 4840, 1992, 3803, 5240, 2998, 3822, 3537, 4353, 3856, 0, 0], [2008, 3948, 1416, 772, 2403, 0], [5675, 5637, 4467, 5303, 5240, 5042, 1940, 5241, 2348, 5629, 3803, 5376], [5675, 4002, 3803, 2045, 5103, 4360, 3803, 3217, 4353, 3537, 842, 0, 0, 5637, 946, 963, 1018, 3534, 3536, 842, 3536]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('11181777')]\n",
      "[[5240, 1378, 3803, 2160, 4087, 1321, 2091, 3054, 1499, 5303, 1028, 4983, 5250, 2410, 4888, 1253, 3286, 1180, 4741], [2832, 5240, 1587, 3503, 5240, 4527, 4662, 3803, 1672, 2410, 3966, 2832, 5240, 3975, 923, 1403, 5667, 5264, 2832, 5240, 2091, 923, 5614, 526, 597, 1466, 2996, 442, 5303, 597], [0, 3803, 5376, 2572, 842, 0, 4253, 5539, 5667, 5122, 5637, 5231, 2832, 5240, 1587, 4281, 2617, 3503], [3966, 2832, 5240, 3975, 923, 5124, 3268, 5237, 5264, 2832, 5240, 2091, 923, 5692, 5122, 4409, 385, 842, 311, 4597, 3391, 5122, 100, 842, 3536, 4597, 3263, 4402, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('22157367')]\n",
      "[[1291, 5177, 3966, 738, 526, 5693, 3856, 3816, 5652, 2591, 711, 3755, 5637, 4395, 5303, 4441, 2009, 3857, 2117, 165, 3445, 1656, 3856, 3857, 5581, 483, 3445, 3822, 1664, 842, 2186, 5632], [3900, 5122, 5614, 3268, 2410, 2004, 3597, 3966, 5237, 2410, 2004, 5662, 5414, 3966, 4]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('23455717')]\n",
      "[[5279, 2012, 3966, 2832, 3637, 3803, 2960, 1297, 3370, 4441, 2381, 5673, 1439, 3965, 4563, 3887, 3942], [5240, 4283, 3803, 3966, 5667, 2173, 1018, 2090, 4351, 1156, 842, 5014, 4370, 5637, 1011, 566, 2832, 2381, 842, 584, 2832, 2379], [3966, 4443, 2381, 2222, 2663, 4605, 4411, 842, 2591, 2825, 1850, 2439, 842, 4261, 2439, 5122, 4031], [5240, 3391, 737, 3803, 3966, 5614, 526, 5693], [1500, 2173, 3966, 5667, 945, 4370, 136, 3644, 2465, 3729, 5150, 3942, 1810, 4841, 2832, 2306, 3803, 3828, 3803, 5240, 5413, 5376, 2573], [2136, 684, 3311, 3793, 2832, 2012, 3966, 1152, 5255, 917, 0, 2832, 3544, 1337, 5384, 842, 3813, 1881, 3741, 4441, 1989, 1378, 1297, 3544, 4232, 2410, 2316, 3803, 0], [797, 5337, 5614, 2663, 2832, 3966, 4443, 2381]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('22544562')]\n",
      "[[4623, 5082, 5238, 1275, 2832, 4769, 1994, 2410, 2301, 4618, 0, 2249, 5240, 2998, 1988, 3822, 2301, 2953, 3379, 2878, 1988, 304, 842, 1275, 2832, 4769, 1994, 2410, 1512, 0, 2249, 5240, 2998, 1988, 3822, 1656, 2953, 3379, 2878, 1988, 119], [1275, 2832, 4769, 1994, 2410, 2301, 4618, 0, 637, 2410, 5240, 2998, 2802, 3822, 3900, 1804, 4360, 808, 3414, 3833, 3379, 2878, 1988, 483]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('21690465')]\n",
      "[[1152, 3741, 2410, 2461, 1848, 5084, 5238, 4135, 4792, 4235, 3370, 4541, 0], [5240, 1517, 3803, 1511, 981, 0, 1060, 304, 3826, 842, 0, 3966, 3899, 2426, 1485, 5585, 0, 0, 5614, 834], [3966, 2832, 5240, 2998, 923, 1847, 3539, 5153, 3899, 5287, 1403, 5667, 3966, 2832, 5240, 972, 1533, 29, 842, 1533, 67, 924], [1848, 5327, 5637, 4168, 4388, 1154, 3966, 0, 4494, 3803, 923, 778]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('10944130')]\n",
      "[[4360, 3803, 3217, 4353, 5614, 946], [5240, 1896, 5614, 2129, 2186, 1664, 2446, 5303, 3568, 1154, 0, 3803, 3568, 5473, 2559, 5337, 3791, 2832, 5648, 1218, 1896, 2131, 5614, 5019], [4605, 4280, 842, 5122, 1802, 3741, 2824, 4841, 5667, 5240, 678, 3803, 144, 1591, 5303, 2764, 5250, 2832, 3966, 5667, 711, 4424], [1415, 3856, 3946, 4606, 5637, 646, 1154, 130, 3803, 3966, 5374, 5667, 2764, 4087, 144, 1591, 842, 3803, 3966, 5374, 5667, 2764, 156], [2764, 5614, 2533, 1656, 5047, 4988, 963, 1896, 3803, 3463, 5468, 3568]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('22336221')]\n",
      "[[4411, 3803, 4594, 5508, 5637, 4995, 2663, 2410, 3992, 5237, 2410, 4068, 702, 5303, 2695, 2410, 1927, 4523, 715, 2183, 199, 5601, 3700, 9, 5357, 396, 100, 5601, 4805, 16, 842, 2125, 5011, 741, 292, 5601, 2426, 42], [2203, 2410, 4888, 2856, 2832, 3272, 3803, 897, 842, 4841, 1713, 5682, 3803, 3921, 842, 2596], [4623, 2410, 4360, 3803, 3217, 5150, 3928, 842, 5313, 917, 4195, 2656], [5240, 4217, 1994, 1659, 2614, 1035, 4563, 4214], [3268, 5287, 5303, 5682, 5614, 4461, 2410, 3921, 2616, 4415, 2708, 546, 597, 1313, 477, 604, 68, 842, 2596, 2708, 475, 597, 1313, 348, 600, 0, 5667, 3992, 5237, 5667, 4068], [3966, 1416, 5240, 3286, 1180, 5150, 4723, 3170, 963, 1018, 733, 1960, 1634, 842, 4113, 1843]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('22011650')]\n",
      "[[3900, 0, 134, 2173, 2711, 2826, 4411, 5637, 4841, 2567, 5667, 2501, 5562, 1195, 3917, 5150, 2826, 4411, 5637, 4843, 2410, 1121, 5377]]\n",
      "[np.int64(0)]\n",
      "[np.str_('22370330')]\n",
      "[[3966, 5667, 4976, 2771, 3856, 3077, 1180, 131, 5637, 4397, 952, 5303, 4805, 4801, 3803, 2009, 2788, 3856, 5247, 3361, 5295], [4217, 3887, 3388, 946, 4948, 5635, 1045, 842, 4360, 3803, 3217, 4755, 3888, 2842, 888, 1741, 2690, 5150, 1150, 842, 5150, 4523, 1865]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('15452187')]\n",
      "[[5261, 1901, 1096, 4068, 1534, 5042, 4397, 952, 3966, 5303, 4068, 3856, 2103, 766, 0, 0, 0, 0, 385, 5706, 5046, 5631, 2410, 173, 5632], [5240, 2838, 3803, 5337, 2832, 5240, 2573, 5614, 4843], [2103, 766, 4841, 2825, 2661, 842, 4477, 5357, 2832, 5261, 3965, 4105], [5240, 2661, 4603, 3051, 3803, 5376, 923, 2591, 3379, 1275, 2832, 2462, 948, 3803, 1180, 5250, 2279, 2304, 4743, 2446, 1018, 3803, 1403, 5667, 2410, 5240, 3719, 27], [5240, 5042, 639, 0, 3966, 0, 5637, 945, 2410, 1994, 842, 0, 5637, 945, 2410, 4353], [4353, 2661, 842, 4422, 5357, 5637, 3385, 4206, 842, 3535]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('23989945')]\n",
      "[[5251, 5614, 3691, 4995, 4840, 4353, 1811, 1060, 5376, 2832, 5240, 1380, 1394, 3260, 4527, 5303, 2695, 1811, 3803, 597, 1466, 2996, 199, 5303, 282], [797, 2921, 4154, 5383, 3003, 4641, 1449, 808, 2631, 1205, 0, 842, 4916, 3966, 4493, 1394, 5376, 0, 5637, 3146, 3539, 2312, 2832, 4113, 5383, 3003], [3683, 4805, 3966, 1045, 3806, 3891, 1017, 1180, 5376, 5637, 4393, 5303, 5376, 1716, 2832, 2695, 1663, 5466, 963, 5240, 3965, 2685, 3856, 2832, 3255, 2508, 4148, 2556, 5114]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('20832982')]\n",
      "[[3966, 1416, 3388, 947, 3921, 4322, 1865, 4360, 3803, 3217, 4353, 842, 4769, 1994, 2410, 5150, 3334, 1208, 1416, 3388, 947, 4322, 1865, 1207, 0, 842, 4769, 1994, 2410, 0, 5240, 3965, 3331, 5153], [5413, 2717, 5260, 5270, 3286, 1180, 3966, 842, 5241, 1208, 5637, 4397, 952, 5303, 4441, 156, 5213, 1017, 4801, 3803, 2009, 1207, 956, 1613, 3856, 1984, 5103, 3037, 5240, 1207], [5261, 5042, 5231, 5240, 1994, 3803, 1207, 956, 1613, 4295, 2832, 4711, 3803, 3966, 5667, 3286, 1180]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15297582')]\n",
      "[[3691, 5068, 1812, 5637, 1767, 1060, 3888, 3803, 2035, 5667, 4340, 3953, 3856, 5389, 655, 2502, 3454], [5251, 5637, 3691, 1812, 2832, 3921, 4810, 3881, 0, 5153, 3856, 3399, 5508, 1060, 5240, 5413, 5376, 2573], [3691, 4840, 1812, 5637, 3744, 963, 1018, 1060, 5240, 5413, 5376, 2573], [4824, 5222, 2832, 2695, 3399, 5508, 842, 3921, 3209, 5637, 4461], [5150, 842, 4360, 3803, 3217, 4353, 4998, 5614, 1774, 5667, 5508, 3803, 4370]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16376417')]\n",
      "[[3900, 2279, 4745, 1802, 3741, 1809, 4841, 1060, 924, 963, 5240, 0, 948, 526, 199, 1588, 532, 183], [5240, 1127, 4815, 4641, 1682, 2832, 3921, 4745, 2832, 1121, 924, 2446, 5240, 2358, 5303, 0, 949], [963, 5240, 0, 5287, 4095, 483, 3803, 3249, 3966, 2832, 1121, 924, 1416, 4353, 948], [1297, 1176, 0, 725, 3966, 4360, 3803, 3217, 4353]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('11766999')]\n",
      "[[0, 1793, 842, 1989, 5376, 3054, 5253, 0], [0, 2826, 2832, 4360, 3803, 3217, 5614, 789, 3781, 2832, 1121, 2573]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('9402173')]\n",
      "[[5413, 2717, 3966, 5667, 711, 1377, 1180, 4442, 4341, 2453, 323, 3445, 3303, 1663, 2410, 3369, 3803, 272, 5632, 842, 5637, 4393, 5303, 4341, 2453, 783, 3856, 4341, 2453, 3493, 100, 3445, 3303, 3445, 3303, 2446, 3095, 216, 5631, 2410, 1582], [3391, 5122, 5614, 156, 3536, 5667, 3493, 842, 164, 3536, 2832, 2453, 783, 3828, 5692, 5122, 442, 5601, 471], [4341, 2453, 3493, 1234, 3539, 3900, 2593, 5337, 1152, 1615, 2561, 5614, 2857, 3833, 2410, 0]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('16787687')]\n",
      "[[5625, 1403, 5240, 1988, 3822, 4353, 3803, 5413, 1883, 4079, 4496, 5667, 5581, 1321], [3900, 3966, 5374, 5667, 2009, 1883, 1515, 4495, 2591, 1059, 4353, 5237, 5550, 5374, 3966, 3170, 2541, 3071, 4353, 5308, 87, 2410, 1666, 842, 39, 2410, 1667, 5562, 5550, 2104, 480, 2541, 3071, 2631, 4989, 5308, 39, 2410, 1666, 842, 9, 2410, 1667, 5562, 5550], [5645, 3921, 4536, 5667, 1667, 842, 5550, 5614, 4843]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('23873790')]\n",
      "[[4623, 4831, 3900, 4840, 2827, 2832, 4325, 5030, 842, 4360, 3803, 3217, 2832, 1121, 5240, 3372, 842, 1985, 5103, 2573, 2791, 4113, 2998], [4325, 5030, 5614, 3385, 5512, 5240, 5153, 1287, 586, 4649, 842, 4360, 3803, 3217, 5614, 3385, 5512, 5240, 3397, 3888, 5042, 4824, 2412, 2631, 5120, 963, 1018, 2791, 4113, 2998, 842, 963, 3536], [5240, 4337, 3803, 5261, 5042, 5614, 5303, 944, 5240, 1992, 3803, 3746, 3468, 2998, 0, 3468, 1017, 0, 5250, 3372, 5562, 4983, 1985, 5103, 3822, 2877, 3803, 5030, 842, 4360, 3803, 3217, 2832, 1135, 1180, 3966, 5667, 2662, 5030, 3209]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('20881891')]\n",
      "[[5143, 5303, 839, 0, 1688, 5240, 2056, 5258, 842, 5515, 5598, 1152, 2857, 4812, 1873], [5240, 4217, 2061, 5637, 5240, 1812, 2832, 1901, 2056, 5258, 842, 5515, 5598, 733, 5692], [4698, 842, 1989, 3000, 917, 3638, 5303, 773, 4812, 1948]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('18093738')]\n",
      "[[949, 963, 1018, 3828, 5270, 842, 3536, 2842, 1014, 5240, 1140, 3921, 3023, 3844, 5508, 842, 4049, 842, 3764, 4414, 3803, 5241, 3966, 3921], [963, 4862, 3827, 1343, 4050, 257, 842, 3765, 265, 2074, 3966, 593, 5652, 5637, 3899, 191, 5693, 3803, 737, 5667, 1180, 1792, 3921, 842, 3217, 2235, 3803, 963, 3180, 3536]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('12181240')]\n",
      "[[5376, 924, 5637, 1011, 5667, 4596, 5303, 4006, 4998, 2559, 566, 5562, 574, 4597, 842, 5240, 4192, 3803, 3430, 590, 5562, 584, 4597], [5240, 4193, 5042, 754, 5303, 1402, 2470, 5667, 2453, 783, 5648, 5614, 5240, 1533, 923, 2832, 2415, 3371, 1336, 5384], [1152, 3965, 5122, 842, 4360, 3803, 3217, 2613, 2825, 5667, 5261, 3502, 2832, 4916, 5384], [5240, 5122, 4411, 963, 3536, 5637, 304, 842, 374, 2410, 5240, 2453, 842, 2470, 924, 4597, 842, 5692, 5122, 4411, 5637, 842, 183, 3263, 4402, 5230, 100], [2832, 4213, 4039, 2768, 5383, 2387, 2453, 4087, 1321, 2470, 5695, 292, 4605, 4409, 842, 311, 5122, 4409, 963, 5692], [2832, 1121, 924, 1297, 5614, 4556, 963, 1663, 311], [3966, 5667, 5487, 0, 3856, 2673, 4300, 3433, 3856, 3258, 711, 684, 3803, 5240, 3932, 5637, 1693, 3383, 3856, 2173]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('22340966')]\n",
      "[[2572, 1239, 4760, 5303, 1028, 4698, 842, 1989, 5376, 2410, 5675, 5652, 2614, 4234, 2660, 733, 1135, 1180, 5376, 5667, 679, 1053, 5303, 3537, 4873, 842, 4360, 3803, 3217], [5240, 5376, 1573, 1028, 2853, 3005, 1135, 1180, 5127, 0, 842, 1716, 1154, 5346, 1135, 1180, 3765], [5240, 5383, 3054, 4501, 0, 842, 5614, 1347, 3345, 164, 239], [3948, 5637, 4397, 777, 5303, 4441, 2009, 5513, 1205, 3856, 5513, 1205, 4087, 2572, 1239], [4698, 1989, 3698, 2691, 5377, 917, 3638], [2572, 1239, 4841, 4477, 2660, 4233, 4413, 963, 5632, 733, 4392, 1403, 5667, 5513, 1205, 3379, 1811, 508, 597, 1313, 404, 5303, 589, 4, 842, 2827, 5637, 3314, 963, 292, 5632, 3379, 1811, 546, 454, 5303, 604, 4]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23352440')]\n",
      "[[4808, 5022, 2844, 2572, 1043, 1985, 3000, 4048, 5348, 842, 3856, 1805, 1984, 2614, 1035, 5231, 5303, 857, 5254, 1819, 5667, 3507, 5074, 2832, 5240, 3267, 4692], [888, 4743, 5614, 4826, 0, 1154, 2998, 3534, 2127, 272, 396, 77, 842, 1741, 4743, 3539, 1938, 2127, 417, 532, 191, 348, 85, 94, 842, 292, 493, 119, 963, 130, 842, 272, 3536], [2998, 1435, 2572, 4048, 5348, 1805, 1984, 842, 4052]]\n",
      "[np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('16487438')]\n",
      "[[797, 5261, 3941, 1802, 3741, 1809, 2446, 5240, 4484, 2572, 963, 1018], [5625, 2175, 4360, 3803, 3217, 5512, 2364, 5530, 4370], [4612, 3803, 2171, 2832, 2508, 1888, 3741, 725, 4360, 3803, 3217], [5411, 2426, 5054, 5667, 2678, 3803, 1816, 5281, 1198, 5667, 100, 5693, 5394, 5108, 5250, 5667, 5284, 1416, 5240, 5042], [5240, 2802, 3803, 4269, 5045, 2730, 3822, 4360, 3803, 3217, 3054, 5443], [5284, 1896, 5614, 4559, 1154, 5042, 3399, 1515, 5284, 3856, 5284, 4087, 4068]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('19064973')]\n",
      "[[5240, 5692, 3900, 5122, 2410, 3646, 5562, 1533, 923, 5614, 596, 842, 508, 2616, 4415, 272, 597, 1313, 0, 5303, 535, 36], [1896, 2961, 3803, 1457, 1321, 3156, 4687, 5336, 842, 4360, 3803, 3217, 4745, 5637, 1398, 2832, 1121, 924], [4039, 2771, 5042, 5303, 0, 5230, 5261, 3646, 1457, 5023, 3054, 5612], [5251, 5614, 2662, 4409, 3803, 2559, 3667, 600, 1152, 3741, 3668, 2334, 130, 1940, 3646, 1297]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('23466360')]\n",
      "[[772, 0, 2844, 5240, 3076, 842, 2169, 5637, 0, 3822, 5167, 0, 3561, 963, 1663, 322, 842, 1898, 4442, 5637, 1166, 2446, 2752, 3803, 0, 3822, 1614, 0, 0, 4402, 1562, 1361, 5614, 5509, 2410, 4994, 828], [1898, 5303, 5240, 3076, 842, 2169, 3822, 3561, 1614, 4161, 5680, 5499, 2461, 5667, 2567, 1122, 0, 5153, 842, 0], [842, 0, 5681, 5642, 5240, 3076, 0, 5614, 604, 374, 45], [0, 2857, 5642, 5240, 3076, 0, 5614, 597, 369, 45, 842, 5240, 2169, 0, 5614, 272, 333, 67], [1122, 5681, 5642, 5240, 3076, 0, 5614, 604, 327, 77, 3856, 5240, 2169, 0, 5614, 493, 327, 77], [5054, 5637, 396, 1485, 3414, 2446, 4287, 5042, 3803, 1129, 932, 3532, 5667, 138, 2410, 2983, 4662, 3257, 4290, 1180]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('16157934')]\n",
      "[[4605, 830, 1403, 5240, 4280, 3803, 3966, 5652, 1725, 814, 2821, 1275, 2832, 4353], [5303, 2174, 5240, 2802, 3803, 3202, 1403, 5667, 4068, 733, 697, 5184, 3822, 4360, 3803, 3217, 4353, 2832, 5240, 3304, 183, 5383], [4888, 1992, 5637, 4761, 2832, 4916, 1891, 1502, 5667, 3477, 3803, 3966, 2243, 1277, 2832, 4353, 0, 5667, 4480, 2832, 2154, 5158], [3202, 1802, 3741, 2614, 814, 715, 2802, 3822, 3900, 4353]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('9849454')]\n",
      "[[5667, 3391, 2403, 5489, 3803, 5693, 5240, 5692, 3900, 5122, 5614, 511, 4983, 2121, 3803, 5240, 3379, 0, 842, 5240, 5692, 2182, 2439, 5122, 450, 0], [5261, 5383, 789, 2410, 5240, 2358, 5287, 4305, 2188, 5238, 4632, 733, 3255, 4520, 1176, 645, 3267, 5222, 4754, 4548], [3966, 5667, 2848, 4580, 5405, 4442, 4862, 5303, 100, 1582, 3803, 3078, 635, 5303, 4976], [1403, 5667, 5240, 4623, 3786, 2832, 5240, 4213, 4854, 5042, 5376, 2832, 3495, 5614, 1017, 3822, 4605, 5303, 2921, 1297, 842, 1760, 814, 3900, 4480, 3803, 5240, 5508, 3803, 3255, 5250, 4841, 2825, 5122, 2410, 3966, 5667, 3698, 3433, 1850], [3966, 648, 1415, 4547, 5667, 1297, 783, 1802, 3741, 5514, 4441, 4386, 3856, 5446, 2262, 5115, 1152, 3966, 4543, 2832, 3946, 4547, 4442, 3255, 5250, 5667, 5115, 842, 3856, 4386], [196, 4214, 5487, 2028, 3966, 2082, 5240, 5042]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16572647')]\n",
      "[[5240, 3379, 810, 3803, 4585, 5403, 5291, 5614, 304, 2410, 4983, 5115, 4967, 842, 144, 2410, 5115, 3037, 3659, 4896], [5240, 989, 1463, 4287, 4395, 5042, 2832, 5648, 417, 3966, 1960, 2609, 4910, 1530, 2070, 3006, 5403, 5637, 4395, 2410, 5115, 5667, 3856, 5673, 3659]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('19884558')]\n",
      "[[5330, 3803, 0, 3948, 2446, 130, 3827, 1343, 5637, 4397, 952, 2832, 1337, 5383], [4805, 1664, 3803, 1800, 842, 0, 1659, 1802, 3741, 1564, 5667, 3535, 0, 4314], [4873, 4360, 1810, 3899, 5693, 5287, 0, 9, 1154, 2572, 185, 57, 842, 3899, 5287, 1154, 2572, 0, 37]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('21751205')]\n",
      "[[4353, 5614, 946, 1154, 5512, 5240, 1979, 5150, 948, 4723, 2128, 842, 5240, 2462, 948, 3803, 1180, 5250, 2279, 845, 4370], [5251, 5614, 3691, 4840, 1811, 2832, 4353, 932, 946, 1154, 5240, 2279, 845, 3856, 2128, 2949], [3379, 3803, 100, 5468, 3803, 4153, 3899, 3536, 5637, 698, 5303, 5264, 5652, 4442, 2103, 786, 1403, 5667, 144, 5468, 2410, 5264, 5652, 1802, 3741, 4441, 2103, 67]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('24092240')]\n",
      "[[5303, 2174, 5240, 1992, 3803, 5240, 1306, 3401, 1352, 2364, 2019, 3594, 3822, 4360, 3803, 3217, 2410, 4773, 842, 3698, 4773, 711, 1180, 3966], [1581, 3803, 5376, 5614, 5632, 5667, 322, 3465, 1960, 1663, 1664, 5629], [3966, 2832, 5240, 1352, 2364, 2019, 3594, 2572, 0, 5303, 1352, 2364, 2019, 3594, 5240, 3966, 2832, 5240, 5638, 3594, 2572, 0, 5303, 5638, 3594, 842, 5240, 3966, 2832, 5240, 3691, 3594, 2572, 1802, 3741, 0, 5303, 3594]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('20888993')]\n",
      "[[3966, 952, 5303, 1713, 5376, 1523, 3359, 1159, 3387, 5667, 5376, 1386, 963, 1337, 3856, 5151, 4520], [772, 3966, 5637, 5374, 635, 5303, 4983, 3255, 4148], [2832, 5240, 0, 0, 2090, 0, 1368, 5383, 5625, 754, 5303, 2142, 5240, 1053, 3803, 1963, 5376, 3822, 5240, 1020, 3803, 2857, 1159, 1446, 1403, 5667, 1713, 5376, 3822, 5240, 1020, 3803, 1337, 4472], [5240, 4217, 3887, 5614, 3900, 5122]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('20178647')]\n",
      "[[2533, 5240, 715, 1988, 3803, 1591, 2760, 5195, 3822, 4353, 5240, 5042, 3803, 3881, 3746, 741, 5238, 5189, 1023, 2293, 0, 3054, 5612], [5376, 5667, 3442, 5614, 5635, 5316, 842, 1725, 1337, 664, 2832, 3966, 5667, 1607], [0, 2559, 3667, 3791, 2832, 191, 842, 3966, 842, 2559, 5303, 5275, 2832, 3966, 842, 3965, 2832, 923, 842, 923, 4597]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('16944295')]\n",
      "[[3966, 1416, 5240, 2462, 948, 3803, 1180, 5250, 1135, 2279, 4369, 4087, 2054, 5060, 2127, 963, 1018, 842, 3536, 842, 2186, 3536, 5252], [2656, 5625, 4562, 0, 2711, 3888, 2446, 5240, 0, 5383, 1405, 839, 5667, 5184, 932, 4217, 697, 5250, 2410, 4121, 5675, 5667, 3257, 1135, 1180], [5240, 2802, 3803, 5376, 3822, 2631, 4523, 4360, 3803, 3217, 2711, 3054, 814, 2821, 1498, 2832, 5240, 697, 5376, 3803, 3839, 1135, 1180]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('19495810')]\n",
      "[[5240, 689, 4409, 5303, 790, 5614, 4841, 2663, 5237, 5240, 689, 5303, 4769, 3334, 5250], [1152, 3691, 3267, 5222, 1988, 5614, 3744], [5240, 4803, 5614, 2832, 0, 0, 1744, 842, 5215, 3803, 333, 341, 1710, 0, 2127, 0], [0, 3292, 5250, 790, 3054, 3746, 5376, 2410, 3226, 5598, 4480]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('21237636')]\n",
      "[[4755, 3888, 5637, 4669, 842, 2045, 2465, 842, 2328, 3803, 1533, 842, 888], [5303, 3025, 5646, 2443, 2695, 2403, 5489, 2832, 5240, 2358, 5692, 733, 1135, 1180, 5376, 3460, 3957, 1028, 4559, 1154, 3764, 3182, 5213, 2403, 5489, 5673, 0, 2631, 4523, 4360, 3803, 3217, 2711, 842, 5646, 4824, 1985, 2572, 4256, 2005, 5684, 2066, 2711], [2651, 3764, 3182, 5213, 2403, 5489, 4760, 814, 910, 5620, 5303, 4476, 1336, 5585, 842, 4570, 814, 631, 796, 5023]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('19349389')]\n",
      "[[1425, 1912, 5287, 2695, 5000, 842, 4360, 3803, 3217, 3397, 3888, 5042, 363, 3071, 4824, 2412, 842, 5679, 2631, 3863, 4360, 3803, 3217, 4370, 5637, 789, 834], [5240, 2790, 5330, 2757, 586, 3286, 2232, 5614, 3781, 2832, 300, 3966, 417, 842, 5614, 3539, 2443, 2832, 5240, 5548, 2572, 483, 5601, 322, 4597, 54], [1940, 2403, 5489, 528, 3803, 5240, 3966, 4831, 0, 3856, 2825, 3286, 2232, 842, 3966, 164, 3638, 3672, 4084, 4237, 5548, 2572, 4473, 5393, 2572, 4473, 0], [4287, 4395, 5042, 5238, 2842, 483, 3966, 417, 5675, 164, 3414, 3379, 737, 460, 5693, 5667, 4474, 3328, 4084, 1999, 1060, 3081, 232, 842, 3081, 235], [3286, 2232, 5614, 2175, 5277, 1298, 1614, 4728, 3786, 842, 3536, 733, 4085]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('20038728')]\n",
      "[[668, 2591, 5240, 679, 1051, 3803, 2857, 4811, 0, 2832, 4916, 5675, 842, 3544, 4563, 814, 2826, 2832, 5241, 2065, 0, 3803, 5266, 842, 4776, 3803, 5635, 1045], [5547, 5153, 917, 1389, 715, 1992, 3803, 876, 2692, 5376, 2832, 1540, 1135, 1180, 1205], [5555, 0, 5240, 5250, 3803, 1307, 2410, 5254, 5153, 2613, 0, 715, 1992], [2344, 3966, 5637, 4397, 952, 5303, 4441, 130, 5632, 3803, 668, 282, 3856, 5555, 282, 5376], [1121, 2573, 2222, 4840, 1689, 2832, 2701, 2370, 1742, 5153, 842, 3881, 4360, 3803, 3217, 5153, 2844, 4840, 2827, 2832, 3419, 2631, 2446, 4154, 5303, 4113, 5376], [5240, 5555, 2572, 2241, 191, 2839, 3803, 715, 1992, 2002, 3625, 1929, 3549, 1876, 888, 5645, 5240, 668, 2572, 2241, 3691, 3641, 715, 1992]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('22290823')]\n",
      "[[1121, 1140, 842, 2262, 4257, 2591, 4108, 3888, 2410, 3965, 1207, 1943], [1659, 5637, 1370, 4224, 5303, 2998, 842, 4113, 2998, 842, 3536, 2446, 1018]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('23017512')]\n",
      "[[1180, 4581, 5614, 946, 1154, 5116, 3346, 4998], [3822, 3585, 3264, 4504, 830, 1415, 1091, 3635, 4198, 5614, 814, 2864, 4108, 4164, 3803, 1519], [5625, 3026, 5240, 2902, 3803, 1091, 3635, 4198, 3822, 5499, 1519, 4360, 3803, 3217, 842, 5116, 3348, 733, 4378, 4291]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('19333052')]\n",
      "[[2705, 3069, 3370, 1028, 957, 5667, 3539, 0, 2832, 5685, 1205, 842, 3268, 2630, 5287, 5237, 0, 2209, 842, 0, 2366], [5240, 0, 842, 0, 4236, 4305, 3539, 1337, 1053, 2832, 5240, 5376, 3803, 4058, 1850, 5237, 0, 2209, 842, 0, 2366, 842, 4828, 1028, 1499, 5240, 4236, 3803, 1307], [1403, 5667, 4538, 3966, 4443, 5430, 2591, 4841, 4825, 1939, 3803, 3841, 842, 2695, 5000, 4825, 5287, 4014, 5303, 5605, 4637, 5303, 1656, 663, 3856, 4857, 5673, 3921, 842, 5303, 4637, 5303, 5676, 3856, 4737, 842, 2336, 1425], [4370, 5637, 698, 1154, 1340, 1097, 5303, 5376], [4122, 3921, 5614, 946, 5667, 5587, 822, 4723, 842, 5240, 0, 3921, 4369], [3966, 5667, 5430, 2591, 3275, 3209, 3803, 3921, 3828, 5629, 733, 5115], [5240, 753, 3803, 5240, 5042, 5614, 5303, 1402, 1337, 3888, 842, 4360, 3803, 3217, 2832, 3966, 5447, 5115, 2410, 4058, 1850, 5667, 0, 842, 0, 5430, 3856, 0, 2209, 842, 0, 2366, 4538, 4237]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('9106647')]\n",
      "[[5042, 1350, 5614, 4457], [3857, 2160, 3054, 814, 662, 4851, 740, 2832, 4888, 1253, 3286, 1180, 4741, 842, 3054, 5659, 4190, 932, 2358, 3233, 5376, 932, 814, 796, 5303, 3014, 1378, 1297, 2832, 3966, 5667, 2262, 1850], [772, 942, 3803, 5150, 1533, 842, 4360, 3803, 3217, 5637, 2009, 5240, 4709, 3856, 5680, 2832, 5240, 3857, 2160, 2572], [5122, 5614, 2897, 963, 5692, 2832, 5240, 3857, 2160, 2572, 1403, 5667, 3014, 5250, 2410, 3857, 5562, 199, 2410, 3014, 1811, 597, 1466, 2996, 3803, 1811, 191, 77, 842, 5251, 5614, 5378, 5333, 2897, 3900, 5122], [2832, 4395, 5383, 3803, 3929, 5376, 2832, 711, 4741, 3857, 2160, 101, 3445, 2533, 5412, 1656, 2410, 1664, 5614, 1403, 5667, 3014, 1297, 1504, 3803, 794, 1635, 3803, 1321, 842, 2160, 3984, 842, 1636, 1908, 842, 5579, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('22370314')]\n",
      "[[5240, 5692, 4031, 4411, 5637, 173, 5667, 2501, 842, 5667, 2556, 2708, 0, 597, 1313, 0, 5303, 0], [1152, 4870, 5336, 842, 3247, 1948, 5637, 3539, 1389, 2832, 5240, 2501, 923], [5270, 2717, 3681, 3966, 5637, 834, 3995, 4295, 2501, 923, 172, 2556, 923, 165]]\n",
      "[np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('7989941')]\n",
      "[[5240, 3777, 5238, 3268, 2910, 4244, 3539, 3606, 1152, 1888, 3741, 0, 2663, 4605, 4411, 4828, 3174, 5303, 2471, 5041, 5303, 1773, 5240, 3849, 1896, 842, 4732, 3803, 5261, 0, 3672, 740], [4841, 3539, 3667, 5614, 4761, 5642, 5197, 5614, 698, 932, 272, 2702, 2910], [5675, 5667, 4079, 4205, 2101, 3897, 1180, 842, 3383, 4474, 1850, 5637, 4395, 2832, 1069, 1754, 5303, 4441, 2009, 186, 3856, 153, 3445, 3303, 3803, 5197, 3899, 2009, 272, 3856, 2703], [5251, 3054, 3513, 1896, 1988, 5667, 3268, 5287, 5303, 4261, 963, 186, 3445, 3303], [2832, 3718, 5384, 1898, 5040, 2614, 4399, 2446, 153, 3445, 3303, 5303, 283, 3445, 3303, 698, 3899, 272, 2703, 5667, 4178, 5303, 995, 0, 4432, 2714]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23139058')]\n",
      "[[0, 2150, 0, 3505, 1774, 5287, 1018, 5632, 4312, 3455, 2998, 3536, 4312, 4113, 2998, 130, 3536, 4312, 2572, 2452, 5211, 5513, 1205, 5418, 842, 5287, 1154, 2572, 1992], [5261, 5365, 2998, 5383, 1716, 2009, 2452, 3856, 5211, 5106, 2217, 932, 2412, 3803, 697, 1135, 1180, 5250, 5238, 1176, 4208, 1684, 2832, 2362, 842, 2461, 1940, 5376, 842, 0, 4466, 4113, 5376], [5380, 3781, 2410, 5240, 5376, 2573, 5637, 4843], [5251, 5637, 3691, 1812, 2832, 3881, 5376, 4523, 4835, 1992, 1060, 2573], [797, 2827, 2832, 5240, 5418, 2572, 3791, 1154, 130, 3536, 4113, 5115]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23917308')]\n",
      "[[5240, 753, 3803, 5261, 2317, 5383, 5614, 5303, 1773, 5240, 4701, 842, 1994, 3803, 4588, 2217, 1154, 4290, 1180, 5126, 5667, 1115, 3433, 1850], [4698, 4588, 2217, 4191, 3370, 0, 5261, 1988], [5240, 2217, 4255, 5614, 5635, 5316, 932, 2189, 1154, 2662, 969, 566, 842, 1422, 4411, 593, 842, 5240, 613, 3803, 5240, 3948, 5303, 2217, 963, 814, 2962, 5672, 5240, 5189, 4398, 2410, 1180, 5126, 4413, 3803, 3996, 0, 144], [5240, 1275, 2832, 4048, 2461, 3590, 5028, 119, 0, 721, 2217, 1189, 842, 0, 130, 4048, 664, 3208, 272, 842, 3178, 3360, 1810, 4841, 1060, 2573, 2405, 5240, 2998, 5667, 2307, 1277, 2832, 5240, 2217, 2572, 1403, 5667, 5240, 5513, 1205, 2572], [2364, 3948, 2217, 5513, 1205, 1802, 3741, 1415, 5240, 2998, 5270, 3803, 5648, 5637, 1935, 5303, 0, 1850, 2217, 5513, 1205]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('22552194')]\n",
      "[[5261, 5383, 3054, 4501, 5667, 5240, 3652, 5383, 0, 0, 5198, 0], [5116, 4581, 3054, 4492, 932, 5240, 3833, 1623, 3853, 2410, 4579, 3798, 1180, 1152, 4336, 1425, 0, 2832, 3539, 5237, 2599, 3803, 3966, 733, 3837, 3799, 917, 2566, 0]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('21821474')]\n",
      "[[814, 5452, 3803, 5240, 0, 3803, 4835, 1992, 842, 5287, 1826, 3803, 5241, 3792, 2410, 1960, 5376, 3054, 2821, 2410, 2457, 3965, 2906], [814, 679, 306, 3414, 5637, 2842, 2832, 4105, 1017, 1533, 2572, 3364, 2410, 4498, 842, 737], [2662, 4769, 946, 4360, 3803, 3217, 5614, 4563, 1154, 490, 355, 3803, 190, 3414, 777, 4378, 4291, 460, 348, 3803, 174, 3414, 952, 5303, 5618, 5603, 842, 593, 417, 3803, 244, 3414, 2832, 5240, 1533, 2572], [2410, 3414, 5667, 3256, 4290, 1180, 5115, 4305, 5122, 1051, 1403, 5667, 5618, 5603], [5377, 917, 957, 5667, 3538]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21751295')]\n",
      "[[5261, 5042, 2198, 2282, 957, 5667, 978, 2446, 4395, 1534, 5383, 1405, 3380, 1258, 2572, 4326, 3375, 814, 2998, 1756, 5303, 2641, 711, 1180, 3966, 5131, 3856, 2066, 5241, 4776, 3803, 3380, 5303, 5240, 5105, 2572, 4326, 4819, 4985, 5103, 2572], [5251, 5637, 3691, 3881, 4840, 0, 1060, 978, 842, 889, 1722, 3397, 4316, 3856, 5042, 4523, 5539]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('11788911')]\n",
      "[[5261, 4495, 1573, 3741, 1028, 4457, 2410, 4039, 2771, 4395, 5042], [2008, 3828, 3997, 3803, 3966, 2591, 4006, 4998, 842, 265, 3803, 3966, 2591, 5634, 3272, 963, 5240, 4986, 3803, 5376], [3540, 5240, 3267, 3900, 5376, 1939, 3595, 1028, 5179, 3005, 1498], [2338, 5614, 698, 3822, 4295, 5303, 130, 3966, 3966, 437, 2591, 1590, 3965, 2591, 4146, 2410, 814, 3873, 3803, 475, 3966, 173, 2591, 4749, 842, 3966, 282, 2591, 3981]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16835585')]\n",
      "[[842, 3274, 1730, 3239, 1309, 4841, 2857, 32, 2832, 5240, 2216, 2572, 963, 5240, 2051, 3803, 5240, 5692, 5042, 4012], [5240, 753, 3803, 5261, 4393, 5042, 5614, 5303, 2174, 5240, 3270, 1277, 2832, 1111, 1430, 842, 0, 4249, 2832, 4121, 5675, 5142, 2446, 5184, 5303, 2216], [5241, 862, 1659, 1111, 1430, 2844, 2301, 3360, 2393, 842, 2301, 2439, 3360, 2337, 842, 0, 4249, 1173, 2953, 842, 4048, 664, 5637, 946, 5629, 1036, 4392, 842, 842, 130, 3536, 3158], [3885, 2353, 5082, 5238, 5143, 3966, 5303, 697, 2216, 5376, 733, 963, 3180, 5693, 3803, 5184, 5250, 3370, 1028, 957, 5667, 814, 712, 3899, 1525, 697, 5184, 5376, 2832, 5226, 3803, 1111, 1430], [2832, 5330, 483, 0, 3856, 3772, 4121, 3966, 5637, 2074], [797, 3344, 3803, 5240, 715, 1992, 957, 5667, 2216, 842, 5184, 2614, 1035, 827, 5251, 917, 3691, 1399, 1659, 1448, 1111, 5634, 842, 1111, 1430], [4444, 5041, 2614, 4833, 5238, 699, 5240, 925, 2918, 2216, 733, 5693, 3803, 5184, 5250, 4841, 2828, 1850, 2439, 5122, 2832, 4121, 5675, 5667, 4217, 1135, 1180, 2832, 1406, 5667, 4983, 5692, 5184, 5376]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('11786563')]\n",
      "[[2832, 5240, 4193, 930, 5240, 1992, 3803, 965, 3822, 1111, 1430, 2065, 2953, 842, 2065, 2238, 932, 4755, 3887, 3388, 2832, 5240, 4709, 3966, 917, 4563], [897, 789, 4542, 4974, 2832, 5240, 965, 2572, 1152, 1688, 4841, 2832, 5240, 1533, 2572, 7], [5254, 1992, 917, 3957, 0, 5303, 3316, 3803, 2065, 2953]]\n",
      "[np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('17932908')]\n",
      "[[3966, 5637, 4395, 2832, 1154, 2281, 1754, 5303, 3403, 643, 225, 3445, 1656, 2978, 766, 3463, 3075, 5289, 5629, 5046, 2981, 3463, 3075, 1656, 3856, 1378, 3803, 1121, 1640], [5046, 2981, 842, 3856, 2978, 766, 4301, 3691, 5122, 1051, 2832, 3433, 4554, 1181, 3803, 2983, 4252, 842, 5255, 0, 4840, 4662, 3803, 5337], [5251, 5637, 3691, 4840, 5122, 1812, 1060, 5240, 278, 2978, 766, 5374, 3966, 842, 0, 0, 766, 3966, 2616, 4415, 5705, 597, 1313, 559, 272, 3856, 1060, 5240, 280, 2981, 842, 0, 0, 5374, 3966, 2616, 4415, 89, 597, 1313, 577, 341, 3263, 4402, 604, 842, 446, 4597]]\n",
      "[np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('22825506')]\n",
      "[[3828, 2572, 5614, 5374, 5667, 1666, 1315, 2794, 1180, 5250, 842, 5240, 3881, 5614, 5179, 932, 1533, 2572], [1666, 1315, 5376, 2591, 4133, 1051, 2410, 3966, 5667, 711, 3698, 4888, 1253, 3286, 1180, 1403, 5667, 5240, 1533, 2572, 842, 2591, 3691, 3788, 4835, 1992]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('20009826')]\n",
      "[[5411, 4805, 5675, 5637, 4395, 4288, 5303, 5446, 2009, 2928, 3856, 3707, 2439, 5351, 2366, 1135, 4459], [2007, 3803, 300, 5675, 4638, 5241, 4370, 3379, 427, 3536, 733, 2439, 5351, 2366, 4459], [5270, 5530, 3887, 5324, 5637, 698, 733, 5115, 5240, 3397, 3888, 5042, 363, 3071, 4824, 2412, 2631, 5120, 5240, 1111, 2782, 733, 1135, 1180, 4369, 842, 5240, 2462, 948, 3803, 1180, 5250, 1135], [5261, 5042, 1726, 5238, 2929, 3803, 2439, 5351, 2367, 5509, 2410, 1135, 4459, 3741, 3833, 2828, 4778, 1152, 789, 2613, 4108, 1988, 3822, 3965, 4410, 4360, 3803, 3217]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('22526159')]\n",
      "[[2419, 5303, 2252, 5240, 4170, 842, 1422, 3803, 2012, 3966, 4493], [845, 5614, 5240, 3544, 1389, 2646, 5337, 582, 3803, 3966], [5411, 2364, 3803, 442, 4467, 3966, 1416, 5250, 963, 5240, 5287, 3803, 5240, 4071, 2980, 830, 3391, 737, 543, 5693, 4398, 526, 563], [3856, 5381, 2410, 5240, 5376, 3803, 4521, 3897, 1198, 5675, 738, 502, 5693, 3856, 3816, 5637, 2842, 2832, 5261, 4287, 3575, 5042], [484, 3445, 1663, 304, 3803, 467, 1663, 1634, 2410, 3369, 3803, 130, 1635], [5381, 814, 0, 740, 2613, 1725, 664, 2832, 4474, 3897, 1198]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('11106123')]\n",
      "[[1589, 119, 3212, 2453, 1404, 2308, 2832, 5222, 3803, 664, 842, 5337, 5667, 3881, 1378, 4496, 2844, 1589, 119, 842, 1526, 2911, 2453], [3828, 2717, 2344, 3681, 3966, 5667, 711, 1377, 1198, 4214, 5487, 2410, 5240, 3433, 1850, 348, 3803, 5242, 4214, 0, 5303, 697, 2453, 5637, 4397, 777, 5303, 4441, 1589, 119, 225, 3445, 3303], [5240, 4684, 5637, 348, 597, 1466, 2996, 597, 1313, 246, 427, 2832, 923, 2844, 1415, 4606, 1609, 842, 164, 3946, 4606, 4310, 272, 597, 1313, 156, 374, 2832, 923, 2844, 1609, 842, 119, 4310, 842, 272, 597, 1313, 156, 374, 5667, 1609, 842, 119, 4310, 2832, 923], [2910, 842, 2453, 571, 3445, 3303], [2910, 842, 2453, 0, 3445, 3303], [3822, 1663, 2404, 3822, 1663, 1154, 3212, 283, 3445, 3303], [1582, 5637, 4556, 2186, 5413, 5632, 2832, 772, 924, 3803, 5240, 5383], [3822, 1663, 2404, 3822, 1663, 1154, 3212, 283, 3445, 3303]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('18306372')]\n",
      "[[0, 5637, 3965, 4170, 2410, 2572, 954, 3350, 4998, 737, 1850, 4976, 842, 1297, 4495], [3966, 5652, 2591, 3691, 4170, 2591, 2825, 4353, 5642, 5255, 5637, 952, 5303, 4441, 724, 1403, 5667, 4626, 3379, 1811, 265, 597, 1313, 391, 38], [2832, 5261, 5042, 5240, 989, 2198, 0, 842, 1337, 2282, 5238, 3370, 4160, 2217, 5348, 4606], [693, 3234, 3488, 3503, 828, 1725, 5238, 3965, 4170, 3508, 4353, 4605, 24], [1135, 1180, 3966, 5652, 5637, 2924, 697, 1297, 275, 5637, 952, 4397, 5303, 4441, 5513, 1205, 5418, 563, 4588, 2217, 5348, 4626, 563, 3856, 721, 2217, 5348, 724, 549, 2410, 5240, 1939, 3803, 1297], [0, 5698, 3966, 5652, 5637, 4443, 0, 1017, 5248, 842, 2591, 3539, 711, 1850, 4976, 2241, 1059, 3888]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23284036')]\n",
      "[[5625, 2424, 4840, 2827, 3899, 5287, 2832, 2306, 3803, 4145, 2410, 5240, 4217, 3887, 2508, 2304, 32, 5667, 4840, 2572, 1812, 963, 3534, 1988, 4866, 292, 28, 842, 3534, 1988, 4866, 355, 24], [2705, 2188, 2410, 5261, 4455, 2446, 4395, 1337, 5384, 3054, 3133], [3966, 4442, 3713, 2998, 2410, 5153, 5667, 4743, 842, 3397, 2998, 2410, 5153, 5667, 4743], [4808, 2579, 3822, 5240, 5376, 3803, 1180, 4523, 2304, 4454, 3851, 5376, 3803, 0, 5153]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15714933')]\n",
      "[[5261, 4006, 4998, 2695, 888, 842, 1741, 2591, 4743, 842, 4951, 4360, 3803, 3217, 2866, 5637, 3744, 1036, 5376, 963, 3534, 733, 5376, 842, 2186, 3536, 5252], [5303, 1773, 5646, 3928, 3803, 1298, 5153, 2446, 100, 2581, 4851, 2428, 4495, 5614, 2111, 5303, 5238, 2446, 322, 2581, 2832, 100, 2431, 4495], [797, 5261, 5383, 3423, 5240, 4154, 1774, 1598, 2410, 2110, 1060, 5240, 5413, 3929, 4496], [3966, 5667, 0, 4300, 5151, 3286, 1180, 3741, 0, 5303, 1623, 5250, 5667, 4006, 4998, 5637, 4393, 5303, 4441, 2009, 322, 2581, 2832, 100, 2431, 3856, 100, 2581, 4851, 2428], [5240, 3391, 5122, 5614, 257, 5632, 2410, 4495, 842, 304, 5632, 2410, 4495, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('18386581')]\n",
      "[[5303, 3780, 5240, 1988, 3803, 5202, 5250, 2410, 4553, 5335, 5065, 842, 5439, 3421, 3822, 4113, 4377, 4360, 3803, 3217, 0, 2832, 460, 3966, 5667, 3286, 1180], [733, 5376, 2832, 5240, 5374, 2572, 5240, 1111, 5634, 5614, 2857, 2832, 119, 3966, 224, 4973, 2832, 304, 437, 842, 1688, 2832, 173, 311, 5649, 2832, 5240, 1533, 2572, 3069, 5614, 2857, 2832, 4973, 2832, 292, 434, 842, 1688, 2832, 257, 391, 1811, 1060, 5240, 5413, 2573, 5614, 4840, 5321, 77]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('18065731')]\n",
      "[[5261, 4480, 2832, 2701, 2369, 2442, 4022, 2410, 5489, 5303, 3536, 733, 5240, 1420, 3803, 5376], [5392, 668, 5614, 957, 5667, 2336, 2701, 2370, 3995, 1663, 5237, 4820, 963, 5632]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('22430268')]\n",
      "[[4395, 5602, 0, 1534, 5383, 5614, 1215, 3886, 2832, 0, 5675, 733, 5115, 1297, 842, 4386, 2410, 1135, 1180], [5303, 3885, 3117, 5261, 5042, 4302, 3746, 2188, 5238, 3374, 1176, 2641, 773, 3267, 5222, 2045, 842, 4048, 715, 1992, 3803, 3397, 5377, 2844, 2054, 5377]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('15514373')]\n",
      "[[5240, 3900, 4605, 4411, 5648, 5637, 1471, 1154, 814, 2864, 2246, 3934, 5637, 144, 597, 1313, 5303, 265, 300, 597, 1313, 156, 5303, 385, 842, 385, 597, 1313, 282, 5303, 454, 2410, 924, 842, 4597]]\n",
      "[np.int64(1)]\n",
      "[np.str_('10561203')]\n",
      "[[4603, 5637, 4785, 1403, 5667, 3719, 3966, 5667, 4749, 521, 3856, 3981, 842, 5667, 3966, 5667, 4749, 521, 842, 3966, 5667, 1337, 1051, 5637, 1403, 5667, 3966, 5667, 3981, 1154, 830, 3803, 1583, 5667, 695, 2410, 1018, 4745], [2474, 5384, 4828, 1028, 1756, 4900, 5238, 5240, 1590, 4146, 842, 4749, 521, 2573, 917, 3026, 4785], [1403, 3833, 5667, 3966, 5667, 4749, 521, 4603, 4831, 3691, 4840, 1811, 2832, 1018, 4349, 842, 5287, 5303, 5376, 2286, 0, 0, 1664], [5240, 1277, 1060, 1018, 842, 3536, 842, 4597, 2870, 2826, 2832, 4603, 1152, 2659, 0, 2832, 3966, 5667, 4749, 521], [5240, 1018, 1279, 5667, 5240, 2204, 3803, 4858, 3803, 3430, 5637, 1011, 808, 3966, 5667, 1590, 4146, 4749, 521, 842, 4262, 1850, 3981]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('18281662')]\n",
      "[[3828, 2717, 2344, 5270, 3966, 5637, 4397, 952, 5303, 4082, 546, 3856, 2503, 547], [2503, 1888, 3741, 4301, 814, 712, 1403, 5667, 4082, 2832, 5226, 3803, 5400, 2832, 3897, 1180, 3966, 5652, 2240, 4472, 5672, 130, 3536, 733, 4217, 5376, 1152, 4828, 1028, 1499, 2832, 5240, 0, 3803, 1928, 5303, 1028, 4112, 5509, 2832, 5240, 4708, 4803], [2559, 3856, 3667, 5614, 3539, 2443, 2832, 2503, 5374, 3966, 5562, 4082, 5374, 3966, 28]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16020979')]\n",
      "[[5240, 2838, 3803, 5652, 2559, 3856, 845, 3667, 842, 5273, 5614, 311, 411, 842, 257, 2500, 2572, 842, 304, 434, 842, 265, 2556, 2572, 4597, 543, 597, 842, 577, 4597], [3391, 5287, 5303, 1850, 4261, 341, 842, 385, 5632, 4597, 842, 3900, 5122, 391, 842, 471, 5632, 4597, 5637, 3803, 1118, 4839, 2832, 2306, 3803, 5240, 2556, 923, 81], [1152, 5240, 892, 2897, 1994, 3370, 0, 2471, 3028], [1152, 5240, 1811, 5614, 3741, 4995, 4840, 2500, 164, 2556, 300, 224]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('15713598')]\n",
      "[[4605, 4409, 5122, 842, 1939, 5637, 4843, 2832, 1121, 924], [5251, 5614, 3691, 4840, 1811, 1060, 5240, 5413, 4734, 2198, 2832, 5226, 3803, 2826, 2832, 3122, 3856, 4353, 1152, 5251, 4759, 5303, 1028, 5378, 2832, 2306, 3803, 5240, 383, 4732], [3828, 2717, 4807, 2426, 3966, 5637, 2074], [5261, 4395, 4039, 2768, 5042, 1403, 5413, 5376, 4734, 3803, 2505, 2832, 3966, 5667, 3698, 4888, 1253, 3286, 1180, 3755, 842, 2805, 3101, 4006, 4998, 3122]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15748464')]\n",
      "[[5261, 4395, 1901, 1096, 4068, 1534, 4059, 5383, 946, 5240, 2317, 3803, 0, 5240, 1992, 3803, 2103, 766, 3822, 1363, 2461, 842, 3537, 842, 2175, 3074, 1992, 3822, 2304, 842, 4353, 2832, 3966, 5667, 1135, 1180, 5374, 5667, 861, 1017, 697, 3856, 3646, 1297], [3379, 1275, 2832, 2227, 4745, 2446, 1018, 5303, 1634, 2832, 5240, 2103, 766, 2572, 5614], [5254, 1659, 5082, 5238, 2103, 766, 3370, 2614, 0, 5240, 1363, 2807, 842, 2304, 5238, 3791, 1940, 697, 1135, 1180, 1297], [3683, 2426, 3966, 5637, 2173, 2410, 1994, 842, 4701], [3966, 5637, 4395, 5303, 4441, 2103, 766, 385, 5706, 5047, 3823, 5631, 3856, 4068, 963, 5240, 1038, 3803, 1635, 3803, 1297, 698, 3899, 130, 5632]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('17039380')]\n",
      "[[3162, 3822, 3803, 2606, 4621, 2832, 4840, 2826, 3803, 1180, 3856, 1180, 5250, 957, 5153], [1152, 5240, 1406, 5614, 3230, 1935, 5303, 3317, 4295, 0, 1154, 5240, 2629, 5652, 5437, 0, 4998, 733, 5240, 2358, 4692, 842, 0, 5240, 5042], [5240, 1988, 3803, 5376, 5614, 3385, 5512, 5635, 1045, 4723, 5667, 5240, 1811, 3803, 5240, 994, 4743, 3803, 5240, 5635, 1045, 4723, 3822, 1663, 100, 1045, 1703, 932, 4217, 842, 5238, 3822, 1663, 932, 4755, 2060], [1960, 2572, 1501, 3803, 385, 3966, 4733, 5303, 4441, 5376, 2410, 3465, 5270, 5289, 5629], [4840, 2826, 2832, 5153, 733, 5376, 5614, 2424, 3822, 1663, 9, 842, 3822, 1663, 100, 5]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('19760046')]\n",
      "[[3966, 4442, 5345, 2325, 4736, 5486, 4590, 3803, 4122, 0, 5303, 4986, 3241, 1804], [4395, 1534, 5383, 5614, 4007, 5303, 944, 5240, 3887, 3803, 1963, 3857, 4122, 2325, 2089, 1403, 5667, 5345, 3857, 2325, 5309, 2832, 2584, 3827, 3966, 5447, 1421, 3141, 2844, 5492, 610, 5115], [1060, 3081, 234, 842, 3747, 183, 234, 5330, 3803, 158, 3966, 5637, 4395, 5303, 4441, 2009, 2089, 3856, 5309]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16476841')]\n",
      "[[842, 5240, 3379, 1689, 2832, 4353, 4745, 5637, 2567, 2410, 1147, 3966, 5667, 1018, 2618, 3209, 100, 1877, 1403, 5667, 5240, 3900, 1147, 2572], [2832, 3966, 5667, 1180, 4443, 4079, 1017, 1297, 842, 5667, 1018, 2618, 3209, 100, 1877, 1963, 2998, 5667, 2102, 4478, 5357, 0, 2618, 3208, 842, 0, 3856, 2828, 4353], [5261, 830, 3803, 5240, 4623, 3803, 4395, 1534, 5383, 2176, 5240, 1992, 3803, 2103, 766, 2102, 5250, 3822, 5356, 4574, 2649, 2618, 842, 4360, 3803, 3217, 4353, 2832, 3966, 5667, 1180, 4443, 4079, 1017, 1297, 5614, 1463, 5303, 2174, 5240, 1988, 3803, 2921, 2618, 3208, 3822, 5042, 3888]]\n",
      "[np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('23696477')]\n",
      "[[5240, 830, 2842, 419, 3966, 2832, 5240, 2119, 2572, 842, 417, 2832, 5240, 4983, 1205, 2572], [2067, 4466, 0, 2120, 2614, 1035, 4833, 5303, 4476, 3189, 3803, 2695, 5000, 0, 842, 1425, 2832, 1377, 5115], [3391, 0, 5287, 5614, 4477, 2832, 5240, 2119, 2572, 1664, 5562, 1664, 5667, 4983, 1205, 9, 932, 5614, 0, 1664, 5562, 1664, 9], [1121, 2573, 4442, 4985, 0, 5667, 2097, 819], [3966, 5374, 2832, 5240, 2119, 4464, 2300, 5637, 1841, 0, 842, 2591, 2336, 3397, 4523, 1425, 842, 2825, 4353], [4217, 2061, 5637, 5287, 5486, 3398, 2361, 2410, 1840, 0, 842, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('12963704')]\n",
      "[[5330, 3803, 0, 3966, 5667, 4214, 5487, 3433, 1377, 1180, 5637, 4397, 952, 5303, 4441, 1114, 2453, 401, 3445, 3303, 3015, 3289, 224, 3445, 3303, 3822, 1664, 5303, 842, 4556, 3822, 1663, 304, 2453, 3289, 3856, 2453, 0, 3445, 3303, 932, 272, 2702, 2910, 783, 2454, 3856, 2832, 1378, 5667, 438, 3445, 3303, 3289, 2454, 3289, 772, 2533, 5631, 0, 2404, 1154, 5629, 4609, 4012], [5261, 5383, 5614, 1463, 5303, 1773, 5646, 2662, 1896, 2387, 2453, 2533, 932, 5631, 272, 2702, 2910, 3054, 3539, 662, 5237, 1114, 2453, 3204, 3289, 842, 5646, 2662, 1896, 2911, 2453, 1176, 1028, 3518, 1154, 3289], [5667, 3391, 2403, 5489, 3803, 3539, 5237, 5693, 5122, 1802, 3741, 1809, 808, 5240, 5376, 2573, 3391, 2453, 3289, 119, 3536, 597, 1313, 100, 5303, 164, 3536, 2454, 144, 3536, 597, 1313, 100, 5303, 164, 3536, 2454, 3289, 144, 3536, 597, 1313, 130, 5303, 173, 3536, 534]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('18809612')]\n",
      "[[4755, 3887, 3388, 5637, 4769, 4565, 3803, 2977, 3803, 2701, 2370, 3822, 1656, 663], [2731, 896, 5303, 4476, 3996, 2701, 2370, 2832, 1135, 1180, 5126, 842, 3370, 2614, 679, 1053, 5077, 932, 4477, 888, 842, 1741, 842, 2825, 4873]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('23294853')]\n",
      "[[184, 3966, 2832, 5240, 5631, 2572, 842, 187, 2832, 5240, 5631, 2572, 5637, 2842, 2832, 5240, 830], [4392, 5614, 1893, 0, 842, 5025, 1154, 1261, 842, 5652, 4006, 4998, 4743, 3803, 5601], [1883, 698, 2186, 5632, 3054, 4983, 5376, 2410, 1223, 4589, 711, 4290, 1180], [100, 3445, 3857, 0, 5614, 698, 1656, 5303, 772, 3966]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('21538680')]\n",
      "[[3948, 5637, 946, 1036, 1180, 5376, 5165, 4120, 5167, 5648, 5614, 963, 3180, 3536, 733, 1180, 5376, 842, 733, 5692, 2403, 5489, 5168], [2832, 678, 5240, 3267, 5222, 1991, 5614, 3026], [4504, 830, 5667, 2968, 5614, 4007, 5303, 1773, 2759, 1891, 3803, 4360, 3803, 3217, 2090, 4351, 1156, 2465, 2631, 5120, 4824, 2412, 363, 3856, 4322, 1865, 5150, 1287, 586, 3508, 5240, 1988, 3803, 1239, 3822, 2304]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('18216523')]\n",
      "[[5240, 3310, 753, 5614, 5240, 1406, 3803, 1850, 2439, 5122, 2832, 5240, 924], [5240, 753, 3803, 5261, 3575, 4395, 5383, 5614, 5303, 944, 5240, 1994, 842, 4701, 3803, 4783, 3291, 3695, 4878, 1085, 1403, 5667, 1004, 3291, 3695, 1858, 782], [2705, 5240, 3759, 3803, 2074, 3966, 5614, 3741, 5081, 5303, 0, 1706, 1453], [4879, 5637, 2753, 2832, 0, 3803, 518, 3966, 597, 842, 4108, 4879, 5637, 2424, 2832, 198, 3803, 0, 3966, 304]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('22321518')]\n",
      "[[932, 3966, 5667, 3932, 842, 0, 1180, 4144, 2240, 2825, 5122, 4411, 842, 0, 5240, 2395, 0, 5333, 3249, 3217, 5649, 5125, 1180], [5512, 0, 0, 830, 0, 0, 5150, 0, 5637, 2753, 2832, 5240, 1364], [3828, 2717, 5413, 3966, 5667, 4580, 4144, 1489, 5303, 3949, 2832, 5261, 5042, 842, 5637, 4395, 5303, 2009, 814, 2998, 2572, 2766, 3856, 5513, 1205, 2572, 5420], [963, 5042, 1420, 3948, 2832, 5240, 2766, 5637, 5606, 5412, 932, 2295, 842, 5637, 4841, 3539, 3223, 5303, 2614, 1523, 5606, 3856, 855, 2412, 3803, 2217, 932, 1403, 5667, 5240, 5420]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('11250991')]\n",
      "[[5240, 5042, 5614, 1756, 932, 2281, 5383], [2441, 3803, 2645, 842, 3702, 5336, 5637, 2140, 4843], [5240, 753, 5614, 5303, 4689, 3886, 1687, 2832, 5240, 5692, 4261, 2439, 5122, 4031, 4409], [2675, 3354, 5536, 842, 1850, 2264, 917, 5635, 1011, 2832, 5240, 5376, 924, 3803, 5240, 5413, 1407]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('15625369')]\n",
      "[[5240, 3318, 3803, 3966, 4442, 5240, 2957, 2426, 1635, 2498, 497, 3450, 486], [2832, 3966, 5667, 711, 3755, 2498, 1297, 5614, 4833, 5303, 1028, 1059, 5316, 5376, 5238, 0, 5122, 712, 3899, 3450], [2028, 3966, 2591, 4214, 5487, 4976, 2773, 3856, 3077, 3755, 5089, 2410, 1321, 1017, 1297]]\n",
      "[np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('17661342')]\n",
      "[[2410, 5261, 4395, 1534, 5383, 483, 3966, 5667, 1180, 5637, 4467, 842, 4442, 2009, 5513, 1205, 3856, 5240, 2998], [5240, 1041, 3867, 2998, 0, 618, 4840, 2827, 2832, 4048, 2465, 2870, 5378, 5333, 2825, 1597, 1152, 1767, 3691, 1988, 2410, 2304, 4523, 1865]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('12488409')]\n",
      "[[3966, 5637, 4395, 5303, 2384, 282, 3445, 3015, 3822, 1664, 5303, 2186, 304, 1664, 3856, 1630, 1636, 544, 3445, 842, 5579, 3445, 1121, 3015, 3822, 1663, 842, 4166, 385, 3445, 3858, 3822, 1664, 5303, 2186, 246, 1664], [5303, 1402, 2832, 4039, 2771, 5042, 5240, 4701, 842, 1994, 3803, 2384, 5303, 5238, 3803, 1636, 5579, 842, 4166, 1630, 2832, 4474, 3274, 2559, 3698, 2684, 3298, 733, 4213, 4605, 5303, 5163, 5376]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('7786823')]\n",
      "[[3156, 794, 1297, 0, 2410, 2663, 4427, 3803, 1321, 706, 842, 2160, 5648, 4623, 2832, 4841, 3268, 3391, 5122, 3803, 3966, 5667, 2262, 1850], [5645, 3966, 5667, 1963, 794, 1297, 645, 1059, 5052, 695], [3966, 5667, 2262, 1850, 5124, 4841, 3268, 5667, 3156, 794, 1297, 5237, 3822, 5240, 1963, 795, 4495, 3391, 5122, 0, 1664, 5562, 324, 1664, 32], [3966, 5637, 4395, 5303, 4441, 2009, 0, 1963, 794, 1297, 3856, 0, 3156, 794, 1297], [4495, 1501, 3803, 3968, 1321, 706, 0, 173, 249, 842, 4495, 3803, 1637, 1636, 3438, 0, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('23211715')]\n",
      "[[1060, 3345, 234, 842, 914, 238, 0, 3803, 5240, 4071, 0, 3966, 5637, 4393, 543, 3876, 5623, 546, 3876], [4365, 5614, 0, 5667, 1121, 3272, 3803, 0, 842, 1963, 1350, 1935, 5303, 4103, 638], [3948, 1018, 1723, 2842, 3391, 737, 508, 5693, 2993, 4398, 490, 535, 483, 3324, 437, 5667, 3101, 4006, 4998, 526, 0, 5507, 5614, 4843, 2832, 5240, 5413, 2573, 497, 543, 570, 4442, 5623, 224, 2581, 2832, 2364, 2431], [3391, 5122, 5614, 3876, 5623, 434, 1664, 597, 1466, 2996, 376, 486, 3876, 442, 1664, 597, 1466, 2996, 300, 471, 2616, 4415, 119, 597, 1466, 2996, 556, 450, 2832, 2311, 3803, 5623], [5240, 4217, 3887, 3384, 3054, 4360, 693, 3217, 5693, 4348], [5255, 4301, 3108, 2906, 2410, 0, 5240, 5383, 5667, 3966, 842, 0, 5240, 0, 2410, 1525, 4365, 5303, 0, 857, 5261, 2821, 1337, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('24067488')]\n",
      "[[3885, 2353, 5082, 5238, 2410, 3544, 3966, 5261, 768, 1017, 1205, 1176, 1028, 2533, 1154, 5346, 3764], [5512, 1440, 2513, 4392, 4787, 5625, 4397, 777, 3966, 5303, 3828, 3803, 5270, 2573, 5025, 1154, 5405, 4858, 0, 2582, 3856, 2492, 842, 1709, 3803, 1126, 1948, 2746, 4743, 483, 5601, 483, 526, 5513, 1205, 1764, 4769, 2641, 1116, 2489, 3182, 768, 1017, 5376, 3856, 3764, 3182, 768, 1017, 5376], [3888, 2832, 5240, 3764, 2572, 5637, 3741, 2897, 5303, 3888, 2832, 5240, 2489, 2572, 3379, 1811, 363, 3828, 4836, 597, 1313, 427], [1312, 2492, 5153, 733, 3991, 4386, 917, 1389, 3581, 2832, 1233, 842, 725, 3966, 4360, 3803, 3217], [5240, 4217, 2060, 5614, 1275, 2832, 2901, 1126, 1850, 4369, 1126, 5064, 4743, 2746, 963, 3536, 827, 1154, 2965, 5303, 5372], [2410, 5261, 5270, 923, 4393, 1534, 5383, 5625, 4467, 3966, 738, 191, 5693, 2446, 1343, 2832, 3266, 5424, 5667, 3672, 3834, 2492, 5153, 0, 3536, 733, 3991, 4386]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('17324657')]\n",
      "[[4556, 3388, 830, 3803, 5540, 1893, 3822, 814, 2965, 5303, 5372, 1020], [5240, 3768, 2998, 2572, 2591, 2663, 3379, 5330, 2065, 57, 842, 4294, 2953, 9, 1403, 5667, 5240, 4983, 4148, 2572]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('22877848')]\n",
      "[[932, 2237, 5240, 3544, 1389, 2559, 715, 2183, 2832, 5240, 5214, 2572, 5637, 3667, 130, 842, 5273, 191], [1121, 5214, 842, 2733, 4386, 4828, 1028, 1499, 932, 4983, 5376, 3854, 2832, 2012, 3966, 5667, 2538], [2832, 5240, 5270, 2572, 4392, 2832, 1406, 5667, 4983, 4386, 3391, 3900, 5122, 5614, 4841, 3268, 5667, 5214, 3536, 597, 1313, 593, 5601, 3536, 597, 1313, 101, 2616, 4415, 2708, 526, 597, 1313, 446, 593, 32, 1152, 3741, 5667, 2733, 4386, 3536, 602, 2708, 570, 497, 130, 272], [3966, 5374, 5667, 5214, 5652, 2591, 5405, 3446, 4273, 3440, 2591, 4841, 3268, 5122, 5237, 5264, 5673, 3446, 4273, 3440, 3536, 597, 1313, 119, 5601, 3536, 2708, 467, 597, 1313, 348, 593, 45, 1152, 3691, 1811, 5614, 3744, 1060, 5264, 5667, 0, 842, 0, 3446, 4273, 5374, 5667, 4386, 2708, 600, 597, 1313, 515, 374, 559], [4983, 4386, 5614, 957, 5667, 4103, 3888, 2139, 2832, 3966, 3816, 5237, 526, 5693]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('9166485')]\n",
      "[[5240, 3379, 2644, 2857, 1154, 3998, 4096, 2832, 5240, 2715, 2572, 5562, 2832, 5240, 4068, 2572], [3828, 2717, 5260, 5413, 846, 1180, 3966, 4443, 0, 1321, 1515, 3608, 1297, 5637, 2175]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('12395333')]\n",
      "[[5251, 5614, 4841, 2663, 4662, 3803, 1672, 2832, 5240, 5298, 2572, 1403, 5667, 5240, 2572, 2616, 4415, 376, 597, 1466, 2996, 89, 559], [814, 2864, 1659, 3528, 1388, 4647, 772, 942, 3803, 5240, 5383], [842, 3966, 5637, 3273, 5303, 2403, 5489]]\n",
      "[np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('12637459')]\n",
      "[[797, 2320, 3667, 842, 2894, 5637, 3539, 2443, 2410, 3966, 5180, 963, 4597, 341, 100, 9, 32], [963, 4841, 2828, 5400, 842, 3873, 1403, 5667, 625, 2832, 3966, 5667, 3373]]\n",
      "[np.int64(1), np.int64(1)]\n",
      "[np.str_('17878480')]\n",
      "[[5251, 5637, 2336, 3966, 5667, 4006, 4998, 3803, 119, 224, 842, 2336, 2591, 1590, 5303, 2921, 5250, 2832, 5240, 5537, 923], [5537, 2284, 5303, 1724, 1994, 932, 3316, 5250, 2410, 4888, 1253, 3286, 1180], [3391, 4261, 2439, 5122, 2410, 5537, 842, 4068, 5614, 842, 3536, 4597, 2616, 4415, 2708, 32, 556, 1313, 543, 5303, 363, 3828, 4836, 442]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23224689')]\n",
      "[[5261, 0, 1097, 5042, 4395, 0, 2682, 3966, 4733, 2410, 1610, 5270, 5631, 1321, 4687, 505, 0, 2581, 4800, 2364, 2431, 5629, 2410, 5632, 5330, 341, 2431, 3005, 3152, 120, 842, 4068, 120, 2573], [2651, 5625, 1802, 5261, 5042, 5303, 2174, 5240, 1992, 3803, 3251, 3822, 3965, 4563, 3388, 3803, 3817, 842, 4353, 2832, 2627, 842, 3635, 1180, 2682, 3966, 4443, 1610], [5251, 3054, 0, 3803, 2188, 4493, 5240, 1992, 3803, 3251, 3822, 3965, 5052, 2240, 3803, 3817, 842, 4353], [0, 0, 110, 0, 359, 0], [3436, 5614, 4843, 5303, 3885, 4445, 4334, 5042, 0, 2155, 760]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('20033227')]\n",
      "[[5625, 2424, 3691, 1811, 2832, 2403, 5489, 1570], [5240, 2861, 1569, 1991, 4415, 3803, 2535, 1403, 5303, 5004, 4069, 5614, 0, 3995, 2266, 1663, 5667, 2486, 3889, 3783, 4746, 5160, 0, 3856, 693, 2410, 5122], [797, 2535, 2591, 2663, 5330, 1570], [2832, 4444, 4395, 5383, 5004, 4069, 5614, 4172, 2832, 3966, 5667, 4528, 4824, 5122, 842, 2535, 2832, 3966, 5667, 3268, 5122], [2408, 2953, 2825, 3539, 4405, 733, 5004, 4069, 5237, 733, 2535], [932, 2631, 1974, 942, 2614, 3833, 1035, 5040, 2832, 2508, 5226, 5625, 2148, 5240, 1569, 3803, 2535, 842, 5238, 3803, 5004, 4069, 2832, 5077, 3966], [3933, 1180, 5614, 5240, 3544, 1389, 1233, 3803, 2552]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('23045575')]\n",
      "[[5240, 4337, 3803, 3885, 5042, 5614, 5303, 2174, 5240, 1988, 3803, 1363, 1040, 5250, 1239, 4048, 2217, 3984, 842, 3803, 5254, 5413, 3000, 1380, 1239, 3984, 3822, 3415, 5153, 4217, 3887, 1111, 2782, 4812, 2465, 4322, 5635, 1045, 842, 2631, 4523, 4360, 3803, 3217, 4755, 3888, 2832, 3966, 5667, 1135, 1180, 2243, 5376, 2886, 3416], [4769, 4562, 4370, 5637, 1416, 963, 1018, 130, 5632, 842, 3536], [3544, 3803, 5254, 1992, 5637, 3781, 963, 1121, 5240, 130, 5629, 842, 3534, 2403, 0], [0, 4237, 5637, 5509, 5303, 1402, 5240, 2998, 2573, 5667, 5240, 1533, 2572, 3899, 5287]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23657969')]\n",
      "[[5261, 4395, 1534, 5383, 4641, 5238, 5395, 4255, 896, 5303, 1028, 2318, 842, 629, 2998, 842, 3370, 1028, 957, 5667, 4824, 5222, 2827, 2832, 3773, 842, 5052, 1363, 2461, 932, 5635, 932, 3419, 2631, 842, 4949, 2832, 1135, 1180, 3966], [4769, 4562, 3388, 3803, 1363, 2461, 2462, 948, 3803, 1180, 5250, 2279, 0, 4360, 3803, 3217, 4815, 363, 1742, 5153, 1257, 2410, 2093, 5041, 1741, 4723, 4873, 1872, 4062, 4873, 4360, 2866, 2304, 1140, 2304, 3023, 842, 4949, 2279, 0, 5637, 1416, 963, 1018, 5240, 2051, 3803, 5376, 842, 3534, 3158]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('19691092')]\n",
      "[[4444, 4395, 5383, 5303, 1402, 2265, 1029, 4377, 5250, 1970, 5303, 1611, 2410, 3257, 1850, 4831, 1611, 5303, 1028, 0, 5303, 2265, 1029, 1970, 2832, 1850, 4261, 842, 3900, 842, 1850, 4936, 5122], [3414, 5652, 0, 5303, 2856, 5241, 3796, 3803, 4628, 4812, 2461, 3460, 1028, 0, 5303, 1310, 1970, 3899, 1611], [3691, 3156, 0, 4353, 3068, 5637, 3781]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23084254')]\n",
      "[[1597, 5614, 4841, 4477, 2832, 5240, 2998, 2572, 1563, 5303, 2304, 4743, 4480, 3803, 67, 1988, 4866, 3803, 411, 597, 1313, 183, 532, 13, 5240, 2279, 814, 4743, 1154, 385, 0, 5240, 2279, 814, 5311, 4743, 1154, 257, 31, 842, 5240, 815, 0, 1154, 546, 13], [2335, 4425, 2217, 2998, 5041, 2614, 2842, 1180, 3966, 5447, 1297]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('12351592')]\n",
      "[[3828, 2717, 5260, 4805, 2028, 3966, 5637, 4395, 5303, 5376, 5667, 2009, 880, 3278, 515, 3966, 3856, 4068, 3278, 511, 3966], [2705, 2832, 5261, 2572, 3803, 3966, 5374, 5667, 1540, 4386, 5240, 3278, 1802, 3741, 2802, 4841, 3822, 5240, 4810, 3803, 3572], [5251, 5637, 3691, 4995, 4840, 1812, 1060, 5240, 924, 2832, 5240, 2264, 3803, 4809, 3572, 932, 3385, 1154, 4049, 2832, 3857, 5336, 932, 4461, 1154, 3966, 3856, 2832, 4386, 1715], [5240, 4217, 2051, 4095, 3803, 5240, 5042, 5614, 5240, 5287, 5303, 1779, 3803, 4809, 3572, 2446, 5240, 4986, 3803, 4386]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0)]\n",
      "[np.str_('17195085')]\n",
      "[[5330, 3803, 181, 3966, 5667, 4469, 1180, 5637, 4397, 952, 5303, 3140, 566, 3856, 3837, 570, 4581], [4824, 5222, 4122, 3538, 5614, 4843, 2832, 5240, 5413, 2573], [5240, 3379, 2403, 5489, 4012, 5614, 450, 3536], [3887, 3942, 5637, 4122, 3538, 3189, 3803, 2695, 5000, 4360, 3803, 3217, 3267, 5222, 5122, 842, 3255, 4473], [3255, 4472, 4409, 842, 2364, 5692, 5122, 5637, 4843, 2832, 1121, 2573]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23569311')]\n",
      "[[3544, 3966, 2591, 5583, 3430, 404, 2591, 1850, 2439, 2996, 3197, 5237, 130, 3536, 842, 570, 2591, 3383, 1850], [1382, 1061, 5667, 1883, 842, 5370, 1802, 3741, 4841, 2824, 3032, 946, 4031], [5240, 0, 5383, 5042, 3803, 0, 1061, 2832, 1378, 5667, 0, 5370, 1883, 2832, 3966, 5667, 2654, 4108, 3433, 1135, 1180, 2175, 2358, 3233, 1061, 1515, 5250, 2410, 2716, 2095, 2574, 2280, 4446, 2654, 4108, 3258, 4474, 3433, 1135, 1180, 3279, 3373], [3966, 5667, 3383, 2173, 2654, 4108, 3279, 3373, 5652, 2591, 3741, 4442, 5370, 3856, 1297, 2410, 3279, 3373, 5637, 5025, 1154, 4224, 697, 5370, 4224, 3645, 697, 5196, 2692, 4446, 4998, 842, 3383, 1850, 842, 5637, 4397, 952, 5303, 4441, 1883, 101, 3445, 4087, 5370, 3445, 3109, 3253, 1896, 2404, 1154, 3445, 3109, 2009, 5667, 1061, 164, 3445, 3109, 3856, 5673, 1061, 772, 698, 2186, 5632], [5240, 4681, 5614, 526, 5562, 539, 4597, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16802254')]\n",
      "[[5625, 3781, 3691, 4840, 1277, 2832, 5240, 1533, 2572], [5240, 4337, 3803, 5261, 4059, 5042, 5614, 5303, 2197, 5240, 1992, 3803, 1380, 1203, 842, 4588, 2217, 5348, 4255, 3803, 4824, 1939, 3822, 5240, 1203, 2362, 5028, 2063, 5192, 4936, 2462, 3590, 1189, 1111, 1430, 842, 4360, 3803, 3217, 4353, 2832, 5675, 1135, 1180, 5126]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('18187401')]\n",
      "[[1121, 5417, 842, 2743, 2825, 2711], [4716, 5614, 834, 5667, 5240, 2360, 2194, 5230]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('12610188')]\n",
      "[[1154, 5240, 1599, 3803, 814, 556, 4290, 4936, 877, 4480, 3314, 2410, 963, 3180, 5632, 119, 322, 3803, 369, 3966, 2832, 5240, 5210, 923, 4600, 5645, 119, 333, 3803, 348, 952, 5303, 3098, 5551, 4600], [3966, 5637, 4288, 5025, 3822, 5240, 1020, 3803, 1850, 5598], [5240, 4217, 2051, 4096, 5637, 4605, 842, 3900, 5122, 5287], [3344, 3966, 272, 2284, 5303, 1415, 963, 3180, 5632, 3803, 5250, 2844, 2364, 5376, 4523, 1963, 1673], [4808, 3577, 4496, 2614, 1035, 4563, 5303, 1028, 5510, 2832, 711, 843, 2864, 4290, 1180]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('22868247')]\n",
      "[[5240, 5042, 5614, 1756, 5303, 1346, 2759, 437, 3803, 3966, 3638, 5303, 4610, 708, 2832, 5240, 2984, 923], [2759, 5240, 4795, 5233, 0, 619, 1221, 3209, 543, 0, 3214, 5637, 4515], [3966, 5652, 1777, 1223, 4589, 4290, 1180, 1608, 5416, 1522, 3822, 843, 1743, 5250, 708], [3966, 5637, 2404, 5667, 1337, 949, 0, 3030, 842, 4353, 4370, 2090, 4351, 1156, 3856, 0, 2186, 3536], [3900, 5122, 2631, 4523, 4360, 3803, 3217, 4353, 842, 1569, 5637, 5240, 3310, 2061], [5251, 5614, 3691, 1811, 2832, 3900, 3856, 1180, 4936, 5122, 1060, 5240, 924]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('16319148')]\n",
      "[[3803, 5240, 282, 3966, 2832, 5654, 3534, 0, 2785, 2403, 5489, 5614, 1416, 5264, 5374, 5667, 5173, 5637, 4841, 3539, 3223, 5303, 2614, 1415, 2892, 3803, 772, 3187, 4862, 3966, 5601, 3828, 3965, 45, 5637, 3539, 3223, 5303, 2614, 963, 3180, 586, 5403, 2892, 2006, 3966, 5601, 2426, 3966, 58, 842, 2591, 3275, 3379, 3997, 3803, 4585, 0, 2340, 5403, 5291, 5601, 411, 23, 1403, 5667, 3966, 5374, 5667, 4340], [3966, 5447, 5417, 5637, 4397, 952, 5303, 4441, 5174, 3856, 4340], [3887, 1659, 5637, 1370, 963, 3536, 733, 2035, 2844, 948, 3803, 1337, 5153, 4745, 2446, 2340, 5403, 4936, 5150, 842, 4360, 3803, 3217, 4353, 4369, 842, 2353, 3822, 1530, 3366, 2067, 3307, 4592, 0, 2785, 2844, 5240, 1709, 3803, 5403, 2892, 842, 5598, 4480]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('15880316')]\n",
      "[[5251, 5614, 3691, 1811, 2832, 4466, 3803, 2408, 2953, 808, 5240, 2573], [3966, 4395, 5303, 3857, 3768, 3833, 0, 932, 1533, 5054], [2842, 3966, 556, 4442, 5105, 2081, 3856, 3943, 3768, 4123, 963, 2685, 1563, 5303, 102, 3103, 5486, 5240, 3966, 1802, 3741, 0, 5303, 1522, 5667, 931, 3768, 2410, 889, 4438]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('16166415')]\n",
      "[[2117, 5614, 5096, 5303, 4068, 2410, 5122, 4261, 2439, 5122, 842, 5403, 4605, 4409], [3822, 3747, 191, 231, 5240, 2315, 0, 2117, 4506, 912, 2410, 5376, 3803, 3966, 5667, 3258, 711, 3856, 3433, 3755, 733, 2286, 3803, 963, 3180, 3828, 4224, 1297, 4495]]\n",
      "[np.int64(0), np.int64(0)]\n",
      "[np.str_('17522936')]\n",
      "[[5251, 5637, 3691, 1812, 2832, 2462, 3887, 3538, 3856, 4353, 1060, 3281, 842, 3872], [5261, 5042, 3054, 5240, 2358, 5303, 4830, 5238, 3872, 2613, 3641, 2802, 3822, 1111, 2782, 842, 1567, 932, 1403, 5667, 3281], [2462, 3887, 4353, 842, 3538, 917, 4843, 2410, 5240, 5413, 908], [3069, 3054, 2705, 5443, 5646, 5240, 4866, 842, 3259, 3803, 0, 725, 1111, 2782, 842, 4353], [1111, 2782, 4369, 5614, 5509, 5303, 2174, 1111, 2782, 842, 1567], [5330, 3803, 450, 3966, 1416, 5240, 4353, 842, 2462, 3887, 4370], [5240, 2331, 3966, 2832, 5240, 3872, 2572, 2591, 4841, 3275, 1111, 2782, 4745, 5237, 5240, 3324, 3966, 156, 5601, 191]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('23333117')]\n",
      "[[1060, 1676, 191, 233, 842, 0, 173, 236, 733, 5116, 4236, 0, 5303, 0, 5240, 1850, 5675, 5667, 2986, 2323, 3803, 2586, 842, 3782, 0, 2662, 4662, 4976, 3077, 2101, 3897, 1180, 5637, 4397, 777, 1154, 1440, 4255, 842, 0, 4392, 5303, 4441, 2009, 4862, 1635, 3803, 4983, 1297, 5330, 191, 5632, 5667, 1195, 918, 5445, 5240, 1626, 3856, 842, 3917, 186, 3445, 783, 3856, 5667, 1061, 3445, 3109, 2533, 3015, 5667, 1297, 842, 1523, 932, 4851, 1927, 5252, 5330, 454, 5632], [963, 5629, 454, 0, 505, 5675, 2832, 5240, 1061, 2572, 842, 0, 442, 5675, 2832, 5240, 4983, 1297, 2572, 4302, 4353, 1659], [0, 5675, 5637, 4397, 952, 5303, 5240, 4983, 1297, 2572, 842, 0, 5303, 5240, 1061, 2572], [2832, 4174, 830, 3803, 2662, 4662, 5064, 3803, 3966, 5251, 5614, 789, 814, 2826, 2832, 3900, 5122], [5240, 5344, 3804, 1060, 5240, 4268, 3803, 4261, 2439, 5122, 842, 5240, 4360, 3803, 5238, 4012, 3803, 5287, 3640, 5303, 1028, 1499, 2832, 1337, 4148, 5642, 3321, 5376, 0], [2832, 5240, 2584, 1180, 2979, 2986, 0, 3822, 3897, 0, 2748, 5383, 1061, 2825, 4261, 2439, 5122, 2832, 3966, 5667, 3897, 1180, 5642, 5509, 2832, 1378, 5667, 2358, 3233, 1297, 842, 932, 4851, 1927, 1521, 5376, 2410, 191, 1635], [5240, 2748, 5383, 2613, 1416, 4468, 842, 4544, 2832, 2403, 5489]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('20855843')]\n",
      "[[5625, 4562, 5303, 3885, 3117, 5240, 2358, 4395, 5383, 3803, 0, 0, 842, 5579, 3980, 5562, 5300, 2832, 1297, 3617, 3966, 5667, 4474, 0], [1152, 5300, 2825, 3900, 4031, 2708, 374, 597, 1313, 77, 5303, 563, 49, 5122, 2708, 333, 597, 1313, 604, 5303, 543, 82, 842, 2541, 4360, 3803, 3217, 434, 199, 2825, 100, 4096, 963, 3536, 4597, 24], [5667, 3391, 2403, 5489, 5287, 3803, 130, 3536, 842, 375, 1673, 5251, 5614, 3691, 1330, 5122, 1051, 5642, 1405, 3980, 5667, 5300, 2616, 4415, 2708, 589, 597, 1313, 539, 5303, 119, 356], [5214, 5300, 3054, 814, 0, 740, 0, 2410, 5376, 3803, 2662, 2559, 2539, 0], [3317, 5337, 5614, 4843, 654, 772, 5270, 2573]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('15542159')]\n",
      "[[622, 1275, 2832, 923, 5598, 963, 130, 3536, 5614, 597, 1313, 385, 5303, 2832, 5240, 5376, 2572, 1403, 5303, 597, 1313, 5303, 2832, 5240, 4068, 2572], [772, 0, 5637, 2533, 1877, 786, 5307, 643, 438, 3445, 5412, 1663, 3858, 4087, 3993, 386, 3445, 5412, 1663, 3858, 3856, 1563, 0, 2410, 3536], [4865, 2006, 2028, 4577, 0, 5667, 3474, 224, 2856, 2832, 923, 5598, 963, 3391, 164, 5693, 4398, 391, 733, 1004, 5109, 4386, 4087, 1004, 5115, 2832, 442, 511, 543, 1219, 5637, 4393, 5303, 662, 1928, 3856, 4068]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('16344321')]\n",
      "[[5330, 3803, 0, 3966, 0, 5303, 5147, 1002, 271, 5303, 5147, 5637, 4397, 952], [1152, 5240, 1812, 5220, 5303, 0, 2832, 5303, 130, 3536]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('11821453')]\n",
      "[[4605, 4411, 2832, 121, 945, 3966, 5667, 842, 2654, 3903, 1154, 2798, 2767, 5637, 355, 597, 1313, 272, 5303, 411, 842, 3700, 597, 1313, 5303, 164, 4597], [5240, 1337, 1051, 4411, 2832, 945, 3966, 5667, 842, 2654, 3903, 5637, 427, 842, 4597], [5240, 4605, 4411, 2832, 116, 945, 3966, 5667, 842, 5673, 2654, 2507, 813, 1154, 0, 2832, 4860, 0, 2359, 830, 5637, 348, 597, 1313, 265, 5303, 417, 842, 597, 1313, 5303, 257, 4597]]\n",
      "[np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('16877734')]\n",
      "[[4605, 4411, 5637, 789, 4841, 2825, 5667, 1883, 5562, 5581, 257, 0], [1883, 3532, 3370, 1028, 1499, 932, 814, 3853, 2832, 5240, 4983, 5376, 3803, 2012, 3966, 5667, 711, 3755], [5240, 3544, 1389, 2559, 5303, 5336, 5637, 3667, 563, 2410, 1883, 515, 2410, 5581, 0, 842, 3207, 475, 2410, 1883, 442, 2410, 5581], [3966, 4397, 4442, 1883, 483, 3445, 3303, 1663, 3856, 5581, 282, 3445, 3303, 1664, 842, 2186, 246, 1664, 2410, 2426, 1635], [1883, 2825, 3900, 1850, 4523, 5153, 3899, 5581, 3796, 4415, 574, 597, 1313, 97, 5303, 224], [1297, 3626, 3966, 737, 526, 5693, 3856, 3816, 5667, 4976, 2773, 3077, 3755, 842, 4006, 4998, 3856, 3275, 5637, 2028], [2832, 5330, 194, 3966, 5637, 2074]]\n",
      "[np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('21444867')]\n",
      "[[5240, 164, 5692, 0, 3261, 4472, 4411, 5637, 2410, 1970, 5562, 164, 2410, 3621, 9, 842, 164, 5692, 3900, 5122, 5614, 446, 5562, 483, 156], [830, 5614, 1154, 2965, 5303, 5372], [3391, 2403, 5489, 5614, 144, 5693], [2710, 5614, 2175, 5667, 5240, 4824, 2412, 363, 3071, 4815, 363, 4369, 5061, 2446, 5240, 2167, 3862, 2410, 4577, 842, 5376, 3803, 1180, 2090, 4147, 3520, 2410, 1126, 842, 1091, 5153, 842, 5240, 0, 842, 0, 3521, 2410, 4812, 5153, 842, 1722, 4371]]\n",
      "[np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('12569144')]\n",
      "[[1152, 3069, 5614, 3741, 4995, 4840, 0, 5562, 0, 1811, 0, 597, 1313, 3822, 5240, 1811, 604, 5303, 0, 84], [1405, 5240, 4851, 842, 3585, 2428, 4386, 4734, 3691, 1812, 5637, 2424, 2832, 3217, 2235, 404, 5562, 385, 5632, 224, 3856, 4360, 693, 3217, 2235, 183, 5562, 173, 5632, 246], [4994, 5234, 5637, 5413, 4836], [797, 4851, 842, 3585, 2428, 4386, 917, 5266, 5303, 4301, 2105, 3928, 5648, 5376, 4732, 4305, 1059, 5535, 2410, 5240, 0, 3054, 5472]]\n",
      "[np.int64(1), np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('19608997')]\n",
      "[[3900, 4353, 4745, 5637, 4843, 2832, 5240, 5413, 5376, 2573], [772, 3966, 4442, 2160, 842, 1195, 2186, 5632, 2410, 5489, 5303, 4862, 1635], [5236, 2832, 1378, 5667, 1297, 1802, 3741, 2824, 5122, 3803, 3966, 5667, 4741, 1152, 5614, 957, 5667, 814, 2857, 4662, 3803, 5276, 2183], [2616, 4419, 2712, 2410, 1405, 5236, 736, 4068, 5637, 2148, 5512, 1587, 4504, 3504]]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('23910712')]\n",
      "[[2061, 2842, 0, 0, 5150, 3023, 2627, 842, 3635, 0, 2681, 842, 2462, 948, 3803, 1180, 5250, 2627, 842, 3635, 2279, 0, 4745, 0, 5667, 1337, 948, 842, 1880, 1445, 3803, 3848], [797, 5240, 4068, 2572, 4831, 4840, 2856, 2832, 1880, 1445, 1940, 5240, 1927, 5383, 72], [797, 3966, 2832, 5240, 4653, 2572, 4831, 2567, 1682, 2832, 1121, 4745, 1940, 5240, 3536, 4131], [3885, 4174, 1659, 5082, 5238, 4653, 1802, 3741, 4841, 2824, 5042, 4217, 2061, 3803, 4353, 3388, 1403, 5667, 5240, 4068, 2572], [5251, 5614, 3691, 1812, 2832, 670, 5336, 1060, 5240, 2573], [2471, 3028, 2832, 3147, 3760, 3803, 3966, 3054, 5612]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(0), np.int64(0)]\n",
      "[np.str_('16179098')]\n",
      "[[3391, 737, 5614, 532, 5693, 4398, 526, 563, 5693], [5240, 842, 5692, 5122, 4411, 5637, 282, 842, 4597], [1994, 3054, 3274, 1152, 4843, 5303, 5238, 3803, 3881, 5377, 5509, 2832, 5261, 4803]]\n",
      "[np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('15338385')]\n",
      "[[963, 5240, 1038, 842, 5240, 2051, 3803, 5240, 5042, 5625, 2175, 4048, 1363, 842, 2045, 4998, 842, 4914, 0, 5667, 5240, 2167, 3863, 2410, 4577, 842, 5376, 3803, 1180, 4360, 3803, 3217, 4369, 1555, 3520, 2090, 4351, 322, 4369, 842, 0, 4048, 4006, 5667, 814, 0, 5030, 5230], [4048, 4006, 3803, 5240, 5348, 2572, 2825, 4841, 1940, 5240, 4256, 224, 0, 32, 1152, 4542, 5442, 2832, 5240, 4529, 2572, 156, 0, 369]]\n",
      "[np.int64(0), np.int64(1)]\n",
      "[np.str_('19704057')]\n",
      "[[4805, 842, 3681, 1673, 5637, 3781, 2832, 5240, 3795, 3144, 842, 4068, 2573, 4597], [5261, 4562, 3054, 1017, 3822, 508, 5403, 0, 842, 173, 3781, 1673, 2832, 570, 3966, 963, 5240, 5287, 3803, 5240, 4071, 2980, 830]]\n",
      "[np.int64(1), np.int64(0)]\n",
      "[np.str_('16446322')]\n",
      "[[949, 5637, 1463, 963, 1018, 2791, 1036, 2186, 5376, 1634, 963, 5240, 2051, 3803, 5376, 842, 2186, 4862, 5632, 2410, 130, 3536], [5413, 2717, 2344, 3966, 5637, 4397, 952, 556, 5637, 3324, 5667, 3391, 737, 3803, 475, 5693, 5652, 4006, 4998, 842, 2832, 282, 490, 842, 144, 3803, 1219], [5261, 1988, 3054, 814, 2821, 1338, 3381, 4480, 2446, 1018, 2832, 5240, 1321, 4389, 923], [963, 3691, 4095, 5614, 889, 4840, 1811, 892, 3822, 5261, 2051, 4095], [2711, 5614, 946, 5667, 5240, 2167, 3862, 2410, 4577, 842, 5376, 3803, 1180, 1555, 4360, 3803, 3217, 4369, 1156, 2090, 4351, 1156, 842, 2090, 3286, 1180, 3520, 4351, 3165, 5324], [5240, 2541, 2711, 4723, 5614, 1398, 963, 1018, 3822, 1121, 5376, 924, 0]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('22705939')]\n",
      "[[3816, 1135, 1180, 5126, 1026, 917, 963, 4662, 2410, 3156, 842, 3267, 5222, 5376, 1992, 3822, 4360, 3803, 3217, 4353, 2844, 3275, 4048, 2465, 842, 2316, 3803, 4472], [5240, 3494, 896, 5303, 1051, 3816, 1026, 1154, 4479, 2316, 3803, 4472, 842, 2829, 3468, 974], [5413, 4271, 908, 5303, 681, 5261, 2841, 0, 3550, 5250, 842, 3468]]\n",
      "[np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('22340282')]\n",
      "[[1152, 3885, 4755, 3887, 5086, 5238, 5413, 4801, 3803, 761, 3983, 3054, 5096, 5303, 2789, 1595, 2410, 5240, 5376, 3803, 758], [4865, 3828, 3966, 5637, 2074, 2446, 5240, 0, 0, 0, 0, 0, 2695, 442, 1416, 5240, 5042, 842, 5637, 2842, 2832, 5240, 830], [5251, 5614, 3691, 4995, 4840, 1811, 2832, 5376, 4605, 5642, 5240, 101, 3856, 543, 1331, 4409, 0, 5614, 5509]]\n",
      "[np.int64(1), np.int64(0), np.int64(0)]\n",
      "[np.str_('24123482')]\n",
      "[[2832, 678, 5303, 4048, 664, 2474, 3000, 4828, 5189, 4769, 1994, 842, 3363], [0, 3390, 3803, 130, 5629, 2572, 1017, 2217, 2998, 3822, 1180, 5126, 4360, 3803, 3217, 4353, 5637, 2198, 5303, 2905, 2474, 2217, 2998, 1779], [5240, 2998, 5614, 957, 5667, 2857, 4048, 664, 419, 597, 1466, 2996, 1313, 156, 477, 2508, 4769, 1994, 391, 597, 1313, 355, 535, 842, 3363, 543, 597, 1313, 363, 549], [2471, 2857, 4048, 664, 5614, 1833, 957, 5667, 2825, 4353, 369, 597, 1313, 32, 454]]\n",
      "[np.int64(0), np.int64(0), np.int64(1), np.int64(1)]\n",
      "[np.str_('22523181')]\n",
      "[[2759, 4561, 2832, 3147, 842, 3539, 2396, 5384, 721, 2217, 3370, 1028, 814, 975, 3853, 5303, 3331, 4873, 1948, 2832, 3966, 5667, 1180, 1031, 3803, 3074, 2307, 4701, 4248, 842, 3881, 1887, 2631, 1053], [2705, 1338, 0, 5050, 895, 5303, 1051], [3966, 5667, 3298, 2240, 4873, 4235, 5238, 3370, 1028, 3333, 5667, 721, 2217, 1152, 3691, 4213, 5042, 2613, 2198, 5261, 3067], [2965, 5303, 5372, 828, 2870, 5238, 724, 4621, 2832, 3722, 173, 2826, 2832, 2541, 4873, 4360, 1403, 5667, 5513, 1205, 3379, 2572, 1811, 497, 597, 1466, 2996, 1313, 467, 5303, 300]]\n",
      "[np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
      "[np.str_('16965866')]\n",
      "[[772, 3966, 5637, 2404, 2410, 3391, 3803, 427, 116, 3536], [2492, 842, 2576, 5337, 4022, 5693, 733, 4687, 842, 1802, 3741, 1809, 1060, 5240, 5413, 1896, 4734, 3881, 5237, 2832, 4491, 5303, 0, 3803, 1695], [5240, 5692, 1074, 1337, 4520, 2439, 842, 3900, 5122, 5614, 460, 842, 570, 4597, 2410, 772, 3966, 842, 1802, 3741, 1809, 1060, 5240, 5413, 4734], [5330, 3803, 546, 1074, 4522, 5667, 3856, 5673, 1337, 4522, 2614, 3791, 4850, 3803, 5254, 369, 5637, 2832, 5240, 2733, 842, 376, 2832, 5240, 1540, 4732]]\n",
      "[np.int64(0), np.int64(1), np.int64(1), np.int64(1)]\n",
      "[np.str_('19286422')]\n",
      "[[843, 5376, 2410, 4290, 1180, 1176, 716, 725, 2462, 1891, 3803, 4360, 3803, 3217], [369, 119, 3803, 356, 2832, 5240, 1380, 2572, 842, 265, 3803, 355, 2832, 5240, 2054, 3833, 2572, 2591, 3900, 1122, 2446, 772, 1126, 5153, 48], [5261, 5042, 3054, 4501, 932, 814, 2986, 4983, 4393, 1534, 5383, 3759, 0]]\n",
      "[np.int64(0), np.int64(1), np.int64(0)]\n",
      "[np.str_('15684319')]\n",
      "[[5330, 3803, 121, 1377, 1180, 3892, 4486, 2410, 4386, 5025, 1154, 4979, 5637, 4397, 952, 2572, 2476, 369, 1805, 1574, 4506, 2409, 2572, 2477, 369, 4294, 5102, 842, 2572, 2478, 369, 671, 0, 2953], [1940, 4386, 1121, 3000, 4109, 2903, 3888], [963, 3536, 2476, 3314, 3769, 2953, 842, 2477, 2478, 4638, 5303, 1018]]\n",
      "[np.int64(0), np.int64(0), np.int64(1)]\n",
      "[np.str_('12122096')]\n",
      "[[772, 4994, 5234, 5637, 5413, 4836]]\n",
      "[np.int64(0)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (6 x 195). Kernel size: (7 x 195). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result_train \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabs_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantifier\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 81\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(collection, quantifier, n)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_instances)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_labels)\n\u001b[1;32m---> 81\u001b[0m estim_dist \u001b[38;5;241m=\u001b[39m \u001b[43mquantifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m total_ae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m qp\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mae(true_dist, estim_dist)\n\u001b[0;32m     84\u001b[0m total_rae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m qp\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mrae(true_dist, estim_dist)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\method\\_neural.py:205\u001b[0m, in \u001b[0;36mQuaNetTrainer.quantify\u001b[1;34m(self, instances)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantify\u001b[39m(\u001b[38;5;28mself\u001b[39m, instances):\n\u001b[1;32m--> 205\u001b[0m     posteriors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mtransform(instances)\n\u001b[0;32m    207\u001b[0m     quant_estims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_aggregative_estims(posteriors)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:235\u001b[0m, in \u001b[0;36mNeuralClassifierTrainer.predict_proba\u001b[1;34m(self, instances)\u001b[0m\n\u001b[0;32m    232\u001b[0m     posteriors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m TorchDataset(instances)\u001b[38;5;241m.\u001b[39masDataloader(\n\u001b[0;32m    234\u001b[0m             opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size_test\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pad_length\u001b[38;5;241m=\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding_length\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m--> 235\u001b[0m         posteriors\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(posteriors)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:346\u001b[0m, in \u001b[0;36mTextClassifierNet.predict_proba\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Predicts posterior probabilities for the instances in `x`\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    :return: array-like of shape `(n_samples, n_classes)` with the posterior probabilities\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:327\u001b[0m, in \u001b[0;36mTextClassifierNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    320\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs the forward pass.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    :param x: a batch of instances, typically generated by a torch's `DataLoader`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m        for each of the instances and classes\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     doc_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(doc_embedded)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:517\u001b[0m, in \u001b[0;36mCNNnet.document_embedding\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    515\u001b[0m max_out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__conv_block(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1)\n\u001b[0;32m    516\u001b[0m max_out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__conv_block(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2)\n\u001b[1;32m--> 517\u001b[0m max_out3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__conv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m all_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((max_out1, max_out2, max_out3), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# all_out.size() = (batch_size, num_kernels*out_channels)\u001b[39;00m\n\u001b[0;32m    520\u001b[0m abstracted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39mrelu(all_out))  \u001b[38;5;66;03m#  (batch_size, num_kernels*out_channels)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:497\u001b[0m, in \u001b[0;36mCNNnet.__conv_block\u001b[1;34m(self, input, conv_layer)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__conv_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, conv_layer):\n\u001b[1;32m--> 497\u001b[0m     conv_out \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# conv_out.size() = (batch_size, out_channels, dim, 1)\u001b[39;00m\n\u001b[0;32m    498\u001b[0m     activation \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(conv_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# activation.size() = (batch_size, out_channels, dim1)\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     max_out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool1d(activation, activation\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# maxpool_out.size() = (batch_size, out_channels)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (6 x 195). Kernel size: (7 x 195). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "result_train = evaluate(collection=abs_dataset.val, n=[1], quantifier=quantifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8f9f37-f3ef-42c1-a148-84dffb5558f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.0015         \t0.0070         \t0.0023         \t0.0013         \t0.0012         \t0.0013         \t0.0012         \n",
      "RAE            \t0.0033         \t0.0202         \t0.0058         \t0.0030         \t0.0025         \t0.0029         \t0.0027         \n",
      "MSE            \t0.0000         \t0.0002         \t0.0000         \t0.0000         \t0.0000         \t0.0000         \t0.0000         \n",
      "MAE            \t0.0015         \t0.0070         \t0.0023         \t0.0013         \t0.0012         \t0.0013         \t0.0012         \n",
      "MRAE           \t0.0033         \t0.0202         \t0.0058         \t0.0030         \t0.0025         \t0.0029         \t0.0027         \n",
      "MKLD           \t0.0000         \t0.0005         \t0.0001         \t0.0000         \t0.0000         \t0.0000         \t0.0000         \n",
      "- Val set:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (6 x 195). Kernel size: (7 x 195). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m result_train \u001b[38;5;241m=\u001b[39m evaluate(collection\u001b[38;5;241m=\u001b[39mabs_dataset\u001b[38;5;241m.\u001b[39mtraining, n\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m, qp\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMPLE_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m15\u001b[39m], quantifier\u001b[38;5;241m=\u001b[39mquantifier)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m- Val set:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m result_train \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabs_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAMPLE_SIZE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantifier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m- Glaucoma test set:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m result_test \u001b[38;5;241m=\u001b[39m evaluate(collection\u001b[38;5;241m=\u001b[39mglaucoma_collection, n\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m, qp\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMPLE_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m15\u001b[39m], quantifier\u001b[38;5;241m=\u001b[39mquantifier)\n",
      "File \u001b[1;32m~\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_4_code.py:468\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(collection, quantifier, n)\u001b[0m\n\u001b[0;32m    465\u001b[0m true_dist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(batch_labels, minlength\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_labels)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Estimated distribution from quantifier\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m estim_dist \u001b[38;5;241m=\u001b[39m \u001b[43mquantifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m total_ae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m qp\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mae(true_dist, estim_dist)\n\u001b[0;32m    471\u001b[0m total_rae \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m qp\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mrae(true_dist, estim_dist)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\method\\_neural.py:205\u001b[0m, in \u001b[0;36mQuaNetTrainer.quantify\u001b[1;34m(self, instances)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantify\u001b[39m(\u001b[38;5;28mself\u001b[39m, instances):\n\u001b[1;32m--> 205\u001b[0m     posteriors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mtransform(instances)\n\u001b[0;32m    207\u001b[0m     quant_estims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_aggregative_estims(posteriors)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:235\u001b[0m, in \u001b[0;36mNeuralClassifierTrainer.predict_proba\u001b[1;34m(self, instances)\u001b[0m\n\u001b[0;32m    232\u001b[0m     posteriors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xi \u001b[38;5;129;01min\u001b[39;00m TorchDataset(instances)\u001b[38;5;241m.\u001b[39masDataloader(\n\u001b[0;32m    234\u001b[0m             opt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size_test\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pad_length\u001b[38;5;241m=\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding_length\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39mopt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m--> 235\u001b[0m         posteriors\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(posteriors)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:346\u001b[0m, in \u001b[0;36mTextClassifierNet.predict_proba\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Predicts posterior probabilities for the instances in `x`\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    :return: array-like of shape `(n_samples, n_classes)` with the posterior probabilities\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:327\u001b[0m, in \u001b[0;36mTextClassifierNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    320\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs the forward pass.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    :param x: a batch of instances, typically generated by a torch's `DataLoader`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m        for each of the instances and classes\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     doc_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(doc_embedded)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:517\u001b[0m, in \u001b[0;36mCNNnet.document_embedding\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    515\u001b[0m max_out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__conv_block(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1)\n\u001b[0;32m    516\u001b[0m max_out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__conv_block(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2)\n\u001b[1;32m--> 517\u001b[0m max_out3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__conv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    519\u001b[0m all_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((max_out1, max_out2, max_out3), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# all_out.size() = (batch_size, num_kernels*out_channels)\u001b[39;00m\n\u001b[0;32m    520\u001b[0m abstracted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(F\u001b[38;5;241m.\u001b[39mrelu(all_out))  \u001b[38;5;66;03m#  (batch_size, num_kernels*out_channels)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:497\u001b[0m, in \u001b[0;36mCNNnet.__conv_block\u001b[1;34m(self, input, conv_layer)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__conv_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, conv_layer):\n\u001b[1;32m--> 497\u001b[0m     conv_out \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# conv_out.size() = (batch_size, out_channels, dim, 1)\u001b[39;00m\n\u001b[0;32m    498\u001b[0m     activation \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(conv_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# activation.size() = (batch_size, out_channels, dim1)\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     max_out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool1d(activation, activation\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# maxpool_out.size() = (batch_size, out_channels)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (6 x 195). Kernel size: (7 x 195). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "print('- Train set:')\n",
    "result_train = evaluate(collection=abs_dataset.training, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)\n",
    "\n",
    "print('- Val set:')\n",
    "result_train = evaluate(collection=abs_dataset.val, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)\n",
    "\n",
    "print('- Glaucoma test set:')\n",
    "result_test = evaluate(collection=glaucoma_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)\n",
    "\n",
    "print('- Neoplasm test set:')\n",
    "result_test = evaluate(collection=neoplasm_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)\n",
    "\n",
    "print('- Mixed test set:')\n",
    "result_test = evaluate(collection=mixed_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2784f5-4a51-4f26-aad1-ada9a77ba52d",
   "metadata": {},
   "source": [
    "DA SCRIVERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fcaf7-1cbd-4b79-b6de-ee445d388465",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(test_set, indexer, comp_quantifier=None, comp_classifier=None, rel_quantifier=quantifier, rel_classifier=cnn_classifier, filename=None, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a2a35-fee8-4458-98ae-7708ae3f554a",
   "metadata": {},
   "source": [
    "DA SCRIVERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
