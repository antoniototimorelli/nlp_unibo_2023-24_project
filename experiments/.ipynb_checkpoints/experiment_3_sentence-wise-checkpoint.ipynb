{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b32ce4d-112a-4c2b-814b-7925eee97ce5",
   "metadata": {},
   "source": [
    "# Experiment 3 - Sentence-Wise Approach\n",
    "\n",
    "In this experiment, we are testing a different method. Instead of treating the document as a whole with the number of arguments as its label, we split each abstract into individual sentences and label them as either *Non-component* or *Component*, corresponding to 0 and 1, respectively.\n",
    "\n",
    "During evaluation and inference, we apply a sampling technique to process all sentences within an abstract, as our goal is to determine the percentage of argument components each abstract contains.\n",
    "\n",
    "Following this, we will explore the possibility of using the same model to infer the number of relations within an abstract. We aim to use the combination of these two outcomes—the distributions of components and relations—to potentially estimate the number of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee6643c-ae6d-4d48-ad83-18f6eb1638bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Antonio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Antonio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from experiment_3_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4b71a-b9f7-4394-ae94-5678e5c8fd67",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "Here we preprocess our data by splitting the abstracts into sentences. Each sentence is then labeled as either being a `Component` of an argument, labeled as `1`, or `Not a component`, labeled as `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471b1a98-8b85-4454-bafe-31ed4ad14d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_brat_dataset('../data/train/neoplasm_train')\n",
    "val_set = read_brat_dataset('../data/dev/neoplasm_dev')\n",
    "\n",
    "glaucoma_test = read_brat_dataset('../data/test/glaucoma_test')\n",
    "neoplasm_test = read_brat_dataset('../data/test/neoplasm_test')\n",
    "mixed_test = read_brat_dataset('../data/test/mixed_test')\n",
    "\n",
    "test_set = glaucoma_test + neoplasm_test + mixed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a2cffa-f391-4e11-bbe0-32124242d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "\tLabel 0: 2760 samples\n",
      "\tLabel 1: 2593 samples\n",
      "\n",
      "\tThere are 2 different labels in the train set -> [0, 1]\n",
      "\tAverage number of sentences per file in train set: 13\n",
      "\tMax sentence length: 107\n",
      "\tAverage components per file: 6.48\n",
      "\tAverage non-components per file: 6.90\n",
      "\n",
      "- Test set:\n",
      "\tLabel 0: 1948 samples\n",
      "\tLabel 1: 1880 samples\n",
      "\n",
      "\tThere are 2 different labels in the test set -> [0, 1]\n",
      "\tAverage number of sentences per file in test set: 14\n",
      "\tMax sentence length: 91\n",
      "\tAverage components per file: 6.99\n",
      "\tAverage non-components per file: 7.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_counts_train, avg_sentences_per_file_train = compute_dataset_statistics(train_set, dataset_name=\"train\")\n",
    "label_counts_test, avg_sentences_per_file_test = compute_dataset_statistics(test_set, dataset_name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b11b11-4c24-4a19-8721-f132b39f4705",
   "metadata": {},
   "source": [
    "Follows an example of what our dictionary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb562ae9-ef26-4bce-bd30-d7c8e7fb543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 23254324 - Text:\n",
      " With the adoption of enhanced recovery and emerging new modalities of analgesia after laparoscopic colorectal resection (LCR), the role of epidural analgesia has been questioned. This pilot trial assessed the feasibility of a randomized controlled trial (RCT) comparing epidural analgesia and use of a local anaesthetic wound infusion catheter (WIC) following LCR. Between April 2010 and May 2011, patients undergoing elective LCR in two centres were randomized to analgesia via epidural or WIC. Sham procedures were used to blind surgeons, patients and outcome assessors. The primary outcome was the feasibility of a large RCT, and all outcomes for a definitive trial were tested. The success of blinding was assessed using a mixed-methods approach. Forty-five patients were eligible, of whom 34 were randomized (mean(s.d.) age 70(11·8) years). Patients were followed up per-protocol; there were no deaths, and five patients had a total of six complications. Challenges with capturing pain data were identified and resolved. Mean(s.d.) pain scores on the day of discharge were 1·9(3·1) in the epidural group and 0·7(0·7) in the WIC group. Median length of stay was 4 (range 2-35, interquartile range 3-5) days. Mean use of additional analgesia (intravenous morphine equivalents) was 12 mg in the WIC arm and 9 mg in the epidural arm. Patient blinding was successful in both arms. Qualitative interviews suggested that patients found participation in the trial acceptable and that they would consider participating in a future trial. A blinded RCT investigating the role of epidural and WIC administration for postoperative analgesia following LCR is feasible. Rigorous standard operating procedures for data collection are required. \n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| sentence                                                                                                                 | label               |\n",
      "+==========================================================================================================================+=====================+\n",
      "| With the adoption of enhanced recovery and emerging new modalities of analgesia after laparoscopic colorectal resection  | Not a component (0) |\n",
      "| (LCR), the role of epidural analgesia has been questioned.                                                               |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| This pilot trial assessed the feasibility of a randomized controlled trial (RCT) comparing epidural analgesia and use of | Not a component (0) |\n",
      "| a local anaesthetic wound infusion catheter (WIC) following LCR.                                                         |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Between April 2010 and May 2011, patients undergoing elective LCR in two centres were randomized to analgesia via        | Not a component (0) |\n",
      "| epidural or WIC.                                                                                                         |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Sham procedures were used to blind surgeons, patients and outcome assessors.                                             | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| The primary outcome was the feasibility of a large RCT, and all outcomes for a definitive trial were tested.             | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| The success of blinding was assessed using a mixed-methods approach.                                                     | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Forty-five patients were eligible, of whom 34 were randomized (mean(s.d.)                                                | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| age 70(11·8) years).                                                                                                     | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Patients were followed up per-protocol; there were no deaths, and five patients had a total of six complications.        | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Challenges with capturing pain data were identified and resolved.                                                        | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Mean(s.d.) pain scores on the day of discharge were 1·9(3·1) in the epidural group and 0·7(0·7) in the WIC group.        | Component (1)       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Median length of stay was 4 (range 2-35, interquartile range 3-5) days.                                                  | Component (1)       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Mean use of additional analgesia (intravenous morphine equivalents) was 12 mg in the WIC arm and 9 mg in the epidural    | Component (1)       |\n",
      "| arm.                                                                                                                     |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Patient blinding was successful in both arms.                                                                            | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Qualitative interviews suggested that patients found participation in the trial acceptable and that they would consider  | Component (1)       |\n",
      "| participating in a future trial.                                                                                         |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| A blinded RCT investigating the role of epidural and WIC administration for postoperative analgesia following LCR is     | Component (1)       |\n",
      "| feasible.                                                                                                                |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Rigorous standard operating procedures for data collection are required.                                                 | Component (1)       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "display_file_info(train_set, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97bf69-2965-4927-ac08-b3cdee1b6238",
   "metadata": {},
   "source": [
    "Next, we create the dataset composed by several `FilenameLabelledCollection`, which inherits from `QuaPy`'s `LabelledCollection` class. \n",
    "\n",
    "We created this class since we needed to keep track of the filenames corresponding to each abstract to which the sentences belong. The `index` method is also modified to return `FilenameLabelledCollection` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e4d204-eea2-4378-b97d-06bc31fddfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collection = FilenameLabelledCollection([data['sentence'] for data in train_set], \n",
    "                                                 [data['label'] for data in train_set], \n",
    "                                                 [data['filename'] for data in train_set])\n",
    "\n",
    "# train_collection, val_collection = train_collection.split_stratified(0.7)\n",
    "# train_collection, val_collection = train_collection.split_stratified_by_filenames(0.75)\n",
    "val_collection = FilenameLabelledCollection([data['sentence'] for data in val_set], \n",
    "                                                 [data['label'] for data in val_set], \n",
    "                                                 [data['filename'] for data in val_set])\n",
    "\n",
    "test_collection = FilenameLabelledCollection([data['sentence'] for data in test_set], \n",
    "                                                 [data['label'] for data in test_set], \n",
    "                                                 [data['filename'] for data in test_set])\n",
    "\n",
    "glaucoma_collection = FilenameLabelledCollection([data['sentence'] for data in glaucoma_test], \n",
    "                                                 [data['label'] for data in glaucoma_test], \n",
    "                                                 [data['filename'] for data in glaucoma_test])\n",
    "\n",
    "neoplasm_collection = FilenameLabelledCollection([data['sentence'] for data in neoplasm_test], \n",
    "                                                 [data['label'] for data in neoplasm_test], \n",
    "                                                 [data['filename'] for data in neoplasm_test])\n",
    "\n",
    "mixed_collection = FilenameLabelledCollection([data['sentence'] for data in mixed_test], \n",
    "                                                 [data['label'] for data in mixed_test], \n",
    "                                                 [data['filename'] for data in mixed_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f26bd85-6bd4-4d48-bb19-6132b2f3c6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "18802161 ['To determine the effectiveness of the Alleviating Depression Among Patients With Cancer (ADAPt-C) collaborative care management for major depression or dysthymia.', 'At 12 months, 63% of intervention patients had a 50% or greater reduction in depressive symptoms from baseline as assessed by the Patient Health Questionnaire-9 (PHQ-9) depression scale compared with 50% of EUC patients (odds ratio [OR] = 1.98; 95% CI, 1.16 to 3.38; P = .01).', 'Intervention patients had access for up to 12 months to a depression clinical specialist (supervised by a psychiatrist) who offered education, structured psychotherapy, and maintenance/relapse prevention support.'] 3\n",
      "\n",
      "Val\n",
      "Study patients included 472 low-income, predominantly female Hispanic patients with cancer age >or= 18 years with major depression (49%), dysthymia (5%), or both (46%).\n",
      "Intervention patients also experienced greater rates of depression treatment (72.3% v 10.4% of EUC patients; P < .0001) and significantly better quality-of-life outcomes, including social/family (adjusted mean difference between groups, 2.7; 95% CI, 1.22 to 4.17; P < .001), emotional (adjusted mean difference, 1.29; 95% CI, 0.26 to 2.22; P = .01), functional (adjusted mean difference, 1.34; 95% CI, 0.08 to 2.59; P = .04), and physical well-being (adjusted mean difference, 2.79; 95% CI, 0.49 to 5.1; P = .02).\n",
      "ADAPt-C collaborative care is feasible and results in significant reduction in depressive symptoms, improvement in quality of life, and lower pain levels compared with EUC for patients with depressive disorders in a low-income, predominantly Hispanic population in public sector oncology clinics.\n",
      "Improvement was also found for 5-point decrease in PHQ-9 score among 72.2% of intervention patients compared with 59.7% of EUC patients (OR = 1.99; 95% CI, 1.14 to 3.50; P = .02).\n",
      "Patients were randomly assigned to intervention (n = 242) or enhanced usual care (EUC; n = 230).\n",
      "The psychiatrist prescribed antidepressant medications for patients preferring or assessed to require medication.\n"
     ]
    }
   ],
   "source": [
    "max_c, mat_t, shortest = 12141412412, [], -1\n",
    "for filename in set(train_collection.filenames):\n",
    "    texts = [instance for i,instance in enumerate(train_collection.instances) if train_collection.filenames[i] == filename]\n",
    "    count = len(texts)\n",
    "    if count < max_c:\n",
    "        max_t = texts\n",
    "        max_c = count\n",
    "        shortest = filename\n",
    "\n",
    "print('Train')\n",
    "print(shortest, max_t, max_c)\n",
    "\n",
    "print('\\nVal')\n",
    "for i, instance in enumerate(val_collection.instances):\n",
    "    if val_collection.filenames[i] == shortest:\n",
    "       print(instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f833f9cb-3a50-4bd9-903a-7ead6a188257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3747\n",
      "400\n",
      "1606\n",
      "391\n"
     ]
    }
   ],
   "source": [
    "print(len(train_collection.instances))\n",
    "print(len(set(train_collection.filenames)))\n",
    "print(len(val_collection.instances))\n",
    "print(len(set(val_collection.filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ae1e54e-10bd-40ed-8632-af7263a83863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|███████████████████████████████████████████████| 4809/4809 [00:00<00:00, 80832.63it/s]\n",
      "indexing: 100%|█████████████████████████████████████████████████| 544/544 [00:00<00:00, 77680.23it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 3828/3828 [00:00<00:00, 72800.21it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 1288/1288 [00:00<00:00, 75618.53it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 1332/1332 [00:00<00:00, 73997.52it/s]\n",
      "indexing: 100%|███████████████████████████████████████████████| 1208/1208 [00:00<00:00, 70913.79it/s]\n"
     ]
    }
   ],
   "source": [
    "indexer = qp.data.preprocessing.IndexTransformer(min_df=1)\n",
    "\n",
    "# Create and index the dataset\n",
    "abs_dataset = CustomDataset(training=train_collection, test=test_collection, val=val_collection)\n",
    "index(abs_dataset, indexer, inplace=True)\n",
    "\n",
    "# Index the test collections\n",
    "index(glaucoma_collection, indexer, fit=False, inplace=True)\n",
    "index(neoplasm_collection, indexer, fit=False, inplace=True)\n",
    "index(mixed_collection, indexer, fit=False, inplace=True)\n",
    "\n",
    "qp.environ['SAMPLE_SIZE'] = avg_sentences_per_file_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97b60c-b816-49b5-8b56-8a0a4f499602",
   "metadata": {},
   "source": [
    "### 2. Classifier\n",
    "`QuaNet` requires a classifier that can provide embedded representations of the inputs. In the original paper, `QuaNet` was tested using an `LSTM` as the base classifier; as `QuaPy`'s authors show in their [example](https://hlt-isti.github.io/QuaPy/manuals/methods.html#the-quanet-neural-network), we will use an instantiation of `QuaNet` that employs a `CNN` as a probabilistic classifier, taking its last layer representation as the document embedding.\n",
    "\n",
    "We decided to conduct a so-called *study* with *Optuna*, an automatic hyperparameter optimization software framework, in order to obtain good hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5685f8-0e3c-4a74-96dd-9d2359ccfe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:18:24,829] A new study created in memory with name: no-name-0a2fdcdf-a9ad-4317-80d7-1100702aca7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 0 with parameters:\n",
      "    Embedding size: 168 - Hidden size: 272\n",
      "    Optimizer: Adam (lr: 1.9629623302707914e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 2, 'T_mult': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=50 tr-loss=0.21776 tr-acc=92.23% tr-macroF1=92.23% patience=1/5 val-loss=0.43C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=50 tr-loss=0.21776 tr-acc=92.23% tr-macroF1=92.23% patience=1/5 val-loss=0.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 45\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:34:47,024] Trial 0 finished with value: 0.8133791285594472 and parameters: {'embedding_size': 168, 'hidden_size': 272, 'optimizer': 'Adam', 'lr': 1.9629623302707914e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 2, 'T_mult': 4}. Best is trial 0 with value: 0.8133791285594472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.43965646624565125 - Best f1 on validation set: 0.8133791285594472\n",
      "Starting trial 1 with parameters:\n",
      "    Embedding size: 150 - Hidden size: 277\n",
      "    Optimizer: Adam (lr: 0.00046025055637465305) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 1, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.02124 tr-acc=99.33% tr-macroF1=99.33% patience=1/5 val-loss=0.56C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=13 tr-loss=0.02124 tr-acc=99.33% tr-macroF1=99.33% patience=1/5 val-loss=0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:39:44,362] Trial 1 finished with value: 0.8214054054054054 and parameters: {'embedding_size': 150, 'hidden_size': 277, 'optimizer': 'Adam', 'lr': 0.00046025055637465305, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 1, 'T_mult': 3}. Best is trial 1 with value: 0.8214054054054054.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.45671650767326355 - Best f1 on validation set: 0.8214054054054054\n",
      "Starting trial 2 with parameters:\n",
      "    Embedding size: 121 - Hidden size: 259\n",
      "    Optimizer: Adam (lr: 2.279852330257769e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 9, 'T_mult': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=16 tr-loss=0.53022 tr-acc=75.03% tr-macroF1=74.99% patience=1/5 val-loss=0.55C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=16 tr-loss=0.53022 tr-acc=75.03% tr-macroF1=74.99% patience=1/5 val-loss=0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 11\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:44:25,762] Trial 2 finished with value: 0.7629470660846975 and parameters: {'embedding_size': 121, 'hidden_size': 259, 'optimizer': 'Adam', 'lr': 2.279852330257769e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 9, 'T_mult': 5}. Best is trial 1 with value: 0.8214054054054054.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5726811289787292 - Best f1 on validation set: 0.7629470660846975\n",
      "Starting trial 3 with parameters:\n",
      "    Embedding size: 144 - Hidden size: 262\n",
      "    Optimizer: Adam (lr: 0.00019370632584897316) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 5, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=19 tr-loss=0.06307 tr-acc=97.67% tr-macroF1=97.67% patience=1/5 val-loss=0.48C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=19 tr-loss=0.06307 tr-acc=97.67% tr-macroF1=97.67% patience=1/5 val-loss=0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 14\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 16:50:38,746] Trial 3 finished with value: 0.8343409214081301 and parameters: {'embedding_size': 144, 'hidden_size': 262, 'optimizer': 'Adam', 'lr': 0.00019370632584897316, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 5, 'T_mult': 2}. Best is trial 3 with value: 0.8343409214081301.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.44644804298877716 - Best f1 on validation set: 0.8343409214081301\n",
      "Starting trial 4 with parameters:\n",
      "    Embedding size: 178 - Hidden size: 292\n",
      "    Optimizer: Adam (lr: 1.806013037049119e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 10, 'T_mult': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=58 tr-loss=0.13027 tr-acc=95.76% tr-macroF1=95.76% patience=1/5 val-loss=0.37C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=58 tr-loss=0.13027 tr-acc=95.76% tr-macroF1=95.76% patience=1/5 val-loss=0.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 53\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:14:43,042] Trial 4 finished with value: 0.8250562996325708 and parameters: {'embedding_size': 178, 'hidden_size': 292, 'optimizer': 'Adam', 'lr': 1.806013037049119e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 10, 'T_mult': 5}. Best is trial 3 with value: 0.8343409214081301.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3807103782892227 - Best f1 on validation set: 0.8250562996325708\n",
      "Starting trial 5 with parameters:\n",
      "    Embedding size: 161 - Hidden size: 292\n",
      "    Optimizer: Adam (lr: 4.460312136508731e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 9, 'T_mult': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=33 tr-loss=0.09703 tr-acc=96.96% tr-macroF1=96.96% patience=1/5 val-loss=0.40C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=33 tr-loss=0.09703 tr-acc=96.96% tr-macroF1=96.96% patience=1/5 val-loss=0.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 28\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:27:44,707] Trial 5 finished with value: 0.8215725806451613 and parameters: {'embedding_size': 161, 'hidden_size': 292, 'optimizer': 'Adam', 'lr': 4.460312136508731e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 9, 'T_mult': 5}. Best is trial 3 with value: 0.8343409214081301.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.39671435952186584 - Best f1 on validation set: 0.8215725806451613\n",
      "Starting trial 6 with parameters:\n",
      "    Embedding size: 190 - Hidden size: 282\n",
      "    Optimizer: Adam (lr: 0.000573590792551479) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=17 tr-loss=0.02281 tr-acc=99.35% tr-macroF1=99.35% patience=1/5 val-loss=0.59C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=17 tr-loss=0.02281 tr-acc=99.35% tr-macroF1=99.35% patience=1/5 val-loss=0.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 12\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:35:18,819] Trial 6 finished with value: 0.8396273125086446 and parameters: {'embedding_size': 190, 'hidden_size': 282, 'optimizer': 'Adam', 'lr': 0.000573590792551479, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 6 with value: 0.8396273125086446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5097274780273438 - Best f1 on validation set: 0.8396273125086446\n",
      "Starting trial 7 with parameters:\n",
      "    Embedding size: 163 - Hidden size: 266\n",
      "    Optimizer: Adam (lr: 0.0007731822753950641) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=16 tr-loss=0.01923 tr-acc=99.44% tr-macroF1=99.44% patience=1/5 val-loss=0.67C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=16 tr-loss=0.01923 tr-acc=99.44% tr-macroF1=99.44% patience=1/5 val-loss=0.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 11\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:41:18,356] Trial 7 finished with value: 0.8386934311820995 and parameters: {'embedding_size': 163, 'hidden_size': 266, 'optimizer': 'Adam', 'lr': 0.0007731822753950641, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 6 with value: 0.8396273125086446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5637769103050232 - Best f1 on validation set: 0.8386934311820995\n",
      "Starting trial 8 with parameters:\n",
      "    Embedding size: 189 - Hidden size: 263\n",
      "    Optimizer: Adam (lr: 7.0802555807133e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 12, 'T_mult': 1})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=33 tr-loss=0.04626 tr-acc=98.84% tr-macroF1=98.84% patience=1/5 val-loss=0.44C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=33 tr-loss=0.04626 tr-acc=98.84% tr-macroF1=98.84% patience=1/5 val-loss=0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 28\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 17:55:23,820] Trial 8 finished with value: 0.827330881177035 and parameters: {'embedding_size': 189, 'hidden_size': 263, 'optimizer': 'Adam', 'lr': 7.0802555807133e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 12, 'T_mult': 1}. Best is trial 6 with value: 0.8396273125086446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4160335659980774 - Best f1 on validation set: 0.827330881177035\n",
      "Starting trial 9 with parameters:\n",
      "    Embedding size: 116 - Hidden size: 275\n",
      "    Optimizer: Adam (lr: 0.00019739267094065648) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=22 tr-loss=0.06281 tr-acc=97.91% tr-macroF1=97.91% patience=1/5 val-loss=0.54C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=22 tr-loss=0.06281 tr-acc=97.91% tr-macroF1=97.91% patience=1/5 val-loss=0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 17\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:01:51,598] Trial 9 finished with value: 0.8074655874331145 and parameters: {'embedding_size': 116, 'hidden_size': 275, 'optimizer': 'Adam', 'lr': 0.00019739267094065648, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 6 with value: 0.8396273125086446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.45263446867465973 - Best f1 on validation set: 0.8074655874331145\n",
      "Starting trial 10 with parameters:\n",
      "    Embedding size: 200 - Hidden size: 285\n",
      "    Optimizer: Adam (lr: 0.0009716216207615512) - Scheduler: CosineAnnealingLR (params: {'T_max': 11})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=10 tr-loss=0.02688 tr-acc=99.31% tr-macroF1=99.31% patience=1/5 val-loss=0.58C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=10 tr-loss=0.02688 tr-acc=99.31% tr-macroF1=99.31% patience=1/5 val-loss=0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 5\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:06:51,997] Trial 10 finished with value: 0.8157962150565179 and parameters: {'embedding_size': 200, 'hidden_size': 285, 'optimizer': 'Adam', 'lr': 0.0009716216207615512, 'scheduler': 'CosineAnnealingLR', 'T_max': 11}. Best is trial 6 with value: 0.8396273125086446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.44705452024936676 - Best f1 on validation set: 0.8157962150565179\n",
      "Starting trial 11 with parameters:\n",
      "    Embedding size: 180 - Hidden size: 269\n",
      "    Optimizer: Adam (lr: 0.0009964893016712443) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=21 tr-loss=0.01527 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.64C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=21 tr-loss=0.01527 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 16\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:15:01,371] Trial 11 finished with value: 0.8500803055606607 and parameters: {'embedding_size': 180, 'hidden_size': 269, 'optimizer': 'Adam', 'lr': 0.0009964893016712443, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.6274630129337311 - Best f1 on validation set: 0.8500803055606607\n",
      "Starting trial 12 with parameters:\n",
      "    Embedding size: 188 - Hidden size: 285\n",
      "    Optimizer: Adam (lr: 0.0003363898648208738) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=23 tr-loss=0.01679 tr-acc=99.57% tr-macroF1=99.57% patience=1/5 val-loss=0.77C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=23 tr-loss=0.01679 tr-acc=99.57% tr-macroF1=99.57% patience=1/5 val-loss=0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 18\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:24:31,915] Trial 12 finished with value: 0.8247188857653974 and parameters: {'embedding_size': 188, 'hidden_size': 285, 'optimizer': 'Adam', 'lr': 0.0003363898648208738, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.6346322596073151 - Best f1 on validation set: 0.8247188857653974\n",
      "Starting trial 13 with parameters:\n",
      "    Embedding size: 199 - Hidden size: 285\n",
      "    Optimizer: Adam (lr: 0.00047973107338745194) - Scheduler: CosineAnnealingLR (params: {'T_max': 6})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=17 tr-loss=0.23515 tr-acc=93.00% tr-macroF1=93.00% patience=1/5 val-loss=0.52C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=17 tr-loss=0.23515 tr-acc=93.00% tr-macroF1=93.00% patience=1/5 val-loss=0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 12\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:31:38,712] Trial 13 finished with value: 0.8386108600542284 and parameters: {'embedding_size': 199, 'hidden_size': 285, 'optimizer': 'Adam', 'lr': 0.00047973107338745194, 'scheduler': 'CosineAnnealingLR', 'T_max': 6}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5374631732702255 - Best f1 on validation set: 0.8386108600542284\n",
      "Starting trial 14 with parameters:\n",
      "    Embedding size: 177 - Hidden size: 270\n",
      "    Optimizer: Adam (lr: 0.00018031716499733065) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=23 tr-loss=0.02741 tr-acc=99.18% tr-macroF1=99.18% patience=1/5 val-loss=0.52C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=23 tr-loss=0.02741 tr-acc=99.18% tr-macroF1=99.18% patience=1/5 val-loss=0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 18\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:40:04,371] Trial 14 finished with value: 0.816347542920082 and parameters: {'embedding_size': 177, 'hidden_size': 270, 'optimizer': 'Adam', 'lr': 0.00018031716499733065, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4966936558485031 - Best f1 on validation set: 0.816347542920082\n",
      "Starting trial 15 with parameters:\n",
      "    Embedding size: 137 - Hidden size: 281\n",
      "    Optimizer: Adam (lr: 0.0006702183183713643) - Scheduler: CosineAnnealingLR (params: {'T_max': 6})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=15 tr-loss=0.09729 tr-acc=96.86% tr-macroF1=96.86% patience=1/5 val-loss=1.27C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=15 tr-loss=0.09729 tr-acc=96.86% tr-macroF1=96.86% patience=1/5 val-loss=1.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 10\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:45:06,422] Trial 15 finished with value: 0.8131283642733169 and parameters: {'embedding_size': 137, 'hidden_size': 281, 'optimizer': 'Adam', 'lr': 0.0006702183183713643, 'scheduler': 'CosineAnnealingLR', 'T_max': 6}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5526565164327621 - Best f1 on validation set: 0.8131283642733169\n",
      "Starting trial 16 with parameters:\n",
      "    Embedding size: 181 - Hidden size: 298\n",
      "    Optimizer: Adam (lr: 0.00031270158812408126) - Scheduler: CosineAnnealingLR (params: {'T_max': 9})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=19 tr-loss=0.02066 tr-acc=99.42% tr-macroF1=99.42% patience=1/5 val-loss=0.58C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=19 tr-loss=0.02066 tr-acc=99.42% tr-macroF1=99.42% patience=1/5 val-loss=0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 14\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 18:52:32,369] Trial 16 finished with value: 0.8271857868832779 and parameters: {'embedding_size': 181, 'hidden_size': 298, 'optimizer': 'Adam', 'lr': 0.00031270158812408126, 'scheduler': 'CosineAnnealingLR', 'T_max': 9}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.49026666581630707 - Best f1 on validation set: 0.8271857868832779\n",
      "Starting trial 17 with parameters:\n",
      "    Embedding size: 105 - Hidden size: 270\n",
      "    Optimizer: Adam (lr: 0.00010424714389739615) - Scheduler: CosineAnnealingLR (params: {'T_max': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=34 tr-loss=0.08011 tr-acc=97.27% tr-macroF1=97.26% patience=1/5 val-loss=0.44C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=34 tr-loss=0.08011 tr-acc=97.27% tr-macroF1=97.26% patience=1/5 val-loss=0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 29\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:00:42,830] Trial 17 finished with value: 0.8176915015719348 and parameters: {'embedding_size': 105, 'hidden_size': 270, 'optimizer': 'Adam', 'lr': 0.00010424714389739615, 'scheduler': 'CosineAnnealingLR', 'T_max': 5}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.43272244930267334 - Best f1 on validation set: 0.8176915015719348\n",
      "Starting trial 18 with parameters:\n",
      "    Embedding size: 167 - Hidden size: 256\n",
      "    Optimizer: Adam (lr: 0.0009430105820563986) - Scheduler: CosineAnnealingLR (params: {'T_max': 8})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.03001 tr-acc=99.20% tr-macroF1=99.20% patience=1/5 val-loss=0.89C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.03001 tr-acc=99.20% tr-macroF1=99.20% patience=1/5 val-loss=0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:04:31,483] Trial 18 finished with value: 0.8333213627331275 and parameters: {'embedding_size': 167, 'hidden_size': 256, 'optimizer': 'Adam', 'lr': 0.0009430105820563986, 'scheduler': 'CosineAnnealingLR', 'T_max': 8}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4886915683746338 - Best f1 on validation set: 0.8333213627331275\n",
      "Starting trial 19 with parameters:\n",
      "    Embedding size: 190 - Hidden size: 282\n",
      "    Optimizer: Adam (lr: 0.0005318585306073118) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=24 tr-loss=0.01528 tr-acc=99.48% tr-macroF1=99.48% patience=1/5 val-loss=0.66C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=24 tr-loss=0.01528 tr-acc=99.48% tr-macroF1=99.48% patience=1/5 val-loss=0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 19\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:14:02,249] Trial 19 finished with value: 0.8400889061228896 and parameters: {'embedding_size': 190, 'hidden_size': 282, 'optimizer': 'Adam', 'lr': 0.0005318585306073118, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5584002733230591 - Best f1 on validation set: 0.8400889061228896\n",
      "Starting trial 20 with parameters:\n",
      "    Embedding size: 157 - Hidden size: 293\n",
      "    Optimizer: Adam (lr: 0.00029834336912563415) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=26 tr-loss=0.01759 tr-acc=99.44% tr-macroF1=99.44% patience=1/5 val-loss=0.54C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=26 tr-loss=0.01759 tr-acc=99.44% tr-macroF1=99.44% patience=1/5 val-loss=0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 21\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:23:03,107] Trial 20 finished with value: 0.826018160568192 and parameters: {'embedding_size': 157, 'hidden_size': 293, 'optimizer': 'Adam', 'lr': 0.00029834336912563415, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5409718304872513 - Best f1 on validation set: 0.826018160568192\n",
      "Starting trial 21 with parameters:\n",
      "    Embedding size: 189 - Hidden size: 281\n",
      "    Optimizer: Adam (lr: 0.0005894738375600022) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.02692 tr-acc=99.07% tr-macroF1=99.07% patience=1/5 val-loss=0.65C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=13 tr-loss=0.02692 tr-acc=99.07% tr-macroF1=99.07% patience=1/5 val-loss=0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:28:14,553] Trial 21 finished with value: 0.8245843989769821 and parameters: {'embedding_size': 189, 'hidden_size': 281, 'optimizer': 'Adam', 'lr': 0.0005894738375600022, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4849175810813904 - Best f1 on validation set: 0.8245843989769821\n",
      "Starting trial 22 with parameters:\n",
      "    Embedding size: 192 - Hidden size: 280\n",
      "    Optimizer: Adam (lr: 0.00045889165985441764) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=20 tr-loss=0.01117 tr-acc=99.63% tr-macroF1=99.63% patience=1/5 val-loss=0.58C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=20 tr-loss=0.01117 tr-acc=99.63% tr-macroF1=99.63% patience=1/5 val-loss=0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 15\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:36:13,040] Trial 22 finished with value: 0.8403875197261187 and parameters: {'embedding_size': 192, 'hidden_size': 280, 'optimizer': 'Adam', 'lr': 0.00045889165985441764, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5621189475059509 - Best f1 on validation set: 0.8403875197261187\n",
      "Starting trial 23 with parameters:\n",
      "    Embedding size: 179 - Hidden size: 274\n",
      "    Optimizer: Adam (lr: 0.00038799590165269345) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.02500 tr-acc=99.40% tr-macroF1=99.40% patience=1/5 val-loss=0.51C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=13 tr-loss=0.02500 tr-acc=99.40% tr-macroF1=99.40% patience=1/5 val-loss=0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:41:05,399] Trial 23 finished with value: 0.8475044034019426 and parameters: {'embedding_size': 179, 'hidden_size': 274, 'optimizer': 'Adam', 'lr': 0.00038799590165269345, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.37978653609752655 - Best f1 on validation set: 0.8475044034019426\n",
      "Starting trial 24 with parameters:\n",
      "    Embedding size: 174 - Hidden size: 273\n",
      "    Optimizer: Adam (lr: 0.00013902866728925927) - Scheduler: CosineAnnealingLR (params: {'T_max': 6})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.06995 tr-acc=98.08% tr-macroF1=98.08% patience=1/5 val-loss=0.41C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=18 tr-loss=0.06995 tr-acc=98.08% tr-macroF1=98.08% patience=1/5 val-loss=0.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 13\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:47:36,988] Trial 24 finished with value: 0.8149625228683323 and parameters: {'embedding_size': 174, 'hidden_size': 273, 'optimizer': 'Adam', 'lr': 0.00013902866728925927, 'scheduler': 'CosineAnnealingLR', 'T_max': 6}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4051789343357086 - Best f1 on validation set: 0.8149625228683323\n",
      "Starting trial 25 with parameters:\n",
      "    Embedding size: 183 - Hidden size: 266\n",
      "    Optimizer: Adam (lr: 0.00037790096962634213) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.01232 tr-acc=99.63% tr-macroF1=99.63% patience=1/5 val-loss=0.55C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=18 tr-loss=0.01232 tr-acc=99.63% tr-macroF1=99.63% patience=1/5 val-loss=0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 13\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 19:54:22,253] Trial 25 finished with value: 0.8312391459801818 and parameters: {'embedding_size': 183, 'hidden_size': 266, 'optimizer': 'Adam', 'lr': 0.00037790096962634213, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.486211359500885 - Best f1 on validation set: 0.8312391459801818\n",
      "Starting trial 26 with parameters:\n",
      "    Embedding size: 172 - Hidden size: 277\n",
      "    Optimizer: Adam (lr: 0.0002515590674009464) - Scheduler: CosineAnnealingLR (params: {'T_max': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=30 tr-loss=0.01021 tr-acc=99.83% tr-macroF1=99.83% patience=1/5 val-loss=0.52C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=30 tr-loss=0.01021 tr-acc=99.83% tr-macroF1=99.83% patience=1/5 val-loss=0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 25\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:05:01,924] Trial 26 finished with value: 0.8451104791074163 and parameters: {'embedding_size': 172, 'hidden_size': 277, 'optimizer': 'Adam', 'lr': 0.0002515590674009464, 'scheduler': 'CosineAnnealingLR', 'T_max': 5}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.48599956929683685 - Best f1 on validation set: 0.8451104791074163\n",
      "Starting trial 27 with parameters:\n",
      "    Embedding size: 173 - Hidden size: 267\n",
      "    Optimizer: Adam (lr: 0.0002418755103294747) - Scheduler: CosineAnnealingLR (params: {'T_max': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=22 tr-loss=0.02289 tr-acc=99.38% tr-macroF1=99.38% patience=1/5 val-loss=0.49C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=22 tr-loss=0.02289 tr-acc=99.38% tr-macroF1=99.38% patience=1/5 val-loss=0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 17\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:12:51,705] Trial 27 finished with value: 0.8113849278602032 and parameters: {'embedding_size': 173, 'hidden_size': 267, 'optimizer': 'Adam', 'lr': 0.0002418755103294747, 'scheduler': 'CosineAnnealingLR', 'T_max': 5}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4771439880132675 - Best f1 on validation set: 0.8113849278602032\n",
      "Starting trial 28 with parameters:\n",
      "    Embedding size: 156 - Hidden size: 277\n",
      "    Optimizer: Adam (lr: 6.549671399661118e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 7})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=37 tr-loss=0.07628 tr-acc=97.74% tr-macroF1=97.74% patience=1/5 val-loss=0.40C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=37 tr-loss=0.07628 tr-acc=97.74% tr-macroF1=97.74% patience=1/5 val-loss=0.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 32\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:25:04,299] Trial 28 finished with value: 0.8222945092015301 and parameters: {'embedding_size': 156, 'hidden_size': 277, 'optimizer': 'Adam', 'lr': 6.549671399661118e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 7}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3990469425916672 - Best f1 on validation set: 0.8222945092015301\n",
      "Starting trial 29 with parameters:\n",
      "    Embedding size: 169 - Hidden size: 273\n",
      "    Optimizer: Adam (lr: 1.0129491724569241e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=21 tr-loss=0.57959 tr-acc=69.47% tr-macroF1=69.35% patience=1/5 val-loss=0.58C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=21 tr-loss=0.57959 tr-acc=69.47% tr-macroF1=69.35% patience=1/5 val-loss=0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 16\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:32:32,399] Trial 29 finished with value: 0.7370435975087137 and parameters: {'embedding_size': 169, 'hidden_size': 273, 'optimizer': 'Adam', 'lr': 1.0129491724569241e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 5}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.605337381362915 - Best f1 on validation set: 0.7370435975087137\n",
      "Starting trial 30 with parameters:\n",
      "    Embedding size: 136 - Hidden size: 271\n",
      "    Optimizer: Adam (lr: 0.00012139876705658531) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=34 tr-loss=0.04251 tr-acc=98.43% tr-macroF1=98.43% patience=1/5 val-loss=0.46C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=34 tr-loss=0.04251 tr-acc=98.43% tr-macroF1=98.43% patience=1/5 val-loss=0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 29\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:42:28,916] Trial 30 finished with value: 0.8255021991564055 and parameters: {'embedding_size': 136, 'hidden_size': 271, 'optimizer': 'Adam', 'lr': 0.00012139876705658531, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4381781369447708 - Best f1 on validation set: 0.8255021991564055\n",
      "Starting trial 31 with parameters:\n",
      "    Embedding size: 195 - Hidden size: 279\n",
      "    Optimizer: Adam (lr: 0.00039881511331787804) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.03102 tr-acc=99.27% tr-macroF1=99.27% patience=1/5 val-loss=0.46C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.03102 tr-acc=99.27% tr-macroF1=99.27% patience=1/5 val-loss=0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:45:40,594] Trial 31 finished with value: 0.8400889061228896 and parameters: {'embedding_size': 195, 'hidden_size': 279, 'optimizer': 'Adam', 'lr': 0.00039881511331787804, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3966093510389328 - Best f1 on validation set: 0.8400889061228896\n",
      "Starting trial 32 with parameters:\n",
      "    Embedding size: 183 - Hidden size: 276\n",
      "    Optimizer: Adam (lr: 0.0007821005742486101) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=14 tr-loss=0.01191 tr-acc=99.72% tr-macroF1=99.72% patience=1/5 val-loss=0.54C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=14 tr-loss=0.01191 tr-acc=99.72% tr-macroF1=99.72% patience=1/5 val-loss=0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 9\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:49:04,131] Trial 32 finished with value: 0.8401272523445166 and parameters: {'embedding_size': 183, 'hidden_size': 276, 'optimizer': 'Adam', 'lr': 0.0007821005742486101, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.47620607912540436 - Best f1 on validation set: 0.8401272523445166\n",
      "Starting trial 33 with parameters:\n",
      "    Embedding size: 170 - Hidden size: 288\n",
      "    Optimizer: Adam (lr: 0.00028098419180759115) - Scheduler: CosineAnnealingLR (params: {'T_max': 5})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=16 tr-loss=0.02559 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.46C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=16 tr-loss=0.02559 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 11\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:52:45,723] Trial 33 finished with value: 0.8090839906599366 and parameters: {'embedding_size': 170, 'hidden_size': 288, 'optimizer': 'Adam', 'lr': 0.00028098419180759115, 'scheduler': 'CosineAnnealingLR', 'T_max': 5}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.42077210545539856 - Best f1 on validation set: 0.8090839906599366\n",
      "Starting trial 34 with parameters:\n",
      "    Embedding size: 150 - Hidden size: 279\n",
      "    Optimizer: Adam (lr: 0.0004680301384838752) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 5, 'T_mult': 1})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=15 tr-loss=0.02529 tr-acc=99.16% tr-macroF1=99.16% patience=1/5 val-loss=0.59C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=15 tr-loss=0.02529 tr-acc=99.16% tr-macroF1=99.16% patience=1/5 val-loss=0.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 10\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 20:55:49,320] Trial 34 finished with value: 0.8248573594541755 and parameters: {'embedding_size': 150, 'hidden_size': 279, 'optimizer': 'Adam', 'lr': 0.0004680301384838752, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 5, 'T_mult': 1}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5234499722719193 - Best f1 on validation set: 0.8248573594541755\n",
      "Starting trial 35 with parameters:\n",
      "    Embedding size: 195 - Hidden size: 274\n",
      "    Optimizer: Adam (lr: 0.00024354939037989158) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 6, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=25 tr-loss=0.01214 tr-acc=99.68% tr-macroF1=99.68% patience=1/5 val-loss=0.58C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=25 tr-loss=0.01214 tr-acc=99.68% tr-macroF1=99.68% patience=1/5 val-loss=0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 20\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:02:08,512] Trial 35 finished with value: 0.8341339367059459 and parameters: {'embedding_size': 195, 'hidden_size': 274, 'optimizer': 'Adam', 'lr': 0.00024354939037989158, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 6, 'T_mult': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5646638870239258 - Best f1 on validation set: 0.8341339367059459\n",
      "Starting trial 36 with parameters:\n",
      "    Embedding size: 183 - Hidden size: 268\n",
      "    Optimizer: Adam (lr: 0.0001570380218233557) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=31 tr-loss=0.02041 tr-acc=99.50% tr-macroF1=99.50% patience=1/5 val-loss=0.49C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=31 tr-loss=0.02041 tr-acc=99.50% tr-macroF1=99.50% patience=1/5 val-loss=0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 26\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:09:34,513] Trial 36 finished with value: 0.8399583912621651 and parameters: {'embedding_size': 183, 'hidden_size': 268, 'optimizer': 'Adam', 'lr': 0.0001570380218233557, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.46722012758255005 - Best f1 on validation set: 0.8399583912621651\n",
      "Starting trial 37 with parameters:\n",
      "    Embedding size: 177 - Hidden size: 262\n",
      "    Optimizer: Adam (lr: 0.0007352091362297835) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 3, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.05207 tr-acc=98.19% tr-macroF1=98.19% patience=1/5 val-loss=1.06C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=18 tr-loss=0.05207 tr-acc=98.19% tr-macroF1=98.19% patience=1/5 val-loss=1.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 13\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:13:42,059] Trial 37 finished with value: 0.8290957042510618 and parameters: {'embedding_size': 177, 'hidden_size': 262, 'optimizer': 'Adam', 'lr': 0.0007352091362297835, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 3, 'T_mult': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.709460437297821 - Best f1 on validation set: 0.8290957042510618\n",
      "Starting trial 38 with parameters:\n",
      "    Embedding size: 143 - Hidden size: 278\n",
      "    Optimizer: Adam (lr: 0.0004330652380747638) - Scheduler: CosineAnnealingLR (params: {'T_max': 7})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=19 tr-loss=0.02741 tr-acc=99.20% tr-macroF1=99.20% patience=1/5 val-loss=0.94C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=19 tr-loss=0.02741 tr-acc=99.20% tr-macroF1=99.20% patience=1/5 val-loss=0.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 14\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:17:31,428] Trial 38 finished with value: 0.8338144867641011 and parameters: {'embedding_size': 143, 'hidden_size': 278, 'optimizer': 'Adam', 'lr': 0.0004330652380747638, 'scheduler': 'CosineAnnealingLR', 'T_max': 7}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5432968735694885 - Best f1 on validation set: 0.8338144867641011\n",
      "Starting trial 39 with parameters:\n",
      "    Embedding size: 164 - Hidden size: 265\n",
      "    Optimizer: Adam (lr: 3.3119635347958725e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 12})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=24 tr-loss=0.34215 tr-acc=86.39% tr-macroF1=86.39% patience=1/5 val-loss=0.46C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=24 tr-loss=0.34215 tr-acc=86.39% tr-macroF1=86.39% patience=1/5 val-loss=0.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 19\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:22:46,836] Trial 39 finished with value: 0.7658031255294602 and parameters: {'embedding_size': 164, 'hidden_size': 265, 'optimizer': 'Adam', 'lr': 3.3119635347958725e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.48842836916446686 - Best f1 on validation set: 0.7658031255294602\n",
      "Starting trial 40 with parameters:\n",
      "    Embedding size: 158 - Hidden size: 259\n",
      "    Optimizer: Adam (lr: 7.872135084946655e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 12, 'T_mult': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=33 tr-loss=0.05288 tr-acc=98.34% tr-macroF1=98.34% patience=1/5 val-loss=0.43C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=33 tr-loss=0.05288 tr-acc=98.34% tr-macroF1=98.34% patience=1/5 val-loss=0.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 28\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:29:29,733] Trial 40 finished with value: 0.8157962150565179 and parameters: {'embedding_size': 158, 'hidden_size': 259, 'optimizer': 'Adam', 'lr': 7.872135084946655e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 12, 'T_mult': 2}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.418350949883461 - Best f1 on validation set: 0.8157962150565179\n",
      "Starting trial 41 with parameters:\n",
      "    Embedding size: 181 - Hidden size: 276\n",
      "    Optimizer: Adam (lr: 0.0008085382461190663) - Scheduler: CosineAnnealingLR (params: {'T_max': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=15 tr-loss=0.00634 tr-acc=99.94% tr-macroF1=99.94% patience=1/5 val-loss=0.60C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=15 tr-loss=0.00634 tr-acc=99.94% tr-macroF1=99.94% patience=1/5 val-loss=0.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 10\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:33:02,829] Trial 41 finished with value: 0.8354636300255636 and parameters: {'embedding_size': 181, 'hidden_size': 276, 'optimizer': 'Adam', 'lr': 0.0008085382461190663, 'scheduler': 'CosineAnnealingLR', 'T_max': 4}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.49442943930625916 - Best f1 on validation set: 0.8354636300255636\n",
      "Starting trial 42 with parameters:\n",
      "    Embedding size: 194 - Hidden size: 275\n",
      "    Optimizer: Adam (lr: 0.0006521297508760211) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.01974 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.56C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.01974 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:35:48,414] Trial 42 finished with value: 0.8453637374638066 and parameters: {'embedding_size': 194, 'hidden_size': 275, 'optimizer': 'Adam', 'lr': 0.0006521297508760211, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4000311344861984 - Best f1 on validation set: 0.8453637374638066\n",
      "Starting trial 43 with parameters:\n",
      "    Embedding size: 194 - Hidden size: 272\n",
      "    Optimizer: Adam (lr: 0.0006185182437491236) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.02529 tr-acc=99.20% tr-macroF1=99.20% patience=1/5 val-loss=0.54C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.02529 tr-acc=99.20% tr-macroF1=99.20% patience=1/5 val-loss=0.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:38:33,238] Trial 43 finished with value: 0.8283395617717486 and parameters: {'embedding_size': 194, 'hidden_size': 272, 'optimizer': 'Adam', 'lr': 0.0006185182437491236, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.40974386036396027 - Best f1 on validation set: 0.8283395617717486\n",
      "Starting trial 44 with parameters:\n",
      "    Embedding size: 186 - Hidden size: 269\n",
      "    Optimizer: Adam (lr: 0.00021944136736283608) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=23 tr-loss=0.01700 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.58C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=23 tr-loss=0.01700 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 18\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:44:02,695] Trial 44 finished with value: 0.828945555615457 and parameters: {'embedding_size': 186, 'hidden_size': 269, 'optimizer': 'Adam', 'lr': 0.00021944136736283608, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5551873445510864 - Best f1 on validation set: 0.828945555615457\n",
      "Starting trial 45 with parameters:\n",
      "    Embedding size: 194 - Hidden size: 275\n",
      "    Optimizer: Adam (lr: 0.0009911483017170642) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.03116 tr-acc=98.97% tr-macroF1=98.97% patience=1/5 val-loss=0.65C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=18 tr-loss=0.03116 tr-acc=98.97% tr-macroF1=98.97% patience=1/5 val-loss=0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 13\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:48:31,411] Trial 45 finished with value: 0.843270786543988 and parameters: {'embedding_size': 194, 'hidden_size': 275, 'optimizer': 'Adam', 'lr': 0.0009911483017170642, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5998983383178711 - Best f1 on validation set: 0.843270786543988\n",
      "Starting trial 46 with parameters:\n",
      "    Embedding size: 197 - Hidden size: 275\n",
      "    Optimizer: Adam (lr: 0.0009668857763881396) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=11 tr-loss=0.03704 tr-acc=98.62% tr-macroF1=98.62% patience=1/5 val-loss=0.62C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=11 tr-loss=0.03704 tr-acc=98.62% tr-macroF1=98.62% patience=1/5 val-loss=0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 6\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:51:20,507] Trial 46 finished with value: 0.8369692540272118 and parameters: {'embedding_size': 197, 'hidden_size': 275, 'optimizer': 'Adam', 'lr': 0.0009668857763881396, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4546121507883072 - Best f1 on validation set: 0.8369692540272118\n",
      "Starting trial 47 with parameters:\n",
      "    Embedding size: 174 - Hidden size: 283\n",
      "    Optimizer: Adam (lr: 0.0006625463329262607) - Scheduler: CosineAnnealingLR (params: {'T_max': 2})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=10 tr-loss=0.02455 tr-acc=99.25% tr-macroF1=99.25% patience=1/5 val-loss=0.52C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=10 tr-loss=0.02455 tr-acc=99.25% tr-macroF1=99.25% patience=1/5 val-loss=0.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 5\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:53:40,234] Trial 47 finished with value: 0.8359697056849775 and parameters: {'embedding_size': 174, 'hidden_size': 283, 'optimizer': 'Adam', 'lr': 0.0006625463329262607, 'scheduler': 'CosineAnnealingLR', 'T_max': 2}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.38582730293273926 - Best f1 on validation set: 0.8359697056849775\n",
      "Starting trial 48 with parameters:\n",
      "    Embedding size: 187 - Hidden size: 272\n",
      "    Optimizer: Adam (lr: 0.0008404505318084539) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 7, 'T_mult': 4})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=15 tr-loss=0.02316 tr-acc=99.35% tr-macroF1=99.35% patience=1/5 val-loss=0.62C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=15 tr-loss=0.02316 tr-acc=99.35% tr-macroF1=99.35% patience=1/5 val-loss=0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 10\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 21:57:18,797] Trial 48 finished with value: 0.8290192978989659 and parameters: {'embedding_size': 187, 'hidden_size': 272, 'optimizer': 'Adam', 'lr': 0.0008404505318084539, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 7, 'T_mult': 4}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5943799167871475 - Best f1 on validation set: 0.8290192978989659\n",
      "Starting trial 49 with parameters:\n",
      "    Embedding size: 199 - Hidden size: 277\n",
      "    Optimizer: Adam (lr: 0.0005764457129067884) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=13 tr-loss=0.01766 tr-acc=99.46% tr-macroF1=99.46% patience=1/5 val-loss=0.63C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:647: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  # Final training pass over validation set\n",
      "[CNNnet] training epoch=13 tr-loss=0.01766 tr-acc=99.46% tr-macroF1=99.46% patience=1/5 val-loss=0.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 22:00:59,343] Trial 49 finished with value: 0.8380408870909486 and parameters: {'embedding_size': 199, 'hidden_size': 277, 'optimizer': 'Adam', 'lr': 0.0005764457129067884, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 11 with value: 0.8500803055606607.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.44739678502082825 - Best f1 on validation set: 0.8380408870909486\n",
      "Best trial:\n",
      "  Macro F1 Score: 0.8500803055606607\n",
      "  Params:\n",
      "    embedding_size: 180\n",
      "    hidden_size: 269\n",
      "    optimizer: Adam\n",
      "    lr: 0.0009964893016712443\n",
      "    scheduler: CosineAnnealingLR\n",
      "    T_max: 2\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, abs_dataset), n_trials=50)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Macro F1 Score: {trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2fede1-29aa-4d6c-bcfc-dad7277f6be3",
   "metadata": {},
   "source": [
    "We can now train and test using the obtained hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e338f136-88cd-4033-a53e-c034be38ec0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=18 tr-loss=0.02346 tr-acc=99.31% tr-macroF1=99.31% patience=1/10 val-loss=0.8C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:645: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=18 tr-loss=0.02346 tr-acc=99.31% tr-macroF1=99.31% patience=1/10 val-loss=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoints/components/classifier_net.dat from epoch 8\n",
      "Performing a final training pass over the validation set...\n",
      "[Training complete] - Best loss on validation set: 0.5136085599660873 - Best f1 on validation set: 0.8535482942165526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<experiment_3_code.ScheduledNeuralClassifierTrainer at 0x231f5a4d880>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "\n",
    "embedding_size = 180\n",
    "hidden_size = 269   \n",
    "lr = 0.0009964893016712443\n",
    "\n",
    "cnn_module = CNNnet(\n",
    "    abs_dataset.vocabulary_size,\n",
    "    abs_dataset.training.n_classes,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size\n",
    ")\n",
    "\n",
    "optimizer = Adam(cnn_module.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=2)\n",
    "\n",
    "cnn_classifier = ScheduledNeuralClassifierTrainer(\n",
    "    cnn_module,\n",
    "    lr_scheduler=scheduler,\n",
    "    optim = optimizer,\n",
    "    device='cpu',\n",
    "    checkpointpath='../checkpoints/components/classifier_net.dat',\n",
    "    padding_length=107,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "cnn_classifier.fit(*abs_dataset.training.Xy, *abs_dataset.val.Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "634a48c7-e2df-4b40-a2b5-b38401810af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "\tF1: 0.9997328638048136\n",
      "\tAccuracy: 0.9997331198291967\n",
      "- Glaucoma test set:\n",
      "\tF1: 0.7999577550538624\n",
      "\tAccuracy: 0.8059006211180124\n",
      "- Neoplasm test set:\n",
      "\tF1: 0.8273175944860675\n",
      "\tAccuracy: 0.8273273273273273\n",
      "- Mixed test set:\n",
      "\tF1: 0.8083786874696514\n",
      "\tAccuracy: 0.8104304635761589\n"
     ]
    }
   ],
   "source": [
    "f1_train = 1-qp.error.f1e(abs_dataset.training.labels, cnn_classifier.predict(abs_dataset.training.instances))\n",
    "accuracy_train = 1-qp.error.acce(abs_dataset.training.labels, cnn_classifier.predict(abs_dataset.training.instances))\n",
    "print('- Train set:')\n",
    "print(f'\\tF1: {f1_train}')    \n",
    "print(f'\\tAccuracy: {accuracy_train}')    \n",
    "\n",
    "f1_test_glaucoma = 1-qp.error.f1e(glaucoma_collection.labels, cnn_classifier.predict(glaucoma_collection.instances))\n",
    "accuracy_test_glaucoma = 1-qp.error.acce(glaucoma_collection.labels, cnn_classifier.predict(glaucoma_collection.instances))\n",
    "\n",
    "print('- Glaucoma test set:')\n",
    "print(f'\\tF1: {f1_test_glaucoma}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_glaucoma}')    \n",
    "\n",
    "f1_test_neoplasm = 1-qp.error.f1e(neoplasm_collection.labels, cnn_classifier.predict(neoplasm_collection.instances))\n",
    "accuracy_test_neoplasm = 1-qp.error.acce(neoplasm_collection.labels, cnn_classifier.predict(neoplasm_collection.instances))\n",
    "\n",
    "print('- Neoplasm test set:')\n",
    "print(f'\\tF1: {f1_test_neoplasm}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_neoplasm}')\n",
    "\n",
    "f1_test_mixed = 1-qp.error.f1e(mixed_collection.labels, cnn_classifier.predict(mixed_collection.instances))\n",
    "accuracy_test_mixed = 1-qp.error.acce(mixed_collection.labels, cnn_classifier.predict(mixed_collection.instances))\n",
    "\n",
    "print('- Mixed test set:')\n",
    "print(f'\\tF1: {f1_test_mixed}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_mixed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d1acfd-378b-42cb-b2f1-96fb36477b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 9643663 - Text:\n",
      " To evaluate the efficacy and tolerability of 'Casodex' monotherapy (150 mg daily) for metastatic and locally advanced prostate cancer. A total of 1,453 patients with either confirmed metastatic disease (M1), or T3/T4 non-metastatic disease with elevated prostate-specific antigen (M0) were recruited into one of two identical, multicentre, randomised studies to compare 'Casodex' 150 mg/day with castration. The protocols allowed for combined analysis. At a median follow-up period of approximately 100 weeks for both studies, 'Casodex' 150 mg was found to be less effective than castration in patients with metastatic disease (M1) at entry (hazard ratio of 1.30 for time to death) with a difference in median survival of 6 weeks. In symptomatic M1 patients, 'Casodex' was associated with a statistically significant improvement in subjective response (70%) compared with castration (58%). Analysis of a validated quality-of-life questionnaire proved an advantage for 'Casodex' in sexual interest and physical capacity. 'Casodex' had a substantially lower incidence of hot flushes compared to castration (6-13% compared with 39-44%) and the most commonly reported adverse events were those expected for a potent antiandrogen. However, in patients with M0 disease at entry, the data are still immature with only 13% of M0 patients having died. An initial analysis of this immature data has suggested that the results in these patients may be different to those obtained in patients with M1 disease. A further survival analysis in patients with M0 disease is therefore planned when the data are more mature. 'Casodex' 150 mg is less effective than castration in patients with M1 disease. However, 'Casodex' has shown a benefit in terms of quality of life and subjective response when compared to castration and has an acceptable tolerability profile. Thus 'Casodex' 150 mg monotherapy is an option for patients with M1 prostate cancer for whom surgical or medical castration is not indicated or is not acceptable. \n",
      "\n",
      "\n",
      "**************************************************\n",
      "Component Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 12823.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tClassification:\n",
      "\t\t# Ground truth components: (0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1)\n",
      "\t\t# Predicted components labels: [0 0 0 1 1 0 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "infer(test_set, indexer, comp_quantifier=None, comp_classifier=cnn_classifier, rel_quantifier=None, rel_classifier=None, filename=None, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc37da-6239-4833-9c63-03cac51fdb78",
   "metadata": {},
   "source": [
    "### 3. QuaNet \n",
    "The results are solid, let's move onto `QuaNet` training phase. `QuaNet` observes the classification predictions to learn higher-order *quantification embeddings*, which are then refined by incorporating quantification predictions of simple classify-and-count-like methods.\n",
    "\n",
    "![architecture](./images/quanet_architecture.png)\n",
    "\n",
    "The QuaNet architecture (see Figure 1) consists of two main components: a **recurrent component** and a **fully connected component**.\n",
    "\n",
    "#### 3.1 Recurrent Component: Bidirectional LSTM\n",
    "- The core of the model is a **Bidirectional LSTM** (Long Short-Term Memory), a type of recurrent neural network. \n",
    "- The LSTM receives as input a **list of pairs** $⟨Pr(c|x), \\vec{x}⟩$, where:\n",
    "  - $Pr(c|x)$ is the probability that a classifier $h$ assigns class $c$ to document $x$.\n",
    "  - $\\vec{x}$ is the **document embedding**, a vector representing the document's content.\n",
    "- The list is **sorted by the value of $Pr(c|x)$**, meaning the documents are arranged from least to most likely to belong to class $c$.\n",
    "  \n",
    "The **intuition** behind this approach is that the LSTM will \"learn to count\" positive and negative examples. By observing the ordered sequence of probabilities, the LSTM should learn to recognize the point where the documents switch from negative to positive examples. The document embedding $\\vec{x}$ helps the LSTM assign different importance to each document when making its prediction.\n",
    "\n",
    "The output of the LSTM is called a **quantification embedding**—a dense vector representing the information about the quantification task learned from the input data.\n",
    "\n",
    "#### 3.2 Fully Connected Component\n",
    "- The vector returned by the LSTM is combined with additional information, specifically **quantification-related statistics**:\n",
    "  - $\\hat{p}_c^{CC}(D)$, $\\hat{p}_c^{ACC}(D)$, $\\hat{p}_c^{PCC}(D)$, and $\\hat{p}_c^{PACC}(D)$, which are quantification predictions from different methods.\n",
    "  - $tpr_b$, $fpr_b$, $tpr_s$, and $fpr_s$, aggregate statistics related to true positive and false positive rates, which are easy to compute from the classifier $h$ using a validation set.\n",
    "\n",
    "This combined vector then passes through the second part of the architecture, which is made up of **fully connected layers** with **ReLU activations**. These layers adjust the quantification embedding using the additional statistics from the classifier to improve the accuracy of the quantification.\n",
    "\n",
    "The final output is a prediction $\\hat{p}_c^{QuaNet}(c|D)$, which represents the probability of class $c$ for the dataset $D$, produced by a **softmax layer**.\n",
    "\n",
    "QuaNet could use quantification predictions from many methods, but it focuses on those that are **computationally efficient** (like CC, ACC, PCC, and PACC). This ensures that the process remains fast while still providing sufficient information for accurate predictions.\n",
    "\n",
    "### Details\n",
    "\n",
    "| Layer | Type | Dimensions | Activation | Dropout |\n",
    "|---|---|---|---|---|\n",
    "| Input | LSTM | 128 | N/A | N/A |\n",
    "| Dense 1 | Dense | 1024 | ReLU | 0.5 |\n",
    "| Dense 2 | Dense | 512 | ReLU | 0.5 |\n",
    "| Output | Dense | 2 | Softmax | N/A |\n",
    "\n",
    "- The LSTM has **64 hidden dimensions**, and since it’s bidirectional, the final LSTM output has **128 dimensions**.\n",
    "- This LSTM output is concatenated with the **8 quantification statistics** (giving a total of 136 dimensions), which is then fed into:\n",
    "  - **Two dense layers** with **1,024** and **512 dimensions**, each using **ReLU activation** and **0.5 dropout**.\n",
    "  - Finally, the output is passed through a **softmax layer** of size 2 to make the final class prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc527bb1-dec8-4f00-b777-f3d407f93165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuaNetModule(\n",
      "  (lstm): LSTM(102, 64, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (ff_layers): ModuleList(\n",
      "    (0): Linear(in_features=136, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/500 [00:00<?, ?it/s]C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\method\\_neural.py:233: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  ptrue = torch.as_tensor([sample_data.prevalence()], dtype=torch.float, device=self.device)\n",
      "[QuaNet] epoch=1 [it=499/500]\ttr-mseloss=0.00463 tr-maeloss=0.03814\tval-mseloss=-1.00000 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 49.62it/s]\n",
      "[QuaNet] epoch=2 [it=499/500]\ttr-mseloss=0.00052 tr-maeloss=0.01684\tval-mseloss=0.00178 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 53.49it/s]\n",
      "[QuaNet] epoch=3 [it=499/500]\ttr-mseloss=0.00062 tr-maeloss=0.01629\tval-mseloss=0.00023 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 47.71it/s]\n",
      "[QuaNet] epoch=4 [it=499/500]\ttr-mseloss=0.00024 tr-maeloss=0.01039\tval-mseloss=0.00009 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 47.99it/s]\n",
      "[QuaNet] epoch=5 [it=499/500]\ttr-mseloss=0.00020 tr-maeloss=0.00948\tval-mseloss=0.00023 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.71it/s]\n",
      "[QuaNet] epoch=6 [it=499/500]\ttr-mseloss=0.00052 tr-maeloss=0.01265\tval-mseloss=0.00003 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 46.62it/s]\n",
      "[QuaNet] epoch=7 [it=499/500]\ttr-mseloss=0.00022 tr-maeloss=0.00942\tval-mseloss=0.00036 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 46.56it/s]\n",
      "[QuaNet] epoch=8 [it=499/500]\ttr-mseloss=0.00005 tr-maeloss=0.00466\tval-mseloss=0.00001 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 48.12it/s]\n",
      "[QuaNet] epoch=9 [it=499/500]\ttr-mseloss=0.00017 tr-maeloss=0.00789\tval-mseloss=0.00002 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 48.26it/s]\n",
      "[QuaNet] epoch=10 [it=499/500]\ttr-mseloss=0.00018 tr-maeloss=0.00842\tval-mseloss=0.00007 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 48.25it/s]\n",
      "[QuaNet] epoch=11 [it=499/500]\ttr-mseloss=0.00027 tr-maeloss=0.01023\tval-mseloss=0.00018 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 49.67it/s]\n",
      "[QuaNet] epoch=12 [it=499/500]\ttr-mseloss=0.00018 tr-maeloss=0.00823\tval-mseloss=0.00011 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 48.65it/s]\n",
      "[QuaNet] epoch=13 [it=499/500]\ttr-mseloss=0.00013 tr-maeloss=0.00642\tval-mseloss=0.00004 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 47.74it/s]\n",
      "[QuaNet] epoch=14 [it=499/500]\ttr-mseloss=0.00004 tr-maeloss=0.00374\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 46.69it/s]\n",
      "[QuaNet] epoch=15 [it=499/500]\ttr-mseloss=0.00023 tr-maeloss=0.00901\tval-mseloss=0.00005 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 50.46it/s]\n",
      "[QuaNet] epoch=16 [it=499/500]\ttr-mseloss=0.00016 tr-maeloss=0.00613\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.75it/s]\n",
      "[QuaNet] epoch=17 [it=499/500]\ttr-mseloss=0.00017 tr-maeloss=0.00697\tval-mseloss=0.00009 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 45.42it/s]\n",
      "[QuaNet] epoch=18 [it=499/500]\ttr-mseloss=0.00007 tr-maeloss=0.00445\tval-mseloss=0.00048 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.96it/s]\n",
      "[QuaNet] epoch=19 [it=499/500]\ttr-mseloss=0.00012 tr-maeloss=0.00516\tval-mseloss=0.00000 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.11it/s]\n",
      "[QuaNet] epoch=20 [it=499/500]\ttr-mseloss=0.00010 tr-maeloss=0.00568\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.72it/s]\n",
      "[QuaNet] epoch=21 [it=499/500]\ttr-mseloss=0.00005 tr-maeloss=0.00416\tval-mseloss=0.00003 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.94it/s]\n",
      "[QuaNet] epoch=22 [it=499/500]\ttr-mseloss=0.00002 tr-maeloss=0.00306\tval-mseloss=0.00002 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 54.15it/s]\n",
      "[QuaNet] epoch=23 [it=499/500]\ttr-mseloss=0.00009 tr-maeloss=0.00560\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 54.23it/s]\n",
      "[QuaNet] epoch=24 [it=499/500]\ttr-mseloss=0.00015 tr-maeloss=0.00785\tval-mseloss=0.00032 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 53.28it/s]\n",
      "[QuaNet] epoch=25 [it=499/500]\ttr-mseloss=0.00009 tr-maeloss=0.00594\tval-mseloss=0.00002 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 49.54it/s]\n",
      "[QuaNet] epoch=26 [it=499/500]\ttr-mseloss=0.00004 tr-maeloss=0.00389\tval-mseloss=0.00002 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.89it/s]\n",
      "[QuaNet] epoch=27 [it=499/500]\ttr-mseloss=0.00008 tr-maeloss=0.00544\tval-mseloss=0.00001 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 53.03it/s]\n",
      "[QuaNet] epoch=28 [it=499/500]\ttr-mseloss=0.00004 tr-maeloss=0.00412\tval-mseloss=0.00014 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 52.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended by patience exhausted; loading best model parameters in ../checkpoints/components\\Quanet-Components for epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\method\\_neural.py:188: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.quanet.load_state_dict(torch.load(checkpoint))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>QuaNetTrainer(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "              classifier__device=device(type=&#x27;cpu&#x27;), classifier__drop_p=0.5,\n",
       "              classifier__embedding_size=180, classifier__epochs=200,\n",
       "              classifier__hidden_size=269, classifier__kernel_heights=[3, 5, 7],\n",
       "              classifier__lr=0.001, classifier__padding_length=107,\n",
       "              classifier__patience=10, classifier__repr_size=100,\n",
       "              classifier__stride=1, classifier__weight_decay=0, qdrop_p=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;QuaNetTrainer<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>QuaNetTrainer(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "              classifier__device=device(type=&#x27;cpu&#x27;), classifier__drop_p=0.5,\n",
       "              classifier__embedding_size=180, classifier__epochs=200,\n",
       "              classifier__hidden_size=269, classifier__kernel_heights=[3, 5, 7],\n",
       "              classifier__lr=0.001, classifier__padding_length=107,\n",
       "              classifier__patience=10, classifier__repr_size=100,\n",
       "              classifier__stride=1, classifier__weight_decay=0, qdrop_p=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "QuaNetTrainer(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "              classifier__device=device(type='cpu'), classifier__drop_p=0.5,\n",
       "              classifier__embedding_size=180, classifier__epochs=200,\n",
       "              classifier__hidden_size=269, classifier__kernel_heights=[3, 5, 7],\n",
       "              classifier__lr=0.001, classifier__padding_length=107,\n",
       "              classifier__patience=10, classifier__repr_size=100,\n",
       "              classifier__stride=1, classifier__weight_decay=0, qdrop_p=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train QuaNet (alternatively, we can set fit_classifier=True and let QuaNet train the classifier)\n",
    "set_seed(42)\n",
    "\n",
    "quantifier = QuaNet(cnn_classifier, qp.environ['SAMPLE_SIZE'], qdrop_p=0, device='cpu', checkpointdir='../checkpoints/components', checkpointname='Quanet-Components')\n",
    "quantifier.fit(abs_dataset.training, fit_classifier=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0de8d-011c-4c6f-bd31-62fe0c9da80e",
   "metadata": {},
   "source": [
    "We wrapped `QuaPy`'s error evaluation function and manually modified how each sample is selected; we adjusted the sampling strategy to work with batches where the batch size is equal to the number of sentences that compose each abstract. This allows us to select the entire document based on the filename associated with each sentence. We will also evaluate the results using the standard random sampling technique, where sentences from different abstracts are grouped into the same batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d8f9f37-f3ef-42c1-a148-84dffb5558f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\n",
      "AE             \t0.0061         \t0.0038         \t0.0031         \t0.0038         \t0.0038         \t0.0039         \n",
      "RAE            \t0.0113         \t0.0089         \t0.0059         \t0.0073         \t0.0072         \t0.0074         \n",
      "MSE            \t0.0000         \t0.0001         \t0.0000         \t0.0000         \t0.0000         \t0.0000         \n",
      "MAE            \t0.0061         \t0.0038         \t0.0031         \t0.0038         \t0.0038         \t0.0039         \n",
      "MRAE           \t0.0113         \t0.0089         \t0.0059         \t0.0073         \t0.0072         \t0.0074         \n",
      "MKLD           \t0.0001         \t0.0001         \t0.0000         \t0.0000         \t0.0000         \t0.0000         \n",
      "- Val set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.0128         \t0.1120         \t0.0716         \t0.0611         \t0.0464         \t0.0347         \t0.0388         \n",
      "RAE            \t0.0238         \t0.5765         \t0.1826         \t0.1213         \t0.0883         \t0.0653         \t0.0737         \n",
      "MSE            \t0.0002         \t0.0387         \t0.0103         \t0.0065         \t0.0034         \t0.0022         \t0.0023         \n",
      "MAE            \t0.0128         \t0.1120         \t0.0716         \t0.0611         \t0.0464         \t0.0347         \t0.0388         \n",
      "MRAE           \t0.0238         \t0.5765         \t0.1826         \t0.1213         \t0.0883         \t0.0653         \t0.0737         \n",
      "MKLD           \t0.0003         \t0.1041         \t0.0200         \t0.0118         \t0.0060         \t0.0038         \t0.0041         \n",
      "- Glaucoma test set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.0906         \t0.1267         \t0.1020         \t0.0932         \t0.0919         \t0.0912         \t0.0936         \n",
      "RAE            \t0.1692         \t0.2464         \t0.1936         \t0.1752         \t0.1725         \t0.1709         \t0.1757         \n",
      "MSE            \t0.0082         \t0.0276         \t0.0143         \t0.0110         \t0.0097         \t0.0092         \t0.0105         \n",
      "MAE            \t0.0906         \t0.1267         \t0.1020         \t0.0932         \t0.0919         \t0.0912         \t0.0936         \n",
      "MRAE           \t0.1692         \t0.2464         \t0.1936         \t0.1752         \t0.1725         \t0.1709         \t0.1757         \n",
      "MKLD           \t0.0147         \t0.0601         \t0.0268         \t0.0200         \t0.0175         \t0.0167         \t0.0189         \n",
      "- Neoplasm test set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.0441         \t0.1089         \t0.0725         \t0.0529         \t0.0552         \t0.0466         \t0.0515         \n",
      "RAE            \t0.0819         \t0.2177         \t0.1383         \t0.0992         \t0.1030         \t0.0870         \t0.0961         \n",
      "MSE            \t0.0019         \t0.0185         \t0.0073         \t0.0044         \t0.0037         \t0.0031         \t0.0033         \n",
      "MAE            \t0.0441         \t0.1089         \t0.0725         \t0.0529         \t0.0552         \t0.0466         \t0.0515         \n",
      "MRAE           \t0.0819         \t0.2177         \t0.1383         \t0.0992         \t0.1030         \t0.0870         \t0.0961         \n",
      "MKLD           \t0.0034         \t0.0339         \t0.0128         \t0.0076         \t0.0065         \t0.0053         \t0.0056         \n",
      "- Mixed test set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.0915         \t0.1238         \t0.1008         \t0.1005         \t0.0942         \t0.0945         \t0.0974         \n",
      "RAE            \t0.1699         \t0.2444         \t0.1959         \t0.1883         \t0.1765         \t0.1765         \t0.1817         \n",
      "MSE            \t0.0084         \t0.0238         \t0.0144         \t0.0132         \t0.0108         \t0.0100         \t0.0112         \n",
      "MAE            \t0.0915         \t0.1238         \t0.1008         \t0.1005         \t0.0942         \t0.0945         \t0.0974         \n",
      "MRAE           \t0.1699         \t0.2444         \t0.1959         \t0.1883         \t0.1765         \t0.1765         \t0.1817         \n",
      "MKLD           \t0.0147         \t0.0465         \t0.0262         \t0.0235         \t0.0190         \t0.0176         \t0.0199         \n"
     ]
    }
   ],
   "source": [
    "print('- Train set:')\n",
    "result_train = evaluate(collection=abs_dataset.training, n=[1,3,5,10, qp.environ['SAMPLE_SIZE']], quantifier=quantifier)\n",
    "\n",
    "print('- Val set:')\n",
    "result_train = evaluate(collection=abs_dataset.val, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)\n",
    "\n",
    "print('- Glaucoma test set:')\n",
    "result_test = evaluate(collection=glaucoma_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)\n",
    "\n",
    "print('- Neoplasm test set:')\n",
    "result_test = evaluate(collection=neoplasm_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)\n",
    "\n",
    "print('- Mixed test set:')\n",
    "result_test = evaluate(collection=mixed_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2784f5-4a51-4f26-aad1-ada9a77ba52d",
   "metadata": {},
   "source": [
    "The results seem promising, although the differences observed with the modified sampling strategy are substantial. This observation led us to investigate the effects of increasing the number of elements per batch, which allows us to notice a decrease in error values that tend to align more closely with the standard random sampling technique. This suggests that we might need to explore several solutions:\n",
    "\n",
    "- **Modify the Sampling Strategy During Training**: Adjusting the sampling strategy at training time could help the model learn the distribution of components within a single abstract.\n",
    "  \n",
    "- **Utilize Longer Documents**: We may consider working with longer documents containing more sentences. For instance, the [dataset suggested by Galassi](https://madoc.bib.uni-mannheim.de/46084/1/argmining-18-multi%20%289%29.pdf) contains entire papers annotated with argument components.\n",
    "\n",
    "Let’s observe how the current model behaves with some sample instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d4fcaf7-1cbd-4b79-b6de-ee445d388465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 9643663 - Text:\n",
      " To evaluate the efficacy and tolerability of 'Casodex' monotherapy (150 mg daily) for metastatic and locally advanced prostate cancer. A total of 1,453 patients with either confirmed metastatic disease (M1), or T3/T4 non-metastatic disease with elevated prostate-specific antigen (M0) were recruited into one of two identical, multicentre, randomised studies to compare 'Casodex' 150 mg/day with castration. The protocols allowed for combined analysis. At a median follow-up period of approximately 100 weeks for both studies, 'Casodex' 150 mg was found to be less effective than castration in patients with metastatic disease (M1) at entry (hazard ratio of 1.30 for time to death) with a difference in median survival of 6 weeks. In symptomatic M1 patients, 'Casodex' was associated with a statistically significant improvement in subjective response (70%) compared with castration (58%). Analysis of a validated quality-of-life questionnaire proved an advantage for 'Casodex' in sexual interest and physical capacity. 'Casodex' had a substantially lower incidence of hot flushes compared to castration (6-13% compared with 39-44%) and the most commonly reported adverse events were those expected for a potent antiandrogen. However, in patients with M0 disease at entry, the data are still immature with only 13% of M0 patients having died. An initial analysis of this immature data has suggested that the results in these patients may be different to those obtained in patients with M1 disease. A further survival analysis in patients with M0 disease is therefore planned when the data are more mature. 'Casodex' 150 mg is less effective than castration in patients with M1 disease. However, 'Casodex' has shown a benefit in terms of quality of life and subjective response when compared to castration and has an acceptable tolerability profile. Thus 'Casodex' 150 mg monotherapy is an option for patients with M1 prostate cancer for whom surgical or medical castration is not indicated or is not acceptable. \n",
      "\n",
      "\n",
      "**************************************************\n",
      "Component Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 13004.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tQuantification:\n",
      "\t\t# True distribution components: [Class 0 = 0.3077, Class 1 = 0.6923]\n",
      "\t\t# Estimated distribution components: [Class 0 = 0.3255, Class 1 = 0.6745]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|██████████████████████████████████████████████████████████████| 13/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t# Estimated distribution on tokenized components: [Class 0 = 0.3255, Class 1 = 0.6745]\n",
      "\n",
      "\t# AE: 0.0178\n",
      "\t# RAE: 0.0378\n",
      "\t# MSE: 0.0003\n",
      "\t# MAE: 0.0178\n",
      "\t# MRAE: 0.0378\n",
      "\t# MKLD: 0.0006\n",
      "\tClassification:\n",
      "\t\t# Ground truth components: (0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1)\n",
      "\t\t# Predicted components labels: [0 0 0 1 1 0 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "infer(test_set, indexer, comp_quantifier=quantifier, comp_classifier=cnn_classifier, filename=None, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a2a35-fee8-4458-98ae-7708ae3f554a",
   "metadata": {},
   "source": [
    "The predictions, although not perfect, appear promising. We compute the posterior estimations in two ways: first, by using sentence tokenization applied to the dataset, taking into account the annotations provided by *Cabrio* and *Villata*; second, by simply applying `sent_tokenize()` to each abstract. The error difference between the two approaches is minimal.\n",
    "\n",
    "### 4. QuaNet with custom training routine\n",
    "\n",
    "Now, let us attempt the first of the previously proposed modifications. We have adjusted the `QuaNet` training routine so that, in each epoch, every batch contains all the sentences from an abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ec734a6-8418-4940-8c3c-7394aa58c5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuaNetModule(\n",
      "  (lstm): LSTM(102, 64, batch_first=True, bidirectional=True)\n",
      "  (dropout): Dropout(p=0, inplace=False)\n",
      "  (ff_layers): ModuleList(\n",
      "    (0): Linear(in_features=136, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[QuaNet] epoch=1 [it=223/500]\ttr-mseloss=0.00495 tr-maeloss=0.05777\tval-mseloss=-1.00000 val-maeloss=C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[QuaNet] epoch=1 [it=398/500]\ttr-mseloss=0.00998 tr-maeloss=0.06780\tval-mseloss=-1.00000 val-maeloss=\n",
      "100%|██████████████████████████████████████████████████████████████| 387/387 [00:07<00:00, 53.74it/s]\n",
      "[QuaNet] epoch=2 [it=398/500]\ttr-mseloss=0.00928 tr-maeloss=0.06200\tval-mseloss=0.04083 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 387/387 [00:06<00:00, 61.53it/s]\n",
      "[QuaNet] epoch=3 [it=398/500]\ttr-mseloss=0.00867 tr-maeloss=0.05949\tval-mseloss=0.04072 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 387/387 [00:05<00:00, 72.61it/s]\n",
      "[QuaNet] epoch=4 [it=398/500]\ttr-mseloss=0.00838 tr-maeloss=0.05746\tval-mseloss=0.04057 val-maeloss=0\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:03<00:00, 118.31it/s]\n",
      "[QuaNet] epoch=5 [it=398/500]\ttr-mseloss=0.00837 tr-maeloss=0.05572\tval-mseloss=0.04067 val-maeloss=0\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:03<00:00, 125.79it/s]\n",
      "[QuaNet] epoch=6 [it=398/500]\ttr-mseloss=0.00810 tr-maeloss=0.05500\tval-mseloss=0.04077 val-maeloss=0\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:03<00:00, 128.57it/s]\n",
      "[QuaNet] epoch=7 [it=398/500]\ttr-mseloss=0.00799 tr-maeloss=0.05440\tval-mseloss=0.04198 val-maeloss=0\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:03<00:00, 120.01it/s]\n",
      "[QuaNet] epoch=8 [it=398/500]\ttr-mseloss=0.00780 tr-maeloss=0.05288\tval-mseloss=0.04184 val-maeloss=0\n",
      "100%|██████████████████████████████████████████████████████████████| 387/387 [00:05<00:00, 76.29it/s]\n",
      "[QuaNet] epoch=9 [it=398/500]\ttr-mseloss=0.00764 tr-maeloss=0.05162\tval-mseloss=0.04224 val-maeloss=0\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:02<00:00, 133.81it/s]\n",
      "[QuaNet] epoch=10 [it=398/500]\ttr-mseloss=0.00747 tr-maeloss=0.05001\tval-mseloss=0.04210 val-maeloss=\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:03<00:00, 123.40it/s]\n",
      "[QuaNet] epoch=11 [it=398/500]\ttr-mseloss=0.00747 tr-maeloss=0.04897\tval-mseloss=0.04419 val-maeloss=\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:03<00:00, 119.37it/s]\n",
      "[QuaNet] epoch=12 [it=398/500]\ttr-mseloss=0.00700 tr-maeloss=0.04537\tval-mseloss=0.04135 val-maeloss=\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:03<00:00, 107.54it/s]\n",
      "[QuaNet] epoch=13 [it=398/500]\ttr-mseloss=0.00690 tr-maeloss=0.04368\tval-mseloss=0.04341 val-maeloss=\n",
      "100%|█████████████████████████████████████████████████████████████| 387/387 [00:03<00:00, 128.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended by patience exhausted; loading best model parameters in ../checkpoints/components-custom\\Quanet-Components for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:775: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.quanet.load_state_dict(torch.load(checkpoint))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>QuaNetTrainerABS(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "                 classifier__device=device(type=&#x27;cpu&#x27;), classifier__drop_p=0.5,\n",
       "                 classifier__embedding_size=180, classifier__epochs=200,\n",
       "                 classifier__hidden_size=269,\n",
       "                 classifier__kernel_heights=[3, 5, 7], classifier__lr=0.001,\n",
       "                 classifier__padding_length=107, classifier__patience=10,\n",
       "                 classifier__repr_size=100, classifier__stride=1,\n",
       "                 classifier__weight_decay=0, qdrop_p=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;QuaNetTrainerABS<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>QuaNetTrainerABS(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "                 classifier__device=device(type=&#x27;cpu&#x27;), classifier__drop_p=0.5,\n",
       "                 classifier__embedding_size=180, classifier__epochs=200,\n",
       "                 classifier__hidden_size=269,\n",
       "                 classifier__kernel_heights=[3, 5, 7], classifier__lr=0.001,\n",
       "                 classifier__padding_length=107, classifier__patience=10,\n",
       "                 classifier__repr_size=100, classifier__stride=1,\n",
       "                 classifier__weight_decay=0, qdrop_p=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "QuaNetTrainerABS(classifier__batch_size=64, classifier__batch_size_test=512,\n",
       "                 classifier__device=device(type='cpu'), classifier__drop_p=0.5,\n",
       "                 classifier__embedding_size=180, classifier__epochs=200,\n",
       "                 classifier__hidden_size=269,\n",
       "                 classifier__kernel_heights=[3, 5, 7], classifier__lr=0.001,\n",
       "                 classifier__padding_length=107, classifier__patience=10,\n",
       "                 classifier__repr_size=100, classifier__stride=1,\n",
       "                 classifier__weight_decay=0, qdrop_p=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train QuaNet (alternatively, we can set fit_classifier=True and let QuaNet train the classifier)\n",
    "set_seed(42)\n",
    "\n",
    "quantifier_custom = QuaNetTrainerABS(cnn_classifier, qp.environ['SAMPLE_SIZE'], qdrop_p=0, device='cpu', checkpointdir='../checkpoints/components-custom', checkpointname='Quanet-Components')\n",
    "quantifier_custom.fit(abs_dataset.training, fit_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84d79122-4774-4458-9bb9-408c4b164847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.1687         \t0.1703         \t0.1612         \t0.1619         \t0.1648         \t0.1661         \t0.1661         \n",
      "RAE            \t0.3135         \t0.4065         \t0.3102         \t0.3056         \t0.3085         \t0.3104         \t0.3104         \n",
      "MSE            \t0.0284         \t0.0437         \t0.0345         \t0.0313         \t0.0299         \t0.0296         \t0.0293         \n",
      "MAE            \t0.1687         \t0.1703         \t0.1612         \t0.1619         \t0.1648         \t0.1661         \t0.1661         \n",
      "MRAE           \t0.3135         \t0.4065         \t0.3102         \t0.3056         \t0.3085         \t0.3104         \t0.3104         \n",
      "MKLD           \t0.0524         \t0.0791         \t0.0625         \t0.0570         \t0.0546         \t0.0541         \t0.0537         \n",
      "- Val set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.1678         \t0.2306         \t0.1582         \t0.1604         \t0.1601         \t0.1597         \t0.1590         \n",
      "RAE            \t0.3118         \t1.7814         \t0.3670         \t0.3161         \t0.3038         \t0.3003         \t0.3010         \n",
      "MSE            \t0.0281         \t0.0822         \t0.0371         \t0.0345         \t0.0311         \t0.0295         \t0.0303         \n",
      "MAE            \t0.1678         \t0.2306         \t0.1582         \t0.1604         \t0.1601         \t0.1597         \t0.1590         \n",
      "MRAE           \t0.3118         \t1.7814         \t0.3670         \t0.3161         \t0.3038         \t0.3003         \t0.3010         \n",
      "MKLD           \t0.0518         \t0.1684         \t0.0672         \t0.0622         \t0.0565         \t0.0537         \t0.0551         \n",
      "- Glaucoma test set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.1442         \t0.1257         \t0.1399         \t0.1410         \t0.1420         \t0.1418         \t0.1444         \n",
      "RAE            \t0.2692         \t0.2459         \t0.2652         \t0.2643         \t0.2658         \t0.2651         \t0.2705         \n",
      "MSE            \t0.0208         \t0.0243         \t0.0243         \t0.0226         \t0.0219         \t0.0213         \t0.0226         \n",
      "MAE            \t0.1442         \t0.1257         \t0.1399         \t0.1410         \t0.1420         \t0.1418         \t0.1444         \n",
      "MRAE           \t0.2692         \t0.2459         \t0.2652         \t0.2643         \t0.2658         \t0.2651         \t0.2705         \n",
      "MKLD           \t0.0385         \t0.0432         \t0.0441         \t0.0414         \t0.0403         \t0.0393         \t0.0415         \n",
      "- Neoplasm test set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.1977         \t0.1759         \t0.1909         \t0.1944         \t0.1957         \t0.1972         \t0.1976         \n",
      "RAE            \t0.3674         \t0.3608         \t0.3634         \t0.3649         \t0.3653         \t0.3677         \t0.3687         \n",
      "MSE            \t0.0391         \t0.0443         \t0.0417         \t0.0401         \t0.0394         \t0.0395         \t0.0399         \n",
      "MAE            \t0.1977         \t0.1759         \t0.1909         \t0.1944         \t0.1957         \t0.1972         \t0.1976         \n",
      "MRAE           \t0.3674         \t0.3608         \t0.3634         \t0.3649         \t0.3653         \t0.3677         \t0.3687         \n",
      "MKLD           \t0.0714         \t0.0789         \t0.0753         \t0.0729         \t0.0717         \t0.0720         \t0.0726         \n",
      "- Mixed test set:\n",
      "Error Metric\tStandard\tByDoc (n=1)\tByDoc (n=3)\tByDoc (n=5)\tByDoc (n=10)\tByDoc (n=13)\tByDoc (n=15)\n",
      "------------\t--------\t-----------\t-----------\t-----------\t------------\t------------\t------------\n",
      "AE             \t0.1788         \t0.1579         \t0.1793         \t0.1749         \t0.1772         \t0.1780         \t0.1797         \n",
      "RAE            \t0.3321         \t0.3262         \t0.3515         \t0.3283         \t0.3314         \t0.3324         \t0.3351         \n",
      "MSE            \t0.0320         \t0.0373         \t0.0400         \t0.0346         \t0.0339         \t0.0338         \t0.0339         \n",
      "MAE            \t0.1788         \t0.1579         \t0.1793         \t0.1749         \t0.1772         \t0.1780         \t0.1797         \n",
      "MRAE           \t0.3321         \t0.3262         \t0.3515         \t0.3283         \t0.3314         \t0.3324         \t0.3351         \n",
      "MKLD           \t0.0586         \t0.0665         \t0.0722         \t0.0627         \t0.0617         \t0.0617         \t0.0618         \n"
     ]
    }
   ],
   "source": [
    "print('- Train set:')\n",
    "result_train = evaluate(collection=abs_dataset.training, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier_custom)\n",
    "\n",
    "print('- Val set:')\n",
    "result_train = evaluate(collection=abs_dataset.val, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier_custom)\n",
    "\n",
    "print('- Glaucoma test set:')\n",
    "result_test = evaluate(collection=glaucoma_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier_custom)\n",
    "\n",
    "print('- Neoplasm test set:')\n",
    "result_test = evaluate(collection=neoplasm_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier_custom)\n",
    "\n",
    "print('- Mixed test set:')\n",
    "result_test = evaluate(collection=mixed_collection, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccbb8325-93d5-4060-8b57-40343ee1d59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard QuaNet:\n",
      "\n",
      "**************************************************\n",
      "Component Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 13041.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tQuantification:\n",
      "\t\t# True distribution components: [Class 0 = 0.3077, Class 1 = 0.6923]\n",
      "\t\t# Estimated distribution components: [Class 0 = 0.3255, Class 1 = 0.6745]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|███████████████████████████████████████████████████| 13/13 [00:00<00:00, 12985.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t# Estimated distribution on tokenized components: [Class 0 = 0.3255, Class 1 = 0.6745]\n",
      "\n",
      "\t# AE: 0.0178\n",
      "\t# RAE: 0.0378\n",
      "\t# MSE: 0.0003\n",
      "\t# MAE: 0.0178\n",
      "\t# MRAE: 0.0378\n",
      "\t# MKLD: 0.0006\n",
      "\tClassification:\n",
      "\t\t# Ground truth components: (0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1)\n",
      "\t\t# Predicted components labels: [0 0 0 1 1 0 1 1 1 1 1 1 1]\n",
      "Custom approach QuaNet:\n",
      "\n",
      "**************************************************\n",
      "Component Analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|██████████████████████████████████████████████████████████████| 13/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tQuantification:\n",
      "\t\t# True distribution components: [Class 0 = 0.3077, Class 1 = 0.6923]\n",
      "\t\t# Estimated distribution components: [Class 0 = 0.6522, Class 1 = 0.3478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|██████████████████████████████████████████████████████████████| 13/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t# Estimated distribution on tokenized components: [Class 0 = 0.6522, Class 1 = 0.3478]\n",
      "\n",
      "\t# AE: 0.3445\n",
      "\t# RAE: 0.7333\n",
      "\t# MSE: 0.1187\n",
      "\t# MAE: 0.3445\n",
      "\t# MRAE: 0.7333\n",
      "\t# MKLD: 0.2106\n",
      "\tClassification:\n",
      "\t\t# Ground truth components: (0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1)\n",
      "\t\t# Predicted components labels: [0 0 0 1 1 0 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "filename = random.choice(test_collection.filenames)\n",
    "\n",
    "print('Standard QuaNet:')\n",
    "infer(test_set, indexer, comp_quantifier=quantifier, comp_classifier=cnn_classifier, rel_quantifier=None, filename=filename, show_text=False, use_tokenizer=True)\n",
    "\n",
    "print('Custom approach QuaNet:')\n",
    "infer(test_set, indexer, comp_quantifier=quantifier_custom, comp_classifier=cnn_classifier, rel_quantifier=None, filename=filename, show_text=False, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd82fb-1046-4ecf-8296-b1bdd1389223",
   "metadata": {},
   "source": [
    "The results are quite similar, slightly better with the standard sampling technique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
