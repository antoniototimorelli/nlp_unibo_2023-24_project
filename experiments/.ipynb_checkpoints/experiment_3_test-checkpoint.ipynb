{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b32ce4d-112a-4c2b-814b-7925eee97ce5",
   "metadata": {},
   "source": [
    "# Experiment 3 - Sentence-wise\n",
    "We are now testing a different approach. Instead of considering the document as a whole and using the number of arguments as the label, we split each abstract into sentences and label them as either *Non-component* or *Component*, corresponding to 0 and 1 respectively.\n",
    "\n",
    "During inference, we implement a sampling technique that processes all the sentences forming an abstract. Our goal is to determine the percentage of arguments' components each abstract contains. Afterward, multiple approaches can be explored to complete the task. One approach could involve using a regressor to learn the number of arguments in an abstract based on the sentence distribution. Another option is to apply a model like in the first experiment, but this time using the number of relations as the label. The resulting data, combined with the premise/claim distribution, can be used to estimate the number of arguments.\n",
    "\n",
    "This experiment is still in progress. To-do list:\n",
    "- Ensure the sentences are split fairly; ✅\n",
    "- Train using the same sampling technique as during inference;\n",
    "- Fix the sampling technique and inference functions; ✅\n",
    "- Investigate the proposed approaches;\n",
    "\n",
    "BISOGNA RISCRIVERE IL METODO EPOCH DI QUANETTRAINER PER CAMBIARE LA STRATEGIA DI SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee6643c-ae6d-4d48-ad83-18f6eb1638bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Antonio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Antonio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from experiment_3_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4b71a-b9f7-4394-ae94-5678e5c8fd67",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "Here we preprocess our data by splitting the abstracts into sentences. Each sentence is then labeled as either being a `Component` of an argument, labeled as `1`, or `Not a component`, labeled as `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471b1a98-8b85-4454-bafe-31ed4ad14d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_brat_dataset('../data/train/neoplasm_train') \n",
    "val_set = read_brat_dataset('../data/dev/neoplasm_dev')\n",
    "\n",
    "glaucoma_test = read_brat_dataset('../data/test/glaucoma_test')\n",
    "neoplasm_test = read_brat_dataset('../data/test/neoplasm_test')\n",
    "mixed_test = read_brat_dataset('../data/test/mixed_test')\n",
    "\n",
    "test_set = glaucoma_test + neoplasm_test + mixed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a2cffa-f391-4e11-bbe0-32124242d23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "\tLabel 0: 2378 samples\n",
      "\tLabel 1: 2267 samples\n",
      "\n",
      "\tThere are 2 different labels in the train set -> [0, 1]\n",
      "\tAverage number of sentences per file in train set: 13\n",
      "\tMax sentence length: 107\n",
      "\tAverage components per file: 6.48\n",
      "\tAverage non-components per file: 6.79\n",
      "\n",
      "- Val set:\n",
      "\tLabel 0: 382 samples\n",
      "\tLabel 1: 326 samples\n",
      "\n",
      "\tThere are 2 different labels in the val set -> [0, 1]\n",
      "\tAverage number of sentences per file in val set: 14\n",
      "\tMax sentence length: 91\n",
      "\tAverage components per file: 6.52\n",
      "\tAverage non-components per file: 7.64\n",
      "\n",
      "- Test set:\n",
      "\tLabel 0: 1948 samples\n",
      "\tLabel 1: 1880 samples\n",
      "\n",
      "\tThere are 2 different labels in the test set -> [0, 1]\n",
      "\tAverage number of sentences per file in test set: 14\n",
      "\tMax sentence length: 91\n",
      "\tAverage components per file: 6.99\n",
      "\tAverage non-components per file: 7.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_counts_train, avg_sentences_per_file_train = compute_dataset_statistics(train_set, dataset_name=\"train\")\n",
    "label_counts_val, avg_sentences_per_file_val = compute_dataset_statistics(val_set, dataset_name=\"val\")\n",
    "label_counts_test, avg_sentences_per_file_test = compute_dataset_statistics(test_set, dataset_name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b11b11-4c24-4a19-8721-f132b39f4705",
   "metadata": {},
   "source": [
    "Follows an example of what our dictionary looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb562ae9-ef26-4bce-bd30-d7c8e7fb543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 17075117 - Text:\n",
      " In the randomized, multinational phase II/III trial (V325) of untreated advanced gastric cancer patients, the phase II part selected docetaxel, cisplatin, and fluorouracil (DCF) over docetaxel and cisplatin for comparison against cisplatin and fluorouracil (CF; reference regimen) in the phase III part. Advanced gastric cancer patients were randomly assigned to docetaxel 75 mg/m2 and cisplatin 75 mg/m2 (day 1) plus fluorouracil 750 mg/m2/d (days 1 to 5) every 3 weeks or cisplatin 100 mg/m2 (day 1) plus fluorouracil 1,000 mg/m2/d (days 1 to 5) every 4 weeks. The primary end point was time-to-progression (TTP). In 445 randomly assigned and treated patients (DCF = 221; CF = 224), TTP was longer with DCF versus CF (32% risk reduction; log-rank P < .001). Overall survival was longer with DCF versus CF (23% risk reduction; log-rank P = .02). Two-year survival rate was 18% with DCF and 9% with CF. Overall response rate was higher with DCF (chi2 P = .01). Grade 3 to 4 treatment-related adverse events occurred in 69% (DCF) v 59% (CF) of patients. Frequent grade 3 to 4 toxicities for DCF v CF were: neutropenia (82% v 57%), stomatitis (21% v 27%), diarrhea (19% v 8%), lethargy (19% v 14%). Complicated neutropenia was more frequent with DCF than CF (29% v 12%). Adding docetaxel to CF significantly improved TTP, survival, and response rate in gastric cancer patients, but resulted in some increase in toxicity. Incorporation of docetaxel, as in DCF or with other active drug(s), is a new therapy option for patients with untreated advanced gastric cancer. \n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| sentence                                                                                                                 | label               |\n",
      "+==========================================================================================================================+=====================+\n",
      "| In the randomized, multinational phase II/III trial (V325) of untreated advanced gastric cancer patients, the phase II   | Not a component (0) |\n",
      "| part selected docetaxel, cisplatin, and fluorouracil (DCF) over docetaxel and cisplatin for comparison against cisplatin |                     |\n",
      "| and fluorouracil (CF; reference regimen) in the phase III part.                                                          |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Advanced gastric cancer patients were randomly assigned to docetaxel 75 mg/m2 and cisplatin 75 mg/m2 (day 1) plus        | Not a component (0) |\n",
      "| fluorouracil 750 mg/m2/d (days 1 to 5) every 3 weeks or cisplatin 100 mg/m2 (day 1) plus fluorouracil 1,000 mg/m2/d      |                     |\n",
      "| (days 1 to 5) every 4 weeks.                                                                                             |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| The primary end point was time-to-progression (TTP).                                                                     | Not a component (0) |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| In 445 randomly assigned and treated patients (DCF = 221; CF = 224), TTP was longer with DCF versus CF (32% risk         | Not a component (0) |\n",
      "| reduction; log-rank P < .001).                                                                                           |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Overall survival was longer with DCF versus CF (23% risk reduction; log-rank P = .02).                                   | Component (1)       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Two-year survival rate was 18% with DCF and 9% with CF.                                                                  | Component (1)       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Overall response rate was higher with DCF (chi2 P = .01).                                                                | Component (1)       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Grade 3 to 4 treatment-related adverse events occurred in 69% (DCF) v 59% (CF) of patients.                              | Component (1)       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Frequent grade 3 to 4 toxicities for DCF v CF were: neutropenia (82% v 57%), stomatitis (21% v 27%), diarrhea (19% v     | Component (1)       |\n",
      "| 8%), lethargy (19% v 14%).                                                                                               |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Complicated neutropenia was more frequent with DCF than CF (29% v 12%).                                                  | Component (1)       |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Adding docetaxel to CF significantly improved TTP, survival, and response rate in gastric cancer patients, but resulted  | Component (1)       |\n",
      "| in some increase in toxicity.                                                                                            |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
      "| Incorporation of docetaxel, as in DCF or with other active drug(s), is a new therapy option for patients with untreated  | Component (1)       |\n",
      "| advanced gastric cancer.                                                                                                 |                     |\n",
      "+--------------------------------------------------------------------------------------------------------------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "display_file_info(train_set, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df97bf69-2965-4927-ac08-b3cdee1b6238",
   "metadata": {},
   "source": [
    "Next, we create the dataset `FilenameLabelledCollection`, which inherits from `QuaPy`'s `LabelledCollection` class. This allows us to keep track of the filenames corresponding to each abstract to which the sentences belong. The `index` method is also modified to return two `FilenameLabelledCollection` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e4d204-eea2-4378-b97d-06bc31fddfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collection = FilenameLabelledCollection([data['sentence'] for data in train_set], \n",
    "                                                 [data['label'] for data in train_set], \n",
    "                                                 [data['filename'] for data in train_set])\n",
    "\n",
    "val_collection = FilenameLabelledCollection([data['sentence'] for data in val_set], \n",
    "                                                 [data['label'] for data in val_set], \n",
    "                                                 [data['filename'] for data in val_set])\n",
    "\n",
    "test_collection = FilenameLabelledCollection([data['sentence'] for data in test_set], \n",
    "                                                 [data['label'] for data in test_set], \n",
    "                                                 [data['filename'] for data in test_set])\n",
    "\n",
    "glaucoma_collection = FilenameLabelledCollection([data['sentence'] for data in glaucoma_test], \n",
    "                                                 [data['label'] for data in glaucoma_test], \n",
    "                                                 [data['filename'] for data in glaucoma_test])\n",
    "\n",
    "neoplasm_collection = FilenameLabelledCollection([data['sentence'] for data in neoplasm_test], \n",
    "                                                 [data['label'] for data in neoplasm_test], \n",
    "                                                 [data['filename'] for data in neoplasm_test])\n",
    "\n",
    "mixed_collection = FilenameLabelledCollection([data['sentence'] for data in mixed_test], \n",
    "                                                 [data['label'] for data in mixed_test], \n",
    "                                                 [data['filename'] for data in mixed_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae1e54e-10bd-40ed-8632-af7263a83863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|███████████████████████████████████████████████| 4645/4645 [00:00<00:00, 27156.62it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'val'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create and index the dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m abs_dataset \u001b[38;5;241m=\u001b[39m Dataset(train_collection, test_collection)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabs_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Index the test collections\u001b[39;00m\n\u001b[0;32m      8\u001b[0m index(glaucoma_collection, indexer, fit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:276\u001b[0m, in \u001b[0;36mindex\u001b[1;34m(data, indexer, inplace, fit, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Fit or transform based on `fit` parameter\u001b[39;00m\n\u001b[0;32m    275\u001b[0m training_index \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mfit_transform(data\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39minstances) \u001b[38;5;28;01mif\u001b[39;00m fit \u001b[38;5;28;01melse\u001b[39;00m indexer\u001b[38;5;241m.\u001b[39mtransform(data\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39minstances)\n\u001b[1;32m--> 276\u001b[0m val_index \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval\u001b[49m\u001b[38;5;241m.\u001b[39minstances)\n\u001b[0;32m    277\u001b[0m test_index \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mtransform(data\u001b[38;5;241m.\u001b[39mtest\u001b[38;5;241m.\u001b[39minstances)\n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# Convert to numpy arrays\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'val'"
     ]
    }
   ],
   "source": [
    "indexer = qp.data.preprocessing.IndexTransformer(min_df=1)\n",
    "\n",
    "# Create and index the dataset\n",
    "abs_dataset = CustomDataset(training=train_collection, test=test_collection, val=val_collection)\n",
    "index(abs_dataset, indexer, inplace=True)\n",
    "\n",
    "# Index the test collections\n",
    "index(glaucoma_collection, indexer, fit=False, inplace=True)\n",
    "index(neoplasm_collection, indexer, fit=False, inplace=True)\n",
    "index(mixed_collection, indexer, fit=False, inplace=True)\n",
    "\n",
    "qp.environ['SAMPLE_SIZE'] = avg_sentences_per_file_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97b60c-b816-49b5-8b56-8a0a4f499602",
   "metadata": {},
   "source": [
    "### 2. Classifier\n",
    "`QuaNet` requires a classifier that can provide embedded representations of the inputs. In the original paper, `QuaNet` was tested using an `LSTM` as the base classifier; as `QuaPy`'s authors show in their [example](https://hlt-isti.github.io/QuaPy/manuals/methods.html#the-quanet-neural-network), we will use an instantiation of `QuaNet` that employs a `CNN` as a probabilistic classifier, taking its last layer representation as the document embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5685f8-0e3c-4a74-96dd-9d2359ccfe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 13:51:07,611] A new study created in memory with name: no-name-512b1d1c-904d-4a95-bd09-dffc24c34565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 0 with parameters:\n",
      "    Embedding size: 191 - Hidden size: 269\n",
      "    Optimizer: Adam (lr: 7.429468607521178e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 11, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=33 tr-loss=0.02935 tr-acc=99.41% tr-macroF1=99.41% patience=1/5 val-loss=0.43C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=33 tr-loss=0.02935 tr-acc=99.41% tr-macroF1=99.41% patience=1/5 val-loss=0.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 28\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 14:03:52,321] Trial 0 finished with value: 0.84483459057097 and parameters: {'embedding_size': 191, 'hidden_size': 269, 'optimizer': 'Adam', 'lr': 7.429468607521178e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 11, 'T_mult': 3}. Best is trial 0 with value: 0.84483459057097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3879459351301193 - Best f1 on validation set: 0.84483459057097\n",
      "Starting trial 1 with parameters:\n",
      "    Embedding size: 106 - Hidden size: 278\n",
      "    Optimizer: Adam (lr: 4.496657263459981e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 11})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=61 tr-loss=0.11613 tr-acc=96.00% tr-macroF1=95.99% patience=1/5 val-loss=0.39C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=61 tr-loss=0.11613 tr-acc=96.00% tr-macroF1=95.99% patience=1/5 val-loss=0.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 56\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 14:18:56,683] Trial 1 finished with value: 0.8256494720010918 and parameters: {'embedding_size': 106, 'hidden_size': 278, 'optimizer': 'Adam', 'lr': 4.496657263459981e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 11}. Best is trial 0 with value: 0.84483459057097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.3825526237487793 - Best f1 on validation set: 0.8256494720010918\n",
      "Starting trial 2 with parameters:\n",
      "    Embedding size: 163 - Hidden size: 281\n",
      "    Optimizer: Adam (lr: 2.1239254662200434e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 12})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=58 tr-loss=0.22415 tr-acc=92.47% tr-macroF1=92.47% patience=1/5 val-loss=0.40C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=58 tr-loss=0.22415 tr-acc=92.47% tr-macroF1=92.47% patience=1/5 val-loss=0.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 53\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 14:38:59,793] Trial 2 finished with value: 0.8053998632946002 and parameters: {'embedding_size': 163, 'hidden_size': 281, 'optimizer': 'Adam', 'lr': 2.1239254662200434e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 0 with value: 0.84483459057097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4101656898856163 - Best f1 on validation set: 0.8053998632946002\n",
      "Starting trial 3 with parameters:\n",
      "    Embedding size: 170 - Hidden size: 298\n",
      "    Optimizer: Adam (lr: 1.806464467046538e-05) - Scheduler: CosineAnnealingLR (params: {'T_max': 12})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=55 tr-loss=0.26673 tr-acc=90.45% tr-macroF1=90.44% patience=1/5 val-loss=0.41C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=55 tr-loss=0.26673 tr-acc=90.45% tr-macroF1=90.44% patience=1/5 val-loss=0.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 50\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 14:59:15,080] Trial 3 finished with value: 0.8064693187048302 and parameters: {'embedding_size': 170, 'hidden_size': 298, 'optimizer': 'Adam', 'lr': 1.806464467046538e-05, 'scheduler': 'CosineAnnealingLR', 'T_max': 12}. Best is trial 0 with value: 0.84483459057097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4288260191679001 - Best f1 on validation set: 0.8064693187048302\n",
      "Starting trial 4 with parameters:\n",
      "    Embedding size: 159 - Hidden size: 284\n",
      "    Optimizer: Adam (lr: 1.1268958994171102e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 9, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=53 tr-loss=0.41050 tr-acc=82.89% tr-macroF1=82.86% patience=1/5 val-loss=0.49C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=53 tr-loss=0.41050 tr-acc=82.89% tr-macroF1=82.86% patience=1/5 val-loss=0.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 48\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 15:16:42,187] Trial 4 finished with value: 0.783466862023817 and parameters: {'embedding_size': 159, 'hidden_size': 284, 'optimizer': 'Adam', 'lr': 1.1268958994171102e-05, 'scheduler': 'CosineAnnealingWarmRestarts', 'T_0': 9, 'T_mult': 3}. Best is trial 0 with value: 0.84483459057097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.4912678897380829 - Best f1 on validation set: 0.783466862023817\n",
      "Starting trial 5 with parameters:\n",
      "    Embedding size: 173 - Hidden size: 256\n",
      "    Optimizer: Adam (lr: 0.00043601308174167394) - Scheduler: CosineAnnealingLR (params: {'T_max': 9})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=17 tr-loss=0.01548 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.50C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=17 tr-loss=0.01548 tr-acc=99.55% tr-macroF1=99.55% patience=1/5 val-loss=0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 12\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 15:22:32,221] Trial 5 finished with value: 0.8491569287709215 and parameters: {'embedding_size': 173, 'hidden_size': 256, 'optimizer': 'Adam', 'lr': 0.00043601308174167394, 'scheduler': 'CosineAnnealingLR', 'T_max': 9}. Best is trial 5 with value: 0.8491569287709215.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.45491214841604233 - Best f1 on validation set: 0.8491569287709215\n",
      "Starting trial 6 with parameters:\n",
      "    Embedding size: 151 - Hidden size: 258\n",
      "    Optimizer: Adam (lr: 0.0005346212833028434) - Scheduler: CosineAnnealingLR (params: {'T_max': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=28 tr-loss=0.01646 tr-acc=99.41% tr-macroF1=99.41% patience=1/5 val-loss=0.66C:\\Users\\Antonio\\Documents\\UniBO\\NLP\\project\\nlp_unibo_2023-24_project\\experiments_antonio\\experiment_3_code.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(self.checkpointpath))\n",
      "[CNNnet] training epoch=28 tr-loss=0.01646 tr-acc=99.41% tr-macroF1=99.41% patience=1/5 val-loss=0.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ended by patience exhausted; loading best model parameters from ../checkpoint/components/classifier_net.dat from epoch 23\n",
      "Performing a final training pass over the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-30 15:31:24,667] Trial 6 finished with value: 0.8549063962891201 and parameters: {'embedding_size': 151, 'hidden_size': 258, 'optimizer': 'Adam', 'lr': 0.0005346212833028434, 'scheduler': 'CosineAnnealingLR', 'T_max': 3}. Best is trial 6 with value: 0.8549063962891201.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training complete] - Best loss on validation set: 0.5792213827371597 - Best f1 on validation set: 0.8549063962891201\n",
      "Starting trial 7 with parameters:\n",
      "    Embedding size: 130 - Hidden size: 292\n",
      "    Optimizer: Adam (lr: 2.3918643253843326e-05) - Scheduler: CosineAnnealingWarmRestarts (params: {'T_0': 11, 'T_mult': 3})\n",
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] epoch=27 lr=0.00002 tr-loss=0.41912 tr-F1=82.02% patience=4/5 val-loss=0.49307 val-F1=77.52%"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, abs_dataset), n_trials=50)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Macro F1 Score: {trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338f136-88cd-4033-a53e-c034be38ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "embedding_size = 161\n",
    "hidden_size = 262   \n",
    "\n",
    "cnn_module = CNNnet(\n",
    "    abs_dataset.vocabulary_size,\n",
    "    abs_dataset.training.n_classes,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size\n",
    ")\n",
    "\n",
    "optimizer = Adam(cnn_module.parameters(), lr=8e-4)\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=4)\n",
    "\n",
    "cnn_classifier = ScheduledNeuralClassifierTrainer(\n",
    "    cnn_module,\n",
    "    lr_scheduler=scheduler,\n",
    "    optim = optimizer,\n",
    "    device='cpu',\n",
    "    checkpointpath='../checkpoint/relations/classifier_net.dat',\n",
    "    padding_length=107,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "cnn_classifier.fit(*abs_dataset.training.Xy, *abs_dataset.val.Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9680a2-8b2f-41e7-8fd3-139ee4e96e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=12 tr-loss=0.00068 tr-acc=100.00% tr-macroF1=100.00% patience=1/10 val-loss=1C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(checkpoint))\n",
      "[CNNnet] training epoch=12 tr-loss=0.00068 tr-acc=100.00% tr-macroF1=100.00% patience=1/10 val-loss=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended by patience exhasted; loading best model parameters in ../checkpoint/components/classifier_net.dat for epoch 2\n",
      "performing one training pass over the validation set...\n",
      "[done]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<quapy.classification.neural.NeuralClassifierTrainer at 0x272742a1dc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the text classifier:\n",
    "# cnn_module = CNNnet(abs_dataset.vocabulary_size, abs_dataset.training.n_classes, padding=1)\n",
    "cnn_module = CNNnet(abs_dataset.vocabulary_size, abs_dataset.training.n_classes)\n",
    "cnn_classifier = NeuralClassifierTrainer(cnn_module, device='cpu', checkpointpath='../checkpoint/components/classifier_net.dat')\n",
    "cnn_classifier.fit(*abs_dataset.training.Xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "634a48c7-e2df-4b40-a2b5-b38401810af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set:\n",
      "\tF1: 0.9517755830616709\n",
      "\tAccuracy: 0.9518027274425556\n",
      "- Composed test set:\n",
      "\tF1: 0.8653922324991364\n",
      "\tAccuracy: 0.8654649947753396\n",
      "- Glaucoma test set:\n",
      "\tF1: 0.8775207855977778\n",
      "\tAccuracy: 0.8781055900621118\n",
      "- Neoplasm test set:\n",
      "\tF1: 0.8648209791345296\n",
      "\tAccuracy: 0.8648648648648649\n",
      "- Mixed test set:\n",
      "\tF1: 0.8525908213408213\n",
      "\tAccuracy: 0.8526490066225165\n"
     ]
    }
   ],
   "source": [
    "f1_train = 1-qp.error.f1e(abs_dataset.training.labels, cnn_classifier.predict(abs_dataset.training.instances))\n",
    "accuracy_train = 1-qp.error.acce(abs_dataset.training.labels, cnn_classifier.predict(abs_dataset.training.instances))\n",
    "print('- Train set:')\n",
    "print(f'\\tF1: {f1_train}')    \n",
    "print(f'\\tAccuracy: {accuracy_train}')    \n",
    "\n",
    "f1_test = 1-qp.error.f1e(abs_dataset.test.labels, cnn_classifier.predict(abs_dataset.test.instances))\n",
    "accuracy_test = 1-qp.error.acce(abs_dataset.test.labels, cnn_classifier.predict(abs_dataset.test.instances))\n",
    "print('- Composed test set:')\n",
    "print(f'\\tF1: {f1_test}')    \n",
    "print(f'\\tAccuracy: {accuracy_test}')    \n",
    "\n",
    "f1_test_glaucoma = 1-qp.error.f1e(glaucoma_collection.labels, cnn_classifier.predict(glaucoma_collection.instances))\n",
    "accuracy_test_glaucoma = 1-qp.error.acce(glaucoma_collection.labels, cnn_classifier.predict(glaucoma_collection.instances))\n",
    "\n",
    "print('- Glaucoma test set:')\n",
    "print(f'\\tF1: {f1_test_glaucoma}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_glaucoma}')    \n",
    "\n",
    "f1_test_neoplasm = 1-qp.error.f1e(neoplasm_collection.labels, cnn_classifier.predict(neoplasm_collection.instances))\n",
    "accuracy_test_neoplasm = 1-qp.error.acce(neoplasm_collection.labels, cnn_classifier.predict(neoplasm_collection.instances))\n",
    "\n",
    "print('- Neoplasm test set:')\n",
    "print(f'\\tF1: {f1_test_neoplasm}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_neoplasm}')\n",
    "\n",
    "f1_test_mixed = 1-qp.error.f1e(mixed_collection.labels, cnn_classifier.predict(mixed_collection.instances))\n",
    "accuracy_test_mixed = 1-qp.error.acce(mixed_collection.labels, cnn_classifier.predict(mixed_collection.instances))\n",
    "\n",
    "print('- Mixed test set:')\n",
    "print(f'\\tF1: {f1_test_mixed}')    \n",
    "print(f'\\tAccuracy: {accuracy_test_mixed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc37da-6239-4833-9c63-03cac51fdb78",
   "metadata": {},
   "source": [
    "### 3. QuaNet \n",
    "The results are solid, let's move onto `QuaNet` training phase. `QuaNet` observes the classification predictions to learn higher-order *quantification embeddings*, which are then refined by incorporating quantification predictions of simple classify-and-count-like methods.\n",
    "\n",
    "![architecture](./images/quanet_architecture.png)\n",
    "\n",
    "The QuaNet architecture (see Figure 1) consists of two main components: a **recurrent component** and a **fully connected component**.\n",
    "\n",
    "#### 3.1 Recurrent Component: Bidirectional LSTM\n",
    "- The core of the model is a **Bidirectional LSTM** (Long Short-Term Memory), a type of recurrent neural network. \n",
    "- The LSTM receives as input a **list of pairs** $⟨Pr(c|x), \\vec{x}⟩$, where:\n",
    "  - $Pr(c|x)$ is the probability that a classifier $h$ assigns class $c$ to document $x$.\n",
    "  - $\\vec{x}$ is the **document embedding**, a vector representing the document's content.\n",
    "- The list is **sorted by the value of $Pr(c|x)$**, meaning the documents are arranged from least to most likely to belong to class $c$.\n",
    "  \n",
    "The **intuition** behind this approach is that the LSTM will \"learn to count\" positive and negative examples. By observing the ordered sequence of probabilities, the LSTM should learn to recognize the point where the documents switch from negative to positive examples. The document embedding $\\vec{x}$ helps the LSTM assign different importance to each document when making its prediction.\n",
    "\n",
    "The output of the LSTM is called a **quantification embedding**—a dense vector representing the information about the quantification task learned from the input data.\n",
    "\n",
    "#### 3.2 Fully Connected Component\n",
    "- The vector returned by the LSTM is combined with additional information, specifically **quantification-related statistics**:\n",
    "  - $\\hat{p}_c^{CC}(D)$, $\\hat{p}_c^{ACC}(D)$, $\\hat{p}_c^{PCC}(D)$, and $\\hat{p}_c^{PACC}(D)$, which are quantification predictions from different methods.\n",
    "  - $tpr_b$, $fpr_b$, $tpr_s$, and $fpr_s$, aggregate statistics related to true positive and false positive rates, which are easy to compute from the classifier $h$ using a validation set.\n",
    "\n",
    "This combined vector then passes through the second part of the architecture, which is made up of **fully connected layers** with **ReLU activations**. These layers adjust the quantification embedding using the additional statistics from the classifier to improve the accuracy of the quantification.\n",
    "\n",
    "The final output is a prediction $\\hat{p}_c^{QuaNet}(c|D)$, which represents the probability of class $c$ for the dataset $D$, produced by a **softmax layer**.\n",
    "\n",
    "QuaNet could use quantification predictions from many methods, but it focuses on those that are **computationally efficient** (like CC, ACC, PCC, and PACC). This ensures that the process remains fast while still providing sufficient information for accurate predictions.\n",
    "\n",
    "### Details\n",
    "\n",
    "| Layer | Type | Dimensions | Activation | Dropout |\n",
    "|---|---|---|---|---|\n",
    "| Input | LSTM | 128 | N/A | N/A |\n",
    "| Dense 1 | Dense | 1024 | ReLU | 0.5 |\n",
    "| Dense 2 | Dense | 512 | ReLU | 0.5 |\n",
    "| Output | Dense | 2 | Softmax | N/A |\n",
    "\n",
    "- The LSTM has **64 hidden dimensions**, and since it’s bidirectional, the final LSTM output has **128 dimensions**.\n",
    "- This LSTM output is concatenated with the **8 quantification statistics** (giving a total of 136 dimensions), which is then fed into:\n",
    "  - **Two dense layers** with **1,024** and **512 dimensions**, each using **ReLU activation** and **0.5 dropout**.\n",
    "  - Finally, the output is passed through a **softmax layer** of size 2 to make the final class prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc527bb1-dec8-4f00-b777-f3d407f93165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train QuaNet (alternatively, we can set fit_classifier=True and let QuaNet train the classifier)\n",
    "quantifier = QuaNet(cnn_classifier, qp.environ['SAMPLE_SIZE'], device='cpu', checkpointdir='../checkpoint/components', checkpointname='Quanet-Components')\n",
    "quantifier.fit(abs_dataset.training, fit_classifier=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0de8d-011c-4c6f-bd31-62fe0c9da80e",
   "metadata": {},
   "source": [
    "We wrapped `QuaPy`'s error evaluation function and manually modified how each sample is selected; we adjusted the sampling strategy to work with batches where the batch size is equal to the number of sentences that compose each abstract. This allows us to select the entire document based on the filename associated with each sentence. We will also evaluate the results using the standard random sampling technique, where sentences from different abstracts are grouped into the same batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f9f37-f3ef-42c1-a148-84dffb5558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results on training set:')\n",
    "result_train = evaluate(collection=abs_dataset.training, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)\n",
    "\n",
    "print('\\nResults on test set:')\n",
    "result_test = evaluate(collection=abs_dataset.test, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2784f5-4a51-4f26-aad1-ada9a77ba52d",
   "metadata": {},
   "source": [
    "The results seem promising, although the differences observed with the modified sampling strategy are substantial. This observation led us to investigate the effects of increasing the number of elements per batch, which allows us to notice a decrease in error values that tend to align more closely with the standard random sampling technique. This suggests that we might need to explore several solutions:\n",
    "\n",
    "- **Modify the Sampling Strategy During Training**: Adjusting the sampling strategy at training time could help the model learn the distribution of components within a single abstract.\n",
    "  \n",
    "- **Utilize Longer Documents**: We may consider working with longer documents containing more sentences. For instance, the [dataset suggested by Galassi](https://madoc.bib.uni-mannheim.de/46084/1/argmining-18-multi%20%289%29.pdf) contains entire papers annotated with argument components.\n",
    "\n",
    "Let’s observe how the current model behaves with some sample instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4fcaf7-1cbd-4b79-b6de-ee445d388465",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(test_set, indexer, comp_quantifier=quantifier, comp_classifier=cnn_classifier, filename=None, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a2a35-fee8-4458-98ae-7708ae3f554a",
   "metadata": {},
   "source": [
    "The predictions, although not perfect, appear promising. We compute the posterior estimations in two ways: first, by using sentence tokenization applied to the dataset, taking into account the annotations provided by *Cabrio* and *Villata*; second, by simply applying `sent_tokenize()` to each abstract. The error difference between the two approaches is minimal.\n",
    "\n",
    "### 4. QuaNet with custom training routine\n",
    "\n",
    "Now, let us attempt the first of the previously proposed modifications. We have adjusted the `QuaNet` training routine so that, in each epoch, every batch contains all the sentences from an abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec734a6-8418-4940-8c3c-7394aa58c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train QuaNet (alternatively, we can set fit_classifier=True and let QuaNet train the classifier)\n",
    "quantifier_custom = QuaNetTrainerABS(cnn_classifier, qp.environ['SAMPLE_SIZE'], device='cpu', checkpointdir='../checkpoint/components-custom', checkpointname='Quanet-Components'))\n",
    "quantifier_custom.fit(abs_dataset.training, fit_classifier=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d79122-4774-4458-9bb9-408c4b164847",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Results on training set:')\n",
    "result_train_custom = evaluate(collection=abs_dataset.training, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier_custom)\n",
    "\n",
    "print('\\nResults on test set:')\n",
    "result_test_custom = evaluate(collection=abs_dataset.test, n=[1,3,5,10, qp.environ['SAMPLE_SIZE'], 15], quantifier=quantifier_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb8325-93d5-4060-8b57-40343ee1d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = random.choice(test_collection.filenames)\n",
    "\n",
    "print('Standard QuaNet:')\n",
    "infer(test_set, indexer, comp_quantifier=quantifier, comp_classifier=cnn_classifier, rel_quantifier=None, filename=filename, show_text=False, use_tokenizer=True)\n",
    "\n",
    "print('Custom approach QuaNet:')\n",
    "infer(test_set, indexer, comp_quantifier=quantifier_custom, comp_classifier=cnn_classifier, rel_quantifier=None, filename=filename, show_text=False, use_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd82fb-1046-4ecf-8296-b1bdd1389223",
   "metadata": {},
   "source": [
    "Unfortunately, we observe generally higher error values with this methodology. Moreover, if we run the previous cell multiple times, we notice that the custom quantifier tends to predict an equal distribution of `Components` and `Non-components`: this might reflects the fact that, on average, both the training and test sets have a roughly equal number of argument components and non-components within each individual abstract.\n",
    "\n",
    "```python\n",
    "- Train set:\n",
    "\tLabel 0: 2760 samples\n",
    "\tLabel 1: 2593 samples\n",
    "\n",
    "\tThere are 2 different labels in the train set -> [0, 1]\n",
    "\tAverage number of sentences per file in train set: 13\n",
    "\tMax sentence length: 107\n",
    "\tAverage components per file: 6.48\n",
    "\tAverage non-components per file: 6.90\n",
    "\n",
    "- Test set:\n",
    "\tLabel 0: 1948 samples\n",
    "\tLabel 1: 1880 samples\n",
    "\n",
    "\tThere are 2 different labels in the test set -> [0, 1]\n",
    "\tAverage number of sentences per file in test set: 14\n",
    "\tMax sentence length: 91\n",
    "\tAverage components per file: 6.99\n",
    "\tAverage non-components per file: 7.24\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
