{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b94aae95-7ac0-463c-8f87-84db518934ea",
   "metadata": {},
   "source": [
    "# Experiment 4\n",
    "DEVO RIFARE READ BRAT DATASET SEGUENDO LA FOTO SUL CELLULARE\n",
    "\n",
    "\n",
    "In this experiment, we aim to solve the counting task using QuaNet, a deep learning architecture for quantification that predicts class prevalence values. It takes as input: (i) class prevalence values estimated by a classifier; (ii) posterior probabilities $Pr(𝑦|x)$ for the positive class (since QuaNet is binary) for each document $x$, and (iii) embedded document representations.\n",
    "\n",
    "We chose QuaNet because, as stated in the detailed overview of the [LeQua challenge](https://ceur-ws.org/Vol-3180/paper-146.pdf), it outperforms other methods in the **binary classification task on raw documents**. However, the methods provided by `QuaPy` are interchangeable black-boxes, meaning one can easily replace them to test which performs best for the given task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee6643c-ae6d-4d48-ad83-18f6eb1638bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_4_code import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4fd05-5634-45e2-9713-4482ff4414e3",
   "metadata": {},
   "source": [
    "We first load the *Cabrio* and *Villata* dataset, constructing a dictionary that, for each abstract, contains the graph structure of its arguments, reconstructed using the *.ann* files provided with the raw documents.\n",
    "\n",
    "For this experiment, we chose the label to be the number of arguments in each document. We then drop labels with counts less than 4 to prevent imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471b1a98-8b85-4454-bafe-31ed4ad14d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_brat_dataset('../data/train/neoplasm_train') + read_brat_dataset('../data/dev/neoplasm_dev')\n",
    "test_set = read_brat_dataset('../data/test/glaucoma_test') + read_brat_dataset('../data/test/neoplasm_test') + read_brat_dataset('../data/test/mixed_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5071a6e1-d402-4747-949d-7c08ec7b1dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set statistics:\n",
      "\n",
      "Number of samples with a certain number of arguments:\n",
      "+---------------+-------------+\n",
      "|   # Arguments |   # Samples |\n",
      "+===============+=============+\n",
      "|             0 |           4 |\n",
      "+---------------+-------------+\n",
      "|             1 |         133 |\n",
      "+---------------+-------------+\n",
      "|             2 |         108 |\n",
      "+---------------+-------------+\n",
      "|             3 |          72 |\n",
      "+---------------+-------------+\n",
      "|             4 |          44 |\n",
      "+---------------+-------------+\n",
      "|             5 |          22 |\n",
      "+---------------+-------------+\n",
      "|             6 |          11 |\n",
      "+---------------+-------------+\n",
      "|             7 |           3 |\n",
      "+---------------+-------------+\n",
      "|             8 |           1 |\n",
      "+---------------+-------------+\n",
      "|            10 |           1 |\n",
      "+---------------+-------------+\n",
      "|            13 |           1 |\n",
      "+---------------+-------------+\n",
      "\n",
      "Number of samples with a certain number of relations:\n",
      "+---------------+-------------+\n",
      "|   # Relations |   # Samples |\n",
      "+===============+=============+\n",
      "|             0 |           7 |\n",
      "+---------------+-------------+\n",
      "|             1 |          18 |\n",
      "+---------------+-------------+\n",
      "|             2 |          60 |\n",
      "+---------------+-------------+\n",
      "|             3 |          70 |\n",
      "+---------------+-------------+\n",
      "|             4 |          85 |\n",
      "+---------------+-------------+\n",
      "|             5 |          81 |\n",
      "+---------------+-------------+\n",
      "|             6 |          40 |\n",
      "+---------------+-------------+\n",
      "|             7 |          22 |\n",
      "+---------------+-------------+\n",
      "|             8 |          10 |\n",
      "+---------------+-------------+\n",
      "|             9 |           2 |\n",
      "+---------------+-------------+\n",
      "|            10 |           3 |\n",
      "+---------------+-------------+\n",
      "|            11 |           2 |\n",
      "+---------------+-------------+\n",
      "\n",
      "Max abstract length: 3986 characters\n",
      "\n",
      "- Test set statistics:\n",
      "\n",
      "Number of samples with a certain number of arguments:\n",
      "+---------------+-------------+\n",
      "|   # Arguments |   # Samples |\n",
      "+===============+=============+\n",
      "|             0 |           5 |\n",
      "+---------------+-------------+\n",
      "|             1 |          68 |\n",
      "+---------------+-------------+\n",
      "|             2 |          93 |\n",
      "+---------------+-------------+\n",
      "|             3 |          71 |\n",
      "+---------------+-------------+\n",
      "|             4 |          31 |\n",
      "+---------------+-------------+\n",
      "|             5 |          18 |\n",
      "+---------------+-------------+\n",
      "|             6 |           7 |\n",
      "+---------------+-------------+\n",
      "|             7 |           4 |\n",
      "+---------------+-------------+\n",
      "|             8 |           1 |\n",
      "+---------------+-------------+\n",
      "|             9 |           1 |\n",
      "+---------------+-------------+\n",
      "|            10 |           1 |\n",
      "+---------------+-------------+\n",
      "\n",
      "Number of samples with a certain number of relations:\n",
      "+---------------+-------------+\n",
      "|   # Relations |   # Samples |\n",
      "+===============+=============+\n",
      "|             0 |           4 |\n",
      "+---------------+-------------+\n",
      "|             1 |          12 |\n",
      "+---------------+-------------+\n",
      "|             2 |          63 |\n",
      "+---------------+-------------+\n",
      "|             3 |          70 |\n",
      "+---------------+-------------+\n",
      "|             4 |          76 |\n",
      "+---------------+-------------+\n",
      "|             5 |          33 |\n",
      "+---------------+-------------+\n",
      "|             6 |          22 |\n",
      "+---------------+-------------+\n",
      "|             7 |           7 |\n",
      "+---------------+-------------+\n",
      "|             8 |           4 |\n",
      "+---------------+-------------+\n",
      "|             9 |           4 |\n",
      "+---------------+-------------+\n",
      "|            10 |           3 |\n",
      "+---------------+-------------+\n",
      "|            12 |           2 |\n",
      "+---------------+-------------+\n",
      "\n",
      "Max abstract length: 4383 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arguments_counts_train, relations_counts_train = compute_dataset_statistics(train_set, dataset_name=\"train\")\n",
    "arguments_counts_test, relations_counts_test = compute_dataset_statistics(test_set, dataset_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb780997-99f2-4be8-8e31-f0a2be52e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [item for item in train_set if relations_counts_train.get(item['n_relations'], 0) > 4]\n",
    "test_set = [item for item in test_set if relations_counts_test.get(item['n_relations'], 0) > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e7c3748-2106-4455-967e-9df681be738b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Train set statistics:\n",
      "\n",
      "Number of samples with a certain number of arguments:\n",
      "+---------------+-------------+\n",
      "|   # Arguments |   # Samples |\n",
      "+===============+=============+\n",
      "|             1 |         132 |\n",
      "+---------------+-------------+\n",
      "|             2 |         107 |\n",
      "+---------------+-------------+\n",
      "|             3 |          71 |\n",
      "+---------------+-------------+\n",
      "|             4 |          43 |\n",
      "+---------------+-------------+\n",
      "|             5 |          22 |\n",
      "+---------------+-------------+\n",
      "|             6 |           8 |\n",
      "+---------------+-------------+\n",
      "\n",
      "Number of samples with a certain number of relations:\n",
      "+---------------+-------------+\n",
      "|   # Relations |   # Samples |\n",
      "+===============+=============+\n",
      "|             0 |           5 |\n",
      "+---------------+-------------+\n",
      "|             1 |          18 |\n",
      "+---------------+-------------+\n",
      "|             2 |          57 |\n",
      "+---------------+-------------+\n",
      "|             3 |          70 |\n",
      "+---------------+-------------+\n",
      "|             4 |          82 |\n",
      "+---------------+-------------+\n",
      "|             5 |          80 |\n",
      "+---------------+-------------+\n",
      "|             6 |          39 |\n",
      "+---------------+-------------+\n",
      "|             7 |          22 |\n",
      "+---------------+-------------+\n",
      "|             8 |          10 |\n",
      "+---------------+-------------+\n",
      "\n",
      "Max abstract length: 3288 characters\n",
      "\n",
      "- Test set statistics:\n",
      "\n",
      "Number of samples with a certain number of arguments:\n",
      "+---------------+-------------+\n",
      "|   # Arguments |   # Samples |\n",
      "+===============+=============+\n",
      "|             0 |           5 |\n",
      "+---------------+-------------+\n",
      "|             1 |          62 |\n",
      "+---------------+-------------+\n",
      "|             2 |          89 |\n",
      "+---------------+-------------+\n",
      "|             3 |          67 |\n",
      "+---------------+-------------+\n",
      "|             4 |          30 |\n",
      "+---------------+-------------+\n",
      "|             5 |          17 |\n",
      "+---------------+-------------+\n",
      "|             6 |           7 |\n",
      "+---------------+-------------+\n",
      "\n",
      "Number of samples with a certain number of relations:\n",
      "+---------------+-------------+\n",
      "|   # Relations |   # Samples |\n",
      "+===============+=============+\n",
      "|             1 |          12 |\n",
      "+---------------+-------------+\n",
      "|             2 |          58 |\n",
      "+---------------+-------------+\n",
      "|             3 |          69 |\n",
      "+---------------+-------------+\n",
      "|             4 |          76 |\n",
      "+---------------+-------------+\n",
      "|             5 |          33 |\n",
      "+---------------+-------------+\n",
      "|             6 |          22 |\n",
      "+---------------+-------------+\n",
      "|             7 |           7 |\n",
      "+---------------+-------------+\n",
      "\n",
      "Max abstract length: 2911 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arguments_counts_train, relations_counts_train= compute_dataset_statistics(train_set, dataset_name=\"train\")\n",
    "arguments_counts_test, relations_counts_test = compute_dataset_statistics(test_set, dataset_name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee986f0-2222-4d9f-8325-1896f0063669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 21224783 - 4 relations - 1 arguments\n",
      "Text: There are very few randomized controlled studies on exercise in cancer patients. Consequently, there are no guidelines available with regard to the exercises that can be recommended and difficulties are encountered in the clinical practice as to which exercise is more suitable to the patients. The purpose of this study was to investigate the impact of pilates exercises on physical performance, flexibility, fatigue, depression and quality of life in women who had been treated for breast cancer. Randomized controlled trial. Out patient group, Department of Physical Medicine and Rehabilitation and Medical Oncology Department, University Hospital. Fifty-two patients with breast cancer were divided into either pilates exercise (group 1) and control group (group 2). Patients in Group 1 performed pilates and home exercises and patients in group 2 performed only home exercises. Pilates exercise sessions were performed three times a week for a period of eight weeks in the rehabilitation unit. Subjects were assessed before and after rehabilitation program, with respect to, 6-min walk test (6MWT), modified sit and reach test, Brief Fatigue Inventory (BFI), Beck Depression Index (BDI) and the European Organisation for Research and Treatment of Cancer Quality of Life C30 (EORTC QLQ-C30) and EORTC QLQ BR23. After the exercise program, improvements were observed in Group 1 in 6-minute walk test, BDI, EORTC QLQ-C30 functional, and EORTC QLQ-C30 BR23 functional scores (P<0.05). In contrast, no significant improvement was observed in Group 2 after the exercise program in any of parameters in comparison to the pre-exercise period (P>0.05). When the two exercise groups were compared, there were significant differences in 6MWT in pilates-exercise group (P<0.05). Pilates exercises are effective and safe in female breast cancer patients. There is a need for further studies so that its effect can be confirmed. This study addressed the effects of pilates exercise, as a new approach, on functional capacity, fatigue, depression and quality of life in breast cancer patients in whom there are doubts regarding the efficacy and usefulness of the exercise.\n"
     ]
    }
   ],
   "source": [
    "display_file_info(train_set, filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72c71d-1334-4bf3-840c-e81a00a48e15",
   "metadata": {},
   "source": [
    "We now construct the `LabelledCollection` as required by `QuaPy`, from which we obtain the *train* and *test* collections; this is wrong, as the test set should be constructed using the other files provided by *Cabrio* and *Villata*. Finally, we create a `Dataset` object and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14e4d204-eea2-4378-b97d-06bc31fddfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)]\n",
      "[np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]\n"
     ]
    }
   ],
   "source": [
    "train_collection = qp.data.LabelledCollection([data['text'] for data in train_set], \n",
    "                                         [data['n_relations'] for data in train_set], \n",
    "                                         classes=list(relations_counts_test.keys()))\n",
    "\n",
    "test_collection = qp.data.LabelledCollection([data['text'] for data in test_set], \n",
    "                                         [data['n_relations'] for data in test_set], \n",
    "                                         classes=list(relations_counts_test.keys()))\n",
    "print(list(set(train_collection.labels)))\n",
    "print(list(set(test_collection.labels)))\n",
    "\n",
    "# train_collection = qp.data.LabelledCollection([data['text'] for data in train_set], \n",
    "#                                          [data['n_arguments'] for data in train_set], \n",
    "#                                          classes=list(relations_counts_test.keys()))\n",
    "\n",
    "# test_collection = qp.data.LabelledCollection([data['text'] for data in test_set], \n",
    "#                                          [data['n_arguments'] for data in test_set], \n",
    "#                                          classes=list(relations_counts_test.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4296d44f-cd0a-40a5-aaf4-e1987ab6c605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "indexing: 100%|████████████████████████████████████████████████████████████████████| 383/383 [00:00<00:00, 6471.10it/s]\n",
      "indexing: 100%|████████████████████████████████████████████████████████████████████| 288/288 [00:00<00:00, 5646.76it/s]\n"
     ]
    }
   ],
   "source": [
    "indexer = qp.data.preprocessing.IndexTransformer(min_df=1)\n",
    "abs_dataset = Dataset(train_collection, test_collection)\n",
    "\n",
    "index(abs_dataset, indexer, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbcbe0-dbee-4d2a-a3d7-77664d2d1863",
   "metadata": {},
   "source": [
    "At this point, we train a simple CNN for the task. Note that this part might be replaced with the baseline in future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb9680a2-8b2f-41e7-8fd3-139ee4e96e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeuralNetwork running on cpu]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[CNNnet] training epoch=42 tr-loss=0.01774 tr-acc=100.00% tr-macroF1=100.00% patience=1/10 val-loss=2.17926 val-acc=16.C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\classification\\neural.py:203: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(checkpoint))\n",
      "[CNNnet] training epoch=42 tr-loss=0.01774 tr-acc=100.00% tr-macroF1=100.00% patience=1/10 val-loss=2.17926 val-acc=16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ended by patience exhasted; loading best model parameters in ../checkpoint/classifier_net.dat for epoch 32\n",
      "performing one training pass over the validation set...\n",
      "[done]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<quapy.classification.neural.NeuralClassifierTrainer at 0x20b848109b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the text classifier:\n",
    "qp.environ['SAMPLE_SIZE'] = 10\n",
    "\n",
    "cnn_module = CNNnet(abs_dataset.vocabulary_size, abs_dataset.training.n_classes)\n",
    "cnn_classifier = NeuralClassifierTrainer(cnn_module, device='cpu')\n",
    "cnn_classifier.fit(*abs_dataset.training.Xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247703e7-f7e5-4c71-b444-5b4eb0b50492",
   "metadata": {},
   "source": [
    "Next, we train `QuaNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1be6fc8-6279-423e-9afe-80a9ba727e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuaNetModule(\n",
      "  (lstm): LSTM(110, 64, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (ff_layers): ModuleList(\n",
      "    (0): Linear(in_features=168, out_features=1024, bias=True)\n",
      "    (1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio\\anaconda3\\envs\\NLP\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m qp\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAMPLE_SIZE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m quantifier \u001b[38;5;241m=\u001b[39m QuaNet(cnn_classifier, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mquantifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mabs_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit_classifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\method\\_neural.py:179\u001b[0m, in \u001b[0;36mQuaNetTrainer.fit\u001b[1;34m(self, data, fit_classifier)\u001b[0m\n\u001b[0;32m    176\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_posteriors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch(valid_data_embed, valid_posteriors, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mva_iter, epoch_i, early_stop, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m     early_stop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mva-loss\u001b[39m\u001b[38;5;124m'\u001b[39m], epoch_i)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\method\\_neural.py:228\u001b[0m, in \u001b[0;36mQuaNetTrainer._epoch\u001b[1;34m(self, data, posteriors, iterations, epoch, early_stop, train)\u001b[0m\n\u001b[0;32m    221\u001b[0m mae_errors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    222\u001b[0m sampler \u001b[38;5;241m=\u001b[39m UPP(\n\u001b[0;32m    223\u001b[0m     data,\n\u001b[0;32m    224\u001b[0m     sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_size,\n\u001b[0;32m    225\u001b[0m     repeats\u001b[38;5;241m=\u001b[39miterations,\n\u001b[0;32m    226\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# different samples during train, same samples during validation\u001b[39;00m\n\u001b[0;32m    227\u001b[0m )\n\u001b[1;32m--> 228\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, total\u001b[38;5;241m=\u001b[39msampler\u001b[38;5;241m.\u001b[39mtotal())\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it, index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m    230\u001b[0m     sample_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msampling_from_index(index)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\protocol.py:388\u001b[0m, in \u001b[0;36mUPP.samples_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m indexes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prevs \u001b[38;5;129;01min\u001b[39;00m F\u001b[38;5;241m.\u001b[39muniform_simplex_sampling(n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mn_classes, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeats):\n\u001b[1;32m--> 388\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprevs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m     indexes\u001b[38;5;241m.\u001b[39mappend(index)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexes\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\NLP\\Lib\\site-packages\\quapy\\data\\base.py:155\u001b[0m, in \u001b[0;36mLabelledCollection.sampling_index\u001b[1;34m(self, size, shuffle, random_state, *prevs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_, n_requested \u001b[38;5;129;01min\u001b[39;00m n_requests\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    153\u001b[0m     n_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[class_])\n\u001b[0;32m    154\u001b[0m     index_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex[class_][\n\u001b[1;32m--> 155\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_requested\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     ] \u001b[38;5;28;01mif\u001b[39;00m n_requested \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    158\u001b[0m     indexes_sample\u001b[38;5;241m.\u001b[39mappend(index_sample)\n\u001b[0;32m    160\u001b[0m indexes_sample \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(indexes_sample)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:968\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "# train QuaNet (alternatively, we can set fit_classifier=True and let QuaNet train the classifier)\n",
    "qp.environ['SAMPLE_SIZE'] = 1\n",
    "quantifier = QuaNet(cnn_classifier, device='cpu')\n",
    "quantifier.fit(abs_dataset.training, fit_classifier=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd71b55-65e5-4bc0-9d55-8647e05c42ab",
   "metadata": {},
   "source": [
    "Finally, we evaluate the accuracy of our model by sampling one document at a time and inferring its distribution (from which the *Single Sample* name of the experiment). The `argmax` of the distribution will represent the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1020f-3dcf-419d-81d9-6c7fd755c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_accuracy(abs_dataset, quantifier, set_type='train')\n",
    "evaluate_accuracy(abs_dataset, quantifier, set_type='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ad552-da29-4df3-8668-8bbf17c0950d",
   "metadata": {},
   "source": [
    "The results on the test set are poor. We might:\n",
    "- Tune the model, as we just used the default configuration. This *must* be tested, has the method might work better with minimum effort;\n",
    "- Explore other quantification techniques instead of QuaPy. If we determine that a method suits better, we might consider to conduct other experiments like the third one differently. Please, give a look to the [LeQua](https://ceur-ws.org/Vol-3180/paper-146.pdf) paper; the UniOviedo should have obtained the best result on the *T2A* task, the one with raw documents and multi-classes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3d0fb1-70ac-49fe-b1d3-b7398a98423a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
